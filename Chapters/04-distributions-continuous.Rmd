# Continuous distributions {#distributions}
After discussing the discrete probability distributions, we can now move to the continuous ones. The idea behind them is similar to the one discussed in Chapter \@ref(countDistributions). In this chapter, we will discuss the main properties of continuous distributions, focusing on several of them, including: Uniform, Normal, Exponential, Laplace, S, Generalised Normal, Asymmetric Laplace, Log Normal, Inverse Gaussian and Gamma.


## What is continuous distribution? {#distributionsContinuousWhat}
The main difference arises from the idea discussed in Section \@ref(whatIsRandomVariable): the probability that a continuous random variable will take a specific value is zero. Because of that we should be discussing the probability of a random variable taking a value in an interval. Figure \@ref(fig:distributionContinuousExample) demonstrates an empirical distribution of continuous random variable.

```{r distributionContinuousExample, echo=FALSE, fig.cap="Distribution of a continuous random variable."}
set.seed(41)
yHist <- hist(rnorm(100,100,10), probability=TRUE, main="",
              xlab="Value of variable", ylab="Probability of outcome",
              axes=FALSE, col=18)
axis(side=2)
axis(side=1, at=yHist$breaks)
```

Based on Figure \@ref(fig:distributionContinuousExample), we can say that the probability of obtaining the value between 100 and 105 is higher than for the variable to get any other interval. It also looks like the variable is continuous on the interval between 75 and 125, and we do not observe any values outside of this interval.

Almost any continuous distribution can be characterised by several functions:

1. Probability density function (PDF);
2. Cumulative distribution function (CDF);
3. Quantile function (QF);
4. Moment Generation Function (MMF);
5. Characteristic function (CF).

PDF has similarities with PMF, but does not return the probabilities, but rather the score for the variable in each specific point because (once again) the probability of continuous variable taking a specific value is equal to zero. PDF however is useful because it represents the shape of the distribution, showing where the values are concentrated. Figure \@ref(fig:dnormPlotIntro) demonstrates an example of PDF of Normal distribution (discussed in more detail in Section \@ref(distributionsNormal)).

```{r dnormPlotIntro, fig.cap="Probability Density Function of Normal distribution", echo=FALSE}
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="y",main="")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)), rep(0,length(seq(-5,5,0.1)))), col=18)
```

It can be seen from the Figure \@ref(fig:dnormPlotIntro) that the density of the distribution is higher at its centre, around zero, which means that it is more likely to get values around the centre rather than near the tails of the distribution.

If we want to work with probabilities in case of the continuous distribution, we need to use CDF, which is similar to the one discussed in the Chapter \@ref(countDistributions). Figure \@ref(fig:dnormPlotIntroCDF) shows example of CDF of Normal distribution.

```{r dnormPlotIntroCDF, fig.cap="Cumulative Distribution Function of Normal distribution", echo=FALSE}
plot(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),type="l",ylab="Probability",xlab="y",main="")
```

The CDF of continuous variable has the same properties as CDF of the discrete one with the minor differences: in a general case it converges to one with the increase of the value $y$ and converges to zero with the decrease of it. There are some continuous distributions that are restricted with an interval. For those distributions, the CDF reaches boundary values.

CDF is obtained by calculating the surface of PDF for each specific value of $y$. Figure \@ref(fig:dnormPlotIntroCDFPDF) shows the connection between them.

```{r dnormPlotIntroCDFPDF, fig.cap="Cumulative Distribution Function of Normal distribution", echo=FALSE}
par(mfcol=c(2,1), mar=c(2,4,1,1))
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="y",main="")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)), rep(0,length(seq(-5,5,0.1)))), col=18)
polygon(c(seq(-3,-1,0.01),rev(seq(-3,-1,0.01))),
        c(dnorm(seq(-3,-1,0.01)), rep(0,length(seq(-3,-1,0.01)))), col=17)
lines(c(-1,-1),c(0,dnorm(-1)),col=3,lwd=2,lty=1)

plot(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),type="l",ylab="Probability",xlab="y",main="")
lines(c(-1,-1),c(0,pnorm(-1)),col=3,lwd=2,lty=2)
lines(c(-4,-1),c(pnorm(-1),pnorm(-1)),col=3,lwd=2,lty=2)
```

The dark area in the first plot in Figure \@ref(fig:dnormPlotIntroCDFPDF) is equal to the probability (the value on y-axis) in the second plot, which is approximately equal to `r round(pnorm(-1),2)`. Mathematically, the CDF is calculated as an integral of the PDF, thus capturing this idea of the surface below the PDF curve.

Another important function is the Quantile Function. It returns the value of $y$ for the given probability. By the definition, QF is the inverse of the CDF. It does not always have a closed form (thus cannot be represented mathematically), and for some distributions numerical optimisation is required to obtain the quantiles. Figure \@ref(fig:dnormPlotIntroQF) demonstrates the quantile function of Normal distribution.

```{r dnormPlotIntroQF, fig.cap="Quantile Function of Normal distribution", echo=FALSE}
plot(seq(0,1,0.01),qnorm(seq(0,1,0.01)),type="l",ylab="y",xlab="Probability",main="")
lines(c(0.25,0.25),c(-3,qnorm(0.25)),col=3,lwd=2,lty=2)
lines(c(-1,0.25),c(qnorm(0.25),qnorm(0.25)),col=3,lwd=2,lty=2)
```

The dashed lines in Figure \@ref(fig:dnormPlotIntroQF) show the value of $y$ for the probability 0.25 according to the quantile function. In this example $y \approx -0.68$, meaning that in 25% of the cases the random variable $y$ will lie below this value. The quantiles of distributions are discussed in more detail in Section \@ref(dataAnalysisNumerical).

Finally, we have already mentioned MGF and CF in the context of discrete distributions. They play similar roles to the ones already discussed and can be used to obtain mean, variance, skewness and other statistics.


## Continuous Uniform distribution {#distributionsUniformContinuous}
Consider an example with elevator. You press the button to call it, and it arrives after some time. This time of arrival can be 0 seconds if the elevator is at your floor, or 1 minute if it is already moving and delivering people to a different floor. From our perspective, we do not know what the state of the elevator is, and we consider each of the possible scenarios equally probable. So, the elevator arriving in 0 seconds has the same probability as it arriving in 5, 15, 30, 55 or 60 seconds. In this case, we could use the continuous uniform distribution to model the lift arrival and to understand, for example, how much time we would need to wait on average.

The contiunuous uniform distribution has similarities to the discrete one, which was discussed in Section \@ref(distributionUniform), but due to the different nature of the random variable is parameterised differently. First, because we are discussing continuous variable, it is always defined on a segment of values, from $a$ to $b$. For example, we can have a random variable which can take any value from 0 to 10 with equal likelihood. Mathematically, the PDF of continuous distribution can be written as:
\begin{equation}
    f(y, b, a) = \left\{\begin{aligned}
                        & \frac{1}{b-a} & \text{ for } y \in [a, b] \\
                        & 0 & \text{ otherwise }
                    \end{aligned} \right. .
    (\#eq:ContinuousUniformPDF)
\end{equation}
It can be represented visually as shown in Figure \@ref(fig:uniformPDF).

```{r uniformPDF, fig.cap="Probability Density Function of Continuous Uniform distribution.", echo=FALSE}
plot(seq(0,10,0.1),dunif(seq(0,10,0.1),0,10),
     ylab="Density",xlab="y", type="l", ylim=c(0,0.14))
polygon(c(seq(0,10,0.1),rev(seq(0,10,0.1))),
        c(dunif(seq(0,10,0.1),0,10), rep(0,length(seq(0,10,0.1)))), col=18)
lines(c(0,0), c(0,0.1), lwd=2, col=3)
lines(c(10,10), c(0,0.1), lwd=2, col=3)
```

According to this distribution, it is equally likely to have 1, 1.1, 1.0001, 9 etc values. The mean of this distribution coincides with the middle of the segment and can be calculated as:
\begin{equation}
    \mathrm{E}(y) = \frac{1}{2}(a+b) ,
    (\#eq:ContinuousUniformPDFMean)
\end{equation}
while the variance is calculated as:
\begin{equation}
    \mathrm{V}(y) = \frac{1}{12}(b-a)^2 .
    (\#eq:ContinuousUniformPDFVariance)
\end{equation}
The CDF of the Uniform distribution corresponds to the straight line going from the point (a,0) to the point (b,1), as shown in Figure \@ref(fig:uniformCDFContinuous).

```{r uniformCDFContinuous, fig.cap="Cumulative Density Function of Continuous Uniform distribution.", echo=FALSE}
plot(seq(0,10,by=0.1),punif(seq(0,10,by=0.1),0,10),
     ylab="Probability",xlab="y", type="l")
```

Mathematically, the CDF can be represented as:
\begin{equation}
    F(y, b, a) = \left\{\begin{aligned}
                        & 0 & \text{ if } y<a \\
                        & \frac{y-a}{b-a} & \text{ for } y \in [a, b] \\
                        & 1 & \text{ if } y>b
                    \end{aligned} \right. .
    (\#eq:ContinuousUniformCDF)
\end{equation}

Continuous uniform distribution is sometimes used in statistics as a prior, when a researcher does not have grounds to assume any other, more complicated distribution.

In R, this distribution is implemented in `stats` package with functions `dunif()`, `punif()`, `qunif()` and `runif()` for PDF, CDF, QF and random generator respectively.


## Normal distribution {#distributionsNormal}
Every statistical textbook has Normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and because it is an asymptotic distribution for many other well-behaved distributions under some conditions (see discussion of "Central Limit Theorem" in Section \@ref(CLT)). For example, consider the the coin tossing example and Binomial distribution discussed in Section \@ref(distributionBinomial). If we toss the coin one time only, we get the Bernoulli distribution with two outcomes. If we do that ten times, the shape of distribution will change and there will be a score with higher probability than the others (that is $\mathcal{Bi}(10, 0.5)$). If we continue tossing the coin and do that for a hundred times, the shape of distribution will start converging to the bell-curve, reminding the Normal distribution. These three cases are shown in Figure \@ref(fig:binomialPMFSeveral).

```{r binomialPMFSeveral, fig.cap="Probability Mass Functions for Binomial distribution with p=0.5 and n={1, 10, 100}.", echo=FALSE}
par(mfcol=c(1,3))
# n=1
test <- barplot(dbinom(c(0:1), size=1, prob=0.5),
                ylab="Probability",xlab="y",
                main="", col=18)
axis(side=1,at=test,labels=c(0:1))
# n=10
test <- barplot(dbinom(c(0:10), size=10, prob=0.5),
                ylab="Probability",xlab="y", ylim=c(0,0.25),
                main="", col=18)
axis(side=1,at=test,labels=c(0:10))
# n=100
test <- barplot(dbinom(c(30:70), size=100, prob=0.5),
                ylab="Probability",xlab="y", ylim=c(0,0.08),
                main="", col=18)
axis(side=1,at=test,labels=c(30:70))
```

This is one of the classical examples of a distribution converging to the Normal one with the increase of the number of trials $n$ under some circumstances. This also tells us that in some circumstances we can use Normal distribution as an approximation of the real distribution: in Figure \@ref(fig:binomialPMFSeveral), the third graph corresponds to the $\mathcal{Bi}(100, 0.5)$, but it can be approximated by the normal distribution $\mathcal{N}(\mu_y, \sigma)$, where $\mu_y=100 \times 0.5 = 50$ is the mean of the distribution and $\sigma^2 = 100 \times 0.5 \times 0.5 = 25$ is the variance (as discussed in Section \@ref(distributionBinomial)), i.e. $\mathcal{N}(50, 25)$.

The probability density function (PDF) of the Normal distribution with some mean $\mu_y$ and variance $\sigma^2$ is:
\begin{equation}
    f(y, \mu_y, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2} \left(\frac{y - \mu_y}{\sigma}\right)^2 \right) ,
    (\#eq:Normal)
\end{equation}

This distribution can be represented in Figure \@ref(fig:dnormPlot).

```{r dnormPlot, fig.cap="Probability Density Function of Standard Normal distribution", echo=FALSE}
plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1)),type="l",ylab="Density",xlab="y",main="")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)), rep(0,length(seq(-5,5,0.1)))), col=18)
abline(v=0, col=3, lwd=2)
text(0.3,0,TeX("$\\mu =0$"), pos=3)
```

Figure \@ref(fig:dnormPlot) demonstrates a standard normal distribution, meaning that $\mu_y=0$ and $\sigma^2=1$. A more general distribution with non-zero $\mu_y$ and a non-unity $\sigma^2$ will have the same shape, but will have different values on the axes. The shape itself demonstrates that there is a central tendency (in our case - the mean $\mu_y$), around which the density of values is the highest and there are other potential cases, further away from the centre of distribution, but their probability of appearance reduces proportionally to the distance from the centre. As we can see from Figure \@ref(fig:dnormPlot), the Normal distribution is symmetric. It has skewness of zero and kurtosis of 3 (see discussion in Section \@ref(dataAnalysisNumerical)).

<!-- Normal distribution is the most popular assumption for the random variable in conventional statistical models. This is represented as $y \sim \mathcal{N}(\mu_y, \sigma)$. -->

When it comes to the cumulative distribution function, it has a form of an S-curve as shown in Figure \@ref(fig:pnormPlot).

```{r pnormPlot, fig.cap="Cumulative Distribution Function of Standard Normal distribution", echo=FALSE}
plot(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),type="l",ylab="Probability",xlab="y",main="")
```

The CDF in Figure \@ref(fig:pnormPlot) has the properties of any other CDF: it converges to one with the increase of the value of $y$ and reaches zero asymptotically with the decrease of the value of $y$. It can be used to solve problems of the style "what is the probability that $y$ will lie between 20 and 30 for the $\mathcal{N}(20, 10)$". In R, this can be done by entering the values of $y$, $\mu$ and $\sigma^2$ in the following function (note that in R, the scale is the standard deviation $\sigma$, not the variance $\sigma^2$):
```{r}
pnorm(q=30, mean=20, sd=10) - pnorm(q=20, mean=20, sd=10)
```
which mathematically is typically represented as:
\begin{equation}
    \Phi(y_2, \mu_y, \sigma^2) - \Phi(y_1, \mu_y, \sigma^2) = \Phi(30, 20, 100) - \Phi(20, 20, 100) = 0.841 - 0.5 \approx 0.341.
    (\#eq:NormalCDFExample)
\end{equation}
The CDF itself is difficult to summarise and involves a complicated integral:
\begin{equation}
    \Phi(y, \mu_y, \sigma^2) = \frac{1}{2} \left(1 + \mathrm{erf}\left(\frac{y-\mu_y}{\sqrt{2\sigma^2}} \right) \right),
    (\#eq:NormalCDFE)
\end{equation}
where $\mathrm{erf(y)= \frac{2}{\sqrt{\pi}} \int_{0}^{y} e^{-x^2} dx}$ is the so called ``error function''. This function does not have a closed form and is typically evaluated numerically or using some approximations. The probability \@ref(eq:NormalCDFExample) corresponds to the difference between the points shown in Figure \@ref(fig:pnormExample).

```{r pnormExample, fig.cap="Values of CDF for the example with Normal distribution", echo=FALSE}
plot(seq(-10,50,0.1),pnorm(seq(-10,50,0.1),20,10),type="l",ylab="Density",xlab="y",main="")
lines(c(20,20),c(0,pnorm(20,20,10)), lty=2)
lines(c(-20,20),rep(pnorm(20,20,10),2), lty=2)
text(-9,pnorm(20,20,10), pnorm(20,20,10), pos=3)
lines(c(30,30),c(0,pnorm(30,20,10)), lty=2)
lines(c(-20,30),rep(pnorm(30,20,10),2), lty=2)
text(-9,pnorm(30,20,10), round(pnorm(30,20,10),3), pos=3)
```

The same number also corresponds to the dark area in PDF of the distribution as shown in Figure \@ref(fig:dnormExample).

```{r dnormExample, fig.cap="Values of PDF for the example with Normal distribution", echo=FALSE}
plot(seq(-15,55,0.1),dnorm(seq(-15,55,0.1),20,10),type="l",ylab="Density",xlab="y",main="",
     ylim=c(0,0.04), xlim=c(-10,50))
polygon(c(seq(-15,55,0.1),rev(seq(-15,55,0.1))),
        c(dnorm(seq(-15,55,0.1),20,10), rep(0,length(seq(-15,55,0.1)))), col=18)
polygon(c(seq(20,30,0.1),rev(seq(20,30,0.1))),
        c(dnorm(seq(20,30,0.1),20,10), rep(0,length(seq(20,30,0.1)))), col=11)
lines(c(20,20),c(0,dnorm(20,20,10)), lwd=2, col=3)
lines(c(30,30),c(0,dnorm(30,20,10)), lwd=2, col=3)
text(25,dnorm(25,20,10)/2, round(pnorm(30,20,10)-pnorm(20,20,10),3))
```

The relation between the area in Figure \@ref(fig:dnormExample) and the difference between the two points in Figure \@ref(fig:pnormExample) comes directly from the definition of CDF, the latter being the function of cumulative values of the density function.

Many distributions converge to the Normal one or can be approximated by it under some circumstances. For example, as shown earlier, Binomial distribution can be approximated by the normal in some cases. More specifically, the approximation works when $n>20$, $n p \geq 5$ and $n(1-p) \geq 5$. Figure \@ref(fig:binomialNormalApprox) demonstrates how the normal curve (the solid red line) approximates the barplots of the Binomial distribution. 

```{r binomialNormalApprox, fig.cap="Binomial distribution and its approximation via the Normal one.", echo=FALSE}
test <- hist(rbinom(100,100,0.5), breaks=c(0:100), plot=F)
test$density <- dbinom(c(0:100), 100, 0.5)
test$breaks <- test$breaks-0.5
plot(test, xlim=c(30,70), main="", freq=FALSE, ylab="Probability", xlab="y", col=18)
lines(seq(30,70,0.1),dnorm(seq(30,70,0.1),50,5), col=4, lwd=2)
```

As we can see from Figure \@ref(fig:binomialNormalApprox), the normal curve fits the bars of the Binomial distribution very well, which means that we can use it, for example, to compute the probability that the variable $y$ will be equal to 41, via the formula:
\begin{equation}
    \mathrm{P}(y=41) \approx \Phi(41.5, 50, 25) - \Phi(40.5, 50, 25)
    (\#eq:BinomialNormalApproximation)
\end{equation}
In the equation \@ref(eq:BinomialNormalApproximation) by adding and subtraction 0.5, we calculate the surface of the area under the normal curve, corresponding roughly to the area of the respective bin in the barplot. We can see that this method of calculation gives a result very close to the one from the Binomial distribution:

```{r}
c(dbinom(41,100,0.5),
  pnorm(41.5,50,5)-pnorm(40.5,50,5)) |>
    setNames(c("Binomial", "Normal"))
```

This calculation based on the approximation is shown visually in Figure \@ref(fig:binomialNormalApprox), where the second figure is the zoomed-in area in the first one. As we can see, the area under the curve of the Normal distribution is roughly equal to the area of the bar of the Binomial distribution.

```{r binomialNormalApproxArea, fig.cap="Calculating the probability based on Normal approximation.", echo=FALSE}
test <- hist(rbinom(100,100,0.5), breaks=c(0:100), plot=F)
test$breaks <- test$breaks-0.5
test$density <- dbinom(c(0:100), 100, 0.5)
par(mfcol=c(1,2))
# First plot
plot(test, xlim=c(30,70),
     main="", freq=FALSE, ylab="Probability", xlab="y", col=18)
polygon(c(seq(40.5,41.5,0.1),rev(seq(40.5,41.5,0.1))),
        c(dnorm(seq(40.5,41.5,0.1),50,5), rep(0,length(seq(40.5,41.5,0.1)))), col=10)
lines(seq(30,70,0.1),dnorm(seq(30,70,0.1),50,5), col=4, lwd=2)
lines(c(40.5,40.5),c(0,dnorm(40.5,50,5)), lwd=2, col=3)
lines(c(41.5,41.5),c(0,dnorm(41.5,50,5)), lwd=2, col=3)
#Second plot
plot(test, xlim=c(38,44), ylim=c(0,0.05),
     main="", freq=FALSE, ylab="Probability", xlab="y", col=18)
polygon(c(seq(40.5,41.5,0.1),rev(seq(40.5,41.5,0.1))),
        c(dnorm(seq(40.5,41.5,0.1),50,5), rep(0,length(seq(40.5,41.5,0.1)))), col=10)
lines(seq(30,70,0.1),dnorm(seq(30,70,0.1),50,5), col=4, lwd=2)
lines(c(40.5,40.5),c(0,dnorm(40.5,50,5)), lwd=2, col=3)
lines(c(41.5,41.5),c(0,dnorm(41.5,50,5)), lwd=2, col=3)
# lines(c(40.5,41.5)+0.5,rep(dbinom(41,100,0.5),2))
```

Normal distribution is implemented in `dnorm()`, `qnorm()`, `pnorm()` and `rnorm()` functions from `stats` package in R.

Finally, as mentioned earlier, Normal distribution is popular among statisticians to use for the error term in a model of a type:
\begin{equation}
    y_j = \mu_j + \epsilon_j,
    (\#eq:NormalModel)
\end{equation}
where $\mu_j$ is some structure and $\epsilon_j \sim \mathcal{N}(0, \sigma^2)$. The main reason for this is the ease of use of the distribution, and because it is described using its mean and variance. For example, based on \@ref(eq:NormalModel), we can say that $y_j \sim \mathcal{N}(\mu_j, \sigma^2)$, because as we remember from Chapter \@ref(probabilityTheory), $\mathrm{E}(y_j)=\mathrm{E}(\mu_j) + \mathrm{E}(\epsilon_j)=\mathrm{E}(\mu_j)$. In reality, the error term of a model might not follow normal distribution, it can be more complicated and sometimes might not follow any theoretical distribution.


## Log-Normal distribution {#distributionLogNormal}
Log-Normal distribution is closely related to the Normal one and is supported for the positive values of $y$. It is defined as a distribution arising after a transformation of a variable $x=\log(y)$ or equivalently $y=e^x$. It is said that $y=e^x \sim \log\mathcal{N}(\mu_y, \sigma^2)$ if $\log y = x \sim \mathcal{N}(\mu_y, \sigma^2)$. Figure \@ref(fig:normalLogConnection) shows the connection between Normal and Log-Normal distributions. In that plot, we can see how the density changes because of the $y=e^x$ transformation.

```{r normalLogConnection, fig.cap="Connection between Normal and Log-Normal distributions", echo=FALSE}
par(mfrow=c(2,2), mar=c(4,5,1,0))
# Normal
plot(seq(-1.5,1.5,0.01), dnorm(seq(-1.5,1.5,0.01),0,0.5), type="l", xlab="x", ylab="Density")
polygon(c(seq(-1.5,1.5,0.01),rev(seq(-1.5,1.5,0.01))),
        c(dnorm(seq(-1.5,1.5,0.01),0,0.5),rep(0,length(seq(-1.5,1.5,0.01)))),
        col=18)
polygon(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01))),
        c(dnorm(seq(-1.5,0.5,0.01),0,0.5),rep(0,length(seq(-1.5,0.5,0.01)))),
        col=11)
lines(rep(0.5,2), c(0,dnorm(0.5,0,0.5)-0.01), lty=1, lwd=2, col="white")
# lines(rep(0.5,2), c(0,dnorm(0.5,0,0.5)), lty=2, lwd=2, col=3)
abline(v=0.5, lty=2, lwd=2, col=3)
#Empty
plot(dlnorm(exp(seq(-1.5,1.5,0.01)),0,0.5),dnorm(seq(-1.5,1.5,0.01),0,0.5),
     col="white",
     axes=F, xlab="", ylab="",type="l")
# exp(x)
plot(seq(-1.5,1.5,0.01), exp(seq(-1.5,1.5,0.01)), type="l", xlab="x", ylab="y")
lines(seq(-1.5,1.5,0.01), exp(seq(-1.5,1.5,0.01)))
lines(rep(0.5,2), c(5,exp(0.5)), lty=2, lwd=2, col=3)
lines(c(0.5,2), rep(exp(0.5),2), lty=2, lwd=2, col=3)
text(0.9,3,TeX("$y=e^x$"),pos=3)
# Log-Normal
plot(dlnorm(exp(seq(-1.5,1.5,0.01)),0,0.5),exp(seq(-1.5,1.5,0.01)), type="l", xlab="Density", ylab="y")
polygon(c(dlnorm(exp(seq(-1.5,1.5,0.01)),0,0.5),rep(0,length(seq(-1.5,1.5,0.01)))),
        exp(c(seq(-1.5,1.5,0.01),rev(seq(-1.5,1.5,0.01)))),
        col=18)
polygon(c(dlnorm(exp(seq(-1.5,0.5,0.01)),0,0.5),rep(0,length(seq(-1.5,0.5,0.01)))),
        exp(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01)))),
        col=11)
abline(h=exp(0.5), lty=1, lwd=2, col="white")
abline(h=exp(0.5), lty=2, lwd=2, col=3)
```

The dark areas on the plots in Figure \@ref(fig:normalLogConnection) show equal probabilities for the Normal and the Log-Normal distributions obtained via specific quantiles. This demonstrates that in order to obtain a quantile of the Log-Normal distribution for $y$, we need to produce a quantile from the Normal one for $x$ and then exponentiate the value.

Because of its shape and support of positive values only, the Log-Normal distribution is often used in multiplicative models of the style:
\begin{equation}
    y_j = \mu_j \epsilon_j ,
    (\#eq:LogNormalModel)
\end{equation}
which is equivalent to:
\begin{equation}
    \log y_j = \log \mu_j + \log \epsilon_j ,
    (\#eq:LogNormalModelLogs)
\end{equation}
where $\epsilon_j \sim \mathrm{log}\mathcal{N}(0, \sigma^2)$ and $\log \epsilon_j \sim \mathcal{N}(0, \sigma^2)$. Log-Normal distribution is also used to model, for example, prices or income of households. When talking about the latter, conceptually, we expect it to have asymmetric distribution, because there will be a lot of households with low income and few with very high ones. Log-Normal distribution can be considered as a reasonable model in this case.

The PDF of the Log-Normal distribution is written mathematically as:
\begin{equation}
    f(y, \mu_y, \sigma^2) = \frac{1}{y \sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2} \left(\frac{\log y - \mu_y}{\sigma}\right)^2 \right) .
    (\#eq:LogNormalPDF)
\end{equation}
Several PDFs of Log-Normal distribution are shown in Figure \@ref(fig:dlnormPlot).

```{r dlnormPlot, fig.cap="Probability Density Function of Log-Normal distribution with a variety of parameters.", echo=FALSE}
plot(seq(0,5,0.01), dlnorm(seq(0,5,0.01),0,0.1), type="l",
     ylab="Density", xlab="y", main="", lwd=2,
     xlim=c(0,4))
lines(seq(0,5,0.01), dlnorm(seq(0,5,0.01),0,0.5), col=3, lwd=2)
lines(seq(0,5,0.01), dlnorm(seq(0,5,0.01),0.5,0.1), col=4, lwd=2)
lines(seq(0,5,0.01), dlnorm(seq(0,5,0.01),0.5,0.5), col=5, lwd=2)
legend("topright", legend=c(TeX("$\\mu=0, \\sigma=0.1$"),TeX("$\\mu=0, \\sigma=0.5$"),TeX("$\\mu=0.5, \\sigma=0.1$"),TeX("$\\mu=0.5, \\sigma=0.5$")),
       lwd=2, col=c("black",3,4,5))
```

The Figure \@ref(fig:dlnormPlot) shows that with the increase of the location parameter $\mu$, the distribution shifts to the right, while with the increase of the scale parameter $\sigma$ it becomes more asymmetric with a longer right tail and its mode moves closer to zero. In fact, the skewness of the Log-Normal distribution depends solely on the value of $\sigma^2$ - the higher it is, the more skewed the distribution is. It can be calculated as:
\begin{equation}
    \mathrm{Sk}(y) = \left(e^{\sigma^2} + 2\right) \sqrt{e^{\sigma^2}-1} .
    (\#eq:LogNormalSkewness)
\end{equation}

Log-Normal distribution is supported by `dlnorm()`, `plnorm()`, `qlnorm()` and `rlnorm()` functions from `stats` package in R.


## Exponential distribution {#distributionsExponential}
We have already touched upon the Exponential distribution, when we discussed the arrival times in Poisson distribution (Section \@ref(distributionPoisson)). Exponential distribution is used in modelling time between arrivals, because it is memoryless. We mentioned earlier that if a process is memoryless, then the following holds:
\begin{equation*}
    \mathrm{P}(t > \tau_1 + \tau_2) = \mathrm{P}(t > \tau_1)\mathrm{P}(t > \tau_2) .
\end{equation*}
Exponential distribution relies on this property. It has only one parameter, the rate $\lambda$ and can be written as $\mathcal{Exp}(\lambda)$. Here is its PDF:
\begin{equation}
    f(t, \lambda) = \lambda e^{-\lambda t} ,
    (\#eq:ExponentialPMF)
\end{equation}
where $t$ is a positive number and $\lambda$ is the rate parameter. This PDF is shown in Figure \@ref(fig:dexpPlot).

```{r dexpPlot, fig.cap="Probability Density Function of Exponential distribution with several values of rate parameter $\\lambda$.", echo=FALSE}
plot(seq(0,4,0.1), dexp(seq(0,4,0.1),2), type="l",
     ylab="Density", xlab="t", main="", lwd=2,
     xlim=c(0,3))
lines(seq(0,4,0.1), dexp(seq(0,4,0.1),1), col=3, lwd=2)
lines(seq(0,4,0.1), dexp(seq(0,4,0.1),0.5), col=4, lwd=2)
legend("topright", legend=c(TeX("$\\lambda=2$"),TeX("$\\lambda=1$"),TeX("$\\lambda=0.5$")),
       lwd=2, col=c("black",3,4))
```

The plot in Figure \@ref(fig:dexpPlot) shows that there is more likely for the variable $t$ to get lower values (closer to zero) than the higher ones. The parameter $\lambda$ controls the steepness of decline of the density curve with increase of $t$: the higher the rate is, the more likely it is that the event will occur earlier and less likely that it will occur later.

The CDF of the distribution is shown in Figure \@ref(fig:pexpPlot).

```{r pexpPlot, fig.cap="Cumulative Distribution Function of Exponential distribution with a variety of rate parameters.", echo=FALSE}
plot(seq(0,4,0.1), pexp(seq(0,4,0.1),2), type="l",
     ylab="Probability", xlab="t", main="", lwd=2,
     xlim=c(0,3))
lines(seq(0,4,0.1), pexp(seq(0,4,0.1),1), col=3, lwd=2)
lines(seq(0,4,0.1), pexp(seq(0,4,0.1),0.5), col=4, lwd=2)
legend("topright", legend=c(TeX("$\\lambda=2$"),TeX("$\\lambda=1$"),TeX("$\\lambda=0.5$")),
       lwd=2, col=c("black",3,4))
```

The CDFs show how fast the probability of one is achieved with different rates. Mathematically, it is written as:
\begin{equation}
    \mathrm{F}(t, \lambda) = 1 - e^{- \lambda t}.
    (\#eq:ExponentialCDF)
\end{equation}
Based on it, we can say, for example what is the probability that an event will occur in 1.5 seconds if the rate is $\lambda=2$ per second. It is:
\begin{equation*}
    \mathrm{F}(t < 1.5, 2) = 1 - e^{- 2 \times 1.5} \approx 0.95 .
\end{equation*}

This can also be calculated in R:

```{r}
pexp(1.5, rate=2)
```

Coming back to the memorylessness property of the distribution, in order to show it visually, consider an example based on the property:
\begin{equation*}
    \mathrm{P}(t > 1.5) = \mathrm{P}(t > 0.5) \mathrm{P}(t > 1)
\end{equation*}
and $\lambda=2$. Note that $P(t>a)=1-F(a)$ by definition of CDF. In terms of probabilities of Exponential distribution, this means that:
\begin{equation*}
    \mathrm{P}(t > 1.5) = (1-\mathrm{F}(1, 2)) (1-\mathrm{F}(0.5, 2)) .
\end{equation*}
Now, in order to show this property visually, we take logarithms of the left and right hand sides of the previous equation to get:
\begin{equation}
    \log \left( \mathrm{P}(t > 1.5) \right) = \log \left(1-\mathrm{F}(1, 2) \right) + \log \left( 1-\mathrm{F}(0.5, 2)  \right) .
    (\#eq:ExponentialCDFMemory01)
\end{equation}
We take logarithms to linearise relation, because it is easier to work with. If we now insert \@ref(eq:ExponentialCDF) in \@ref(eq:ExponentialCDFMemory01), we will get:
\begin{equation}
    \log \left( \mathrm{P}(t > 1.5) \right) = \log \left(1- 1 + e^{- 2 \times 1} \right) + \log \left(1- 1 + e^{- 2 \times 0.5} \right) = -2 - 1 = -3 ,
    (\#eq:ExponentialCDFMemory02)
\end{equation}
which is obtained independently for $\log(1-F(1.5, 2))=\log(e^{-2 \times 1.5})=-3$. We can also plot the function $\log(1-F(t, 2))$ in Figure \@ref(fig:pexpPlotMemory) to show that \@ref(eq:ExponentialCDFMemory02) holds, when the values on y-axis are added.

```{r pexpPlotMemory, fig.cap="Memoryless property of Exponential distibution with $\\lambda=2$.", echo=FALSE}
plot(seq(0,4,0.1), log(1-pexp(seq(0,4,0.1),2)), type="l",
     ylab="log(1-F(t,2))", xlab="t", main="", lwd=2,
     xlim=c(0,3), ylim=c(-5,0))
# 1.5
lines(c(1.5,1.5),c(-5,log(1-pexp(1.5,2))), lty=2, lwd=2, col=17)
lines(c(-1,1.5),log(rep(1-pexp(1.5,2),2)), lty=2, lwd=2, col=17)
# 0.5
lines(c(0.5,0.5),c(-5,log(1-pexp(0.5,2))), lty=2, lwd=2, col=17)
lines(c(-1,0.5),log(rep(1-pexp(0.5,2),2)), lty=2, lwd=2, col=17)
# 1
lines(c(1,1),c(-5,log(1-pexp(1,2))), lty=2, lwd=2, col=17)
lines(c(-1,1),log(rep(1-pexp(1,2),2)), lty=2, lwd=2, col=17)
```

Note that because of the logarithm of the CDF, the function has now become linear, and the property becomes more apparent.
<!-- Accidentally, $-\log(1-F(t, \lambda))$ is called cumulative hazard function. -->

When it comes to the mean of the distribution, it is equal to:
\begin{equation}
    \mathrm{E}(t) = \frac{1}{\lambda},
    (\#eq:ExponentialMean)
\end{equation}
which makes it easy to estimate on a sample of observations -- just take the mean, and you get an estimate of the rate parameter $\lambda$. Furthermore, the rate parameter in Exponential distribution is directly related to the one in the Poisson (Section \@ref(distributionPoisson)): the parameter in the latter equals the ratio of time interval $t$ and $\lambda$:
\begin{equation}
    \lambda_P = \frac{t}{\lambda},
    (\#eq:ExponentialLambdaPoisson)
\end{equation}
where $\lambda_P$ is the parameter of the Poisson distribution. The main difference between the distributions is that Poisson explains the number of arrivals over a fixed period of time, while the Exponential represents the time between the arrivals.

Finally, the variance in Exponential distribution is calculated as:
\begin{equation}
    \mathrm{V}(t) = \frac{1}{\lambda^2},
    (\#eq:ExponentialVariance)
\end{equation}

In R, the Exponential distribution is implemented in `dexp()`, `pexp()`, `qexp()` and `rexp()` for PDF, CDF, QF and Random variables respectively.



<!-- In addition, it is possible to derive the log-versions of the Normal, $\mathcal{Laplace}$, $\mathcal{S}$, and $\mathcal{GN}$ distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows: -->
<!-- \begin{equation} -->
<!--     f_{log}(\log(y_t)) = \frac{1}{y_t} f(\log y_t). -->
<!--     (\#eq:logDistribution) -->
<!-- \end{equation} -->
<!-- They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. $\exp(\mu_{\log y,t})$ in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than $\exp(\mu_{\log y,t})$ and depends on the value of the scale parameter. It is known for log$\mathcal{N}$ and is equal to: -->
<!-- \begin{equation} -->
<!--     \mathrm{E}(y_t) = \mathrm{exp}\left(\mu_{\log y,t} + \frac{\sigma^2}{2} \right). -->
<!--     (\#eq:logNMean) -->
<!-- \end{equation} -->
<!-- However, it does not have a simple form for the other distributions. -->


<!-- ## Laplace distribution {#distributionsLaplace} -->
<!-- A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF: -->

<!-- \begin{equation} -->
<!--     f(y_t) = \frac{1}{2 s} \exp \left( -\frac{\left| y_t - \mu_{y,t} \right|}{s} \right) , -->
<!--     (\#eq:Laplace) -->
<!-- \end{equation} -->
<!-- where $s$ is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error ([MAE](#errorMeasures)): -->
<!-- \begin{equation} -->
<!--     \hat{s} = \frac{1}{T} \sum_{t=1}^T \left| y_t - \hat{\mu}_{y,t} \right| . -->
<!--     (\#eq:sLaplace) -->
<!-- \end{equation} -->

<!-- It has the shape shown on Figure \@ref(fig:dlaplacePlot). -->

<!-- ```{r dlaplacePlot, fig.cap="Probability Density Function of Laplace distribution", echo=FALSE} -->
<!-- plot(seq(-3,3,0.01),dlaplace(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of Laplace distribution") -->
<!-- abline(v=0, col="red") -->
<!-- ``` -->

<!-- Similar to the Normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3. -->

<!-- The variance of the random variable following Laplace distribution is equal to: -->
<!-- \begin{equation} -->
<!--     \sigma^2 = 2 s^2. -->
<!--     (\#eq:varianceLaplace) -->
<!-- \end{equation} -->

<!-- The `dlaplace`, `qlaplace`, `plaplace` and `rlaplace` functions from `greybox` package implement different sides of Laplace distribution in R. -->


<!-- ## S distribution -->
<!-- This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function (it is as a special case of [Generalised Normal distribution](distributionsGeneralisedNormal), when $\beta=0.5$): -->
<!-- \begin{equation} -->
<!--     f(y_t) = \frac{1}{4 s^2} \exp \left( -\frac{\sqrt{|y_t - \mu_{y,t}|}}{s} \right) , -->
<!--     (\#eq:S) -->
<!-- \end{equation} -->
<!-- where $s$ is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to: -->
<!-- \begin{equation} -->
<!--     \hat{s} = \frac{1}{2T} \sum_{t=1}^T \sqrt{\left| y_t - \hat{\mu}_{y,t} \right|} , -->
<!--     (\#eq:sS) -->
<!-- \end{equation} -->
<!-- which corresponds to the minimisation of a half of "Mean Root Absolute Error" or "Half Absolute Moment" (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?). -->

<!-- ```{r dsPlot, fig.cap="Probability Density Function of S distribution", echo=FALSE} -->
<!-- plot(seq(-3,3,0.01),ds(seq(-3,3,0.01)),type="l",ylab="Density",xlab="y",main="PDF of S distribution") -->
<!-- abline(v=0, col="red") -->
<!-- ``` -->

<!-- The variance of the random variable following S distribution is equal to: -->
<!-- \begin{equation} -->
<!--     \sigma^2 = 120 s^4. -->
<!--     (\#eq:varianceS) -->
<!-- \end{equation} -->

<!-- The `ds`, `qs`, `ps` and `rs` from `greybox` package implement the density, quantile, cumulative and random generation functions. -->


<!-- ## Generalised Normal distribution {#distributionsGeneralisedNormal} -->
<!-- Generalised Normal ($\mathcal{GN}$) distribution (as the name says) is a generalisation for Normal distribution, which also includes Laplace and S as special cases [@Nadarajah2005]. There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF: -->
<!-- \begin{equation} -->
<!--     f(y_t) = \frac{\beta}{2 s \Gamma(\beta^{-1})} \exp \left( -\left(\frac{|y_t - \mu_{y,t}|}{s}\right)^{\beta} \right), -->
<!--     (\#eq:GND) -->
<!-- \end{equation} -->
<!-- where $\beta$ is the shape parameter, and $s$ is the scale of the distribution, which, when estimated via MLE, is equal to: -->
<!-- \begin{equation} -->
<!--     \hat{s} = \sqrt[^{\beta}]{\frac{\beta}{T} \sum_{t=1}^T\left| y_t - \hat{\mu}_{y,t} \right|^{\beta}}, -->
<!--     (\#eq:sGND) -->
<!-- \end{equation} -->
<!-- which has MSE, MAE and HAM as special cases, when $\beta$ is equal to 2, 1 and 0.5 respectively. The parameter $\beta$ influences the kurtosis directly, it can be calculated for each special case as $\frac{\Gamma(5/\beta)\Gamma(1/\beta)}{\Gamma(3/\beta)^2}$. The higher $\beta$ is, the lower the kurtosis is. -->

<!-- The advantage of $\mathcal{GN}$ distribution is its flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter $\beta$ is fractional and close to zero. Alternatively, when $\beta \rightarrow \infty$, the distribution converges point-wise to the uniform distribution on $(\mu_{y,t} - s, \mu_{y,t} + s)$. -->

<!-- Note that the estimation of $\beta$ is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality. -->

<!-- Depending on the value of $\beta$, the distribution can have different shapes shown in Figure \@ref(fig:dgnormPlot) -->

<!-- ```{r dgnormPlot, fig.cap="Probability Density Functions of Generalised Normal distribution", echo=FALSE} -->
<!-- plot(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Generalised Normal distribution", -->
<!--      ylim=c(0,0.6)) -->
<!-- lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1),col="darkblue") -->
<!-- lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,2),col=3) -->
<!-- lines(seq(-3,3,0.01),dgnorm(seq(-3,3,0.01),0,1,1000),col="purple") -->
<!-- abline(v=0, col="red") -->
<!-- legend("topright",legend=c(TeX("$\\beta =0.5$"),TeX("$\\beta =1$"),TeX("$\\beta =2$"),TeX("$\\beta =1000$")), -->
<!--        col=c("black","darkblue",3,"purple"),lwd=1) -->
<!-- ``` -->

<!-- Typically, estimating $\beta$ consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function \@ref(eq:GND). -->

<!-- The variance of the random variable following Generalised Normal distribution is equal to: -->
<!-- \begin{equation} -->
<!--     \sigma^2 = s^2\frac{\Gamma(3/\beta)}{\Gamma(1/\beta)}. -->
<!--     (\#eq:varianceGN) -->
<!-- \end{equation} -->

<!-- The working functions for the Generalised Normal distribution are implemented in the `greybox` package for R. -->


<!-- ## Asymmetric Laplace distribution {#distributionsALaplace} -->

<!-- Asymmetric Laplace distribution ($\mathcal{AL}$) can be considered as a two Laplace distributions with different parameters $s$ for left and right sides from the location $\mu_{y,t}$. There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter $\alpha$ [@Yu2005]: -->
<!-- \begin{equation} -->
<!--     f(y_t) = \frac{\alpha (1- \alpha)}{s} \exp \left( -\frac{y_t - \mu_{y,t}}{s} (\alpha - I(y_t \leq \mu_{y,t})) \right) , -->
<!--     (\#eq:ALaplace) -->
<!-- \end{equation} -->
<!-- where $s$ is the scale parameter, $\alpha$ is the skewness parameter and $I(y_t \leq \mu_{y,t})$ is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter $s$ estimated using likelihood is equal to the quantile loss: -->
<!-- \begin{equation} -->
<!--     \hat{s} = \frac{1}{T} \sum_{t=1}^T \left(y_t - \hat{\mu}_{y,t} \right)(\alpha - I(y_t \leq \hat{\mu}_{y,t})) . -->
<!--     (\#eq:sALaplace) -->
<!-- \end{equation} -->
<!-- Thus maximising the likelihood \@ref(eq:ALaplace) is equivalent to estimating the model via the minimisation of $\alpha$ quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is $\epsilon_t \sim \mathcal{AL}(0, s, \alpha)$ [@Geraci2007]. -->

<!-- Depending on the value of $\alpha$, the distribution can have different shapes, shown in Figure \@ref(fig:dALaplacePlot). -->

<!-- ```{r dALaplacePlot, fig.cap="Probability Density Functions of Asymmetric Laplace distribution", echo=FALSE} -->
<!-- plot(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.5),type="l",ylab="Density",xlab="y",main="PDF of Asymmetric Laplace distribution", -->
<!--      ylim=c(0,0.6)) -->
<!-- lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.2),col="darkblue") -->
<!-- lines(seq(-3,3,0.01),dalaplace(seq(-3,3,0.01),0,1,0.8),col=3) -->
<!-- abline(v=0, col="red") -->
<!-- legend("topright",legend=c(TeX("$\\alpha =0.5$"),TeX("$\\alpha =0.2$"),TeX("$\\alpha =0.8$")), -->
<!--        col=c("black","darkblue",3),lwd=1) -->
<!-- ``` -->

<!-- Similarly to $\mathcal{GN}$ distribution, the parameter $\alpha$ can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution. -->

<!-- The variance of the random variable following Asymmetric Laplace distribution is equal to: -->
<!-- \begin{equation} -->
<!--     \sigma^2 = s^2\frac{(1-\alpha)^2+\alpha^2}{\alpha^2(1-\alpha)^2}. -->
<!--     (\#eq:varianceALaplace) -->
<!-- \end{equation} -->

<!-- Functions `dalaplace`, `qalaplace`, `palaplace` and `ralaplace` from `greybox` package implement the Asymmetric Laplace distribution. -->

<!-- ## Log Laplace, Log S and Log GN distributions -->

<!-- In addition, it is possible to derive the log-versions of the Normal, $\mathcal{Laplace}$, $\mathcal{S}$, and $\mathcal{GN}$ distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows: -->
<!-- \begin{equation} -->
<!--     f_{log}(\log(y_t)) = \frac{1}{y_t} f(\log y_t). -->
<!--     (\#eq:logDistribution) -->
<!-- \end{equation} -->
<!-- They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. $\exp(\mu_{\log y,t})$ in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than $\exp(\mu_{\log y,t})$ and depends on the value of the scale parameter. It is known for log$\mathcal{N}$ and is equal to: -->
<!-- \begin{equation} -->
<!--     \mathrm{E}(y_t) = \mathrm{exp}\left(\mu_{\log y,t} + \frac{\sigma^2}{2} \right). -->
<!--     (\#eq:logNMean) -->
<!-- \end{equation} -->
<!-- However, it does not have a simple form for the other distributions. -->

<!-- ## Inverse Gaussian distribution {#IGDistribution} -->

<!-- An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian ($\mathcal{IG}$), which is parameterised using mean value $\mu_{y,t}$ and either the dispersion parameter $s$ or the scale $\lambda$ and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following: -->
<!-- \begin{equation} -->
<!--     \text{if } (1+\epsilon_t) \sim \mathcal{IG}(1, s) \text{, then } -->
<!--     y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{IG}\left(\mu_{y,t}, \frac{s}{\mu_{y,t}} \right), -->
<!--     (\#eq:InverseGaussianModel) -->
<!-- \end{equation} -->
<!-- implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of $1+\epsilon_t$ is: -->

<!-- \begin{equation} -->
<!--     f(1+\epsilon_t) = \frac{1}{\sqrt{2 \pi s (1+\epsilon_t)^3}} \exp \left( -\frac{\epsilon_t^2}{2 s (1+\epsilon_t)} \right) , -->
<!--     (\#eq:InverseGaussian) -->
<!-- \end{equation} -->
<!-- where the dispersion parameter can be estimated via maximising the likelihood and is calculated using: -->
<!-- \begin{equation} -->
<!--     \hat{s} = \frac{1}{T} \sum_{t=1}^T \frac{e_t^2}{1+e_t} , -->
<!--     (\#eq:InverseGaussianDispersion) -->
<!-- \end{equation} -->
<!-- where $e_t$ is the estimate of $\epsilon_t$. This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive. -->

<!-- Figure \@ref(fig:dIGPlot) shows how the PDF of $\mathcal{IG}(1,s)$ looks for different values of the dispersion $s$ -->

<!-- ```{r dIGPlot, fig.cap="Probability Density Functions of Inverse Gaussian distribution", echo=FALSE} -->
<!-- plot(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=10),type="l",ylab="Density",xlab="y",main="PDF of Inverse Gaussian distribution") -->
<!-- lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=1),col="darkblue") -->
<!-- lines(seq(0,6,0.01),dinvgauss(seq(0,6,0.01),1,dispersion=0.1),col=3) -->
<!-- abline(v=1, col="red") -->
<!-- legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")), -->
<!--        col=c("black","darkblue",3),lwd=1) -->
<!-- ``` -->

<!-- `statmod` package implements density, quantile, cumulative and random number generator functions for the $\mathcal{IG}$. -->


<!-- ## Gamma distribution {#GammaDistribution} -->
<!-- Finally, another distribution that will be useful for [ETS](#ADAMETSIntroduction) and [ARIMA](#ADAMARIMA) is Gamma ($\mathcal{\Gamma}$), which is parameterised using shape $\xi$ and scale $s$, and is defined for positive values only. This distribution is useful because it is scalable and is as flexible as ($\mathcal{IG}$) in terms of possible shapes. It also has an important scalability property (simila to $\mathcal{IG}$), but the shape needs to be restricted in order to make sense in ETS model: -->
<!-- \begin{equation} -->
<!--     \text{if } (1+\epsilon_t) \sim \mathcal{\Gamma}(s^{-1}, s) \text{, then } -->
<!--     y_t = \mu_{y,t} \times (1+\epsilon_t) \sim \mathcal{\Gamma}\left(s^{-1}, s \mu_{y,t} \right), -->
<!--     (\#eq:GammaModel) -->
<!-- \end{equation} -->
<!-- implying that the scale of the model changes together with the expectation. The restriction on the shape parameters is needed in order to make the expectation of $(1+\epsilon_t)$ equal to one. The PDF of the distribution of $1+\epsilon_t$ is: -->

<!-- \begin{equation} -->
<!--     f(1+\epsilon_t) = \frac{1}{\Gamma(s^{-1}) (s)^{s^{-1}}} (1+\epsilon_t)^{s^{-1}-1}\exp \left(-\frac{1+\epsilon_t}{s}\right) . -->
<!--     (\#eq:Gamma) -->
<!-- \end{equation} -->
<!-- However, the scale $s$ cannot be estimated via the maximisation of likelihood analytically due to the restriction \@ref(eq:GammaModel). Luckliy, the method of moments can be used instead, where based on the expectation and variance we get: -->
<!-- \begin{equation} -->
<!--     \hat{s} = \frac{1}{T} \sum_{t=1}^T e_t^2 , -->
<!--     (\#eq:GammaScale) -->
<!-- \end{equation} -->
<!-- where $e_t$ is the estimate of $\epsilon_t$. So, imposing the restrictions \@ref(eq:GammaModel) implies that the scale of $\mathcal{\Gamma}$ is equal to the variance of the error term. -->

<!-- Figure \@ref(fig:dGammaPlot) demonstrates how the PDF of $\mathcal{\Gamma}(s^{-1},s)$ looks for different values of $s$: -->

<!-- ```{r dGammaPlot, fig.cap="Probability Density Functions of Gamma distribution", echo=FALSE} -->
<!-- plot(seq(0,6,0.01),dgamma(seq(0,6,0.01),0.1,scale=10),type="l",ylab="Density",xlab="y",main="PDF of Gamma distribution") -->
<!-- lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),1,scale=1),col="darkblue") -->
<!-- lines(seq(0,6,0.01),dgamma(seq(0,6,0.01),10,scale=0.1),col=3) -->
<!-- abline(v=1, col="red") -->
<!-- legend("topright",legend=c(TeX("$s =10$"),TeX("$s =1$"),TeX("$s =0.1$")), -->
<!--        col=c("black","darkblue",3),lwd=1) -->
<!-- ``` -->

<!-- With the increase of the shape $\xi=s^{-1}$ (in our case this implies the decrease of variance $s$), $\mathcal{\Gamma}$ distribution converges to the normal one with $\mu=\xi s=1$ and variance $\sigma^2=s$. This demonstrates indirectly that the estimate of the scale \@ref(eq:GammaScale) maximises the likelihood of the function \@ref(eq:Gamma), although I do not have any proper proof of this. -->


<!-- ## Beta distribution -->

## Convolution of distributions {#distributionsConvolution}
Whenever we want to add or take a mean of two random variable, and want to know what we get a result, we are dealing with the operation called in mathematics "convolution". By doing convolution we combine two random processes to form a new one.


### Convolution of two discrete distributions {#distributionsConvolutionDiscrete}
To explain it better, we take a step back and consider a case of rolling dices. As we discussed, in that case we are dealing wi the uniform distribution (Section \@ref(distributionUniform)): it is equally probably to get 1, 2, 3, 4, 5 or 6 when you roll a dice once. Now, what would happen if we roll two dices at the same time?

Let us record the value on one dice as $x$, and the value on the second one as $y$. Rolling both of them, means that we need to add $x$ to $y$ to get a new score $z$. For example, we had 3 on one dice and 5 on the other one, which means that $x=3$, $y=5$ and $z=x+y=3+5=8$. The span of values of $z$ is now between 2 and 12, because the lowest score is 1 (thus $1+1=2$), while the highest one is 6 ($6+6=12$).

But what happens with the probability distribution? Remember, it was uniform for both dices and looked like this (Figure \@ref(fig:uniformPMFRepeated)):
```{r uniformPMFRepeated, fig.cap="Probability Mass Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(rep(1/6,6), ylim=c(0,0.3),
                ylab="Probability",xlab="x", col=11)
axis(side=1,at=test,labels=c(1:6))
```

To get the PMF of the new convolved distribution, we need to assign probabilities for each of the new combined values. Getting the score of 2 is only possible if we have 1 on each dice, which corresponds to the probability of $\frac{1}{6}$ for each of them, meaning that this can happen only in $\frac{1}{6} \times \frac{1}{6} = \frac{1}{36}$ cases. The score $z=3$ is possible if we get either $x=1$ and $y=2$ or $x=2$ and $y=1$. This translates to the probability of $\frac{1}{36} + \frac{1}{36} = \frac{2}{36} = \frac{1}{18}$. We could continue with other distributions, obtaining probability of each outcome from $z=2$ to $z=12$.

One way of looking at this process, is to first acknowledge that $y=z-x$ and then by sliding one distribution above the other by changing the value of $x$. For example, to have $z=3$ we can have $x=1$ and $y=3-1=2$, or $x=2$ and $y=3-2=1$. This corresponds to the sliding shown in Figure \@ref(fig:uniformPMFSlide01), where the red distribution is placed above the blue one and in this specific example, we get an intersection of two bars, each of which gives us the same $z=3$. The probabilities of each bar are $\frac{1}{6}$, the intersection gives us the multiplication of those probabilities (thus $\frac{1}{36}$) and we have two bars that have these intersections, so the final answer is the same as we obtained above: $\frac{1}{36} + \frac{1}{36} = \frac{1}{18}$.

```{r uniformPMFSlide01, fig.cap="Convolution of two PMFs for 1d6.", echo=FALSE}
x <- c(1:6)
y <- c(rep(0,4),table(12-x+1))

test1 <- barplot(table(x)/6,
                 ylim=c(-0.1,0.3), xlim=c(0,12),
                 col=9,
                 axes=F, names.arg="")
test2 <- barplot(y/6,
                 ylim=c(-0.1,0.3), xlim=c(0,12),
                 col=11,
                 axes=F, names.arg="",
                 ylab="Probability",xlab="x",
                 add=T)
abline(h=0, lwd=2)
text(test1, 0, labels=c(6:1), pos=1, col=3)
text(test2[-c(1:4)], 1/6, labels=c(1:6), pos=3, col=5)
text(test1[c(5:6)], 1/12, labels=rep(3,2), col=2)
```

The more we move the red bars to the right, the more intersections we will have, giving us higher and higher probability of occurrence until we reach the moment shown in Figure \@ref(fig:uniformPMFSlide02).

```{r uniformPMFSlide02, fig.cap="Convolution of two PMFs for 1d6.", echo=FALSE}
x <- c(1:6)
y <- table(12-x+1)

test1 <- barplot(table(x)/6,
                 ylim=c(-0.1,0.3), xlim=c(0,7.3),
                 col=9,
                 axes=F, names.arg="")
test2 <- barplot(y/6,
                 ylim=c(-0.1,0.3), xlim=c(0,7.3),
                 col=11,
                 axes=F, names.arg="",
                 ylab="Probability",xlab="x",
                 add=T)
abline(h=0, lwd=2)
text(test1, 0, labels=c(6:1), pos=1, col=3)
text(test2, 1/6, labels=c(1:6), pos=3, col=5)
text(test1[c(1:6)], 1/12, labels=rep(7,6), col=2)
```

In this specific example, we can obtain $z=7$ 6 different ways, which means that the probability of that outcome is $\frac{1}{36} \times 6 = \frac{1}{6}$. This is the highest probability outcome if you roll two 6 sided dices (2d6). But that's not all, after that, we can continue sliding the distribution, and we will notice that the probability of outcomes starts declining as well (Figure \@ref(fig:uniformPMFSlide03)), because we have fewer and fewer intersections between the distributions that would give us the specific scores.

```{r uniformPMFSlide03, fig.cap="Convolution of two PMFs for 1d6.", echo=FALSE}
x <- c(1:6)
y <- c(rep(0,2),table(12-x+1))
test2 <- barplot(y/6,
                 ylim=c(-0.1,0.3), xlim=c(0,12),
                 col=9,
                 axes=F, names.arg="")
test1 <- barplot(table(x)/6,
                 ylim=c(-0.1,0.3), xlim=c(0,12),
                 col=11,
                 axes=F, names.arg="",
                 ylab="Probability",xlab="x",
                 add=T)
abline(h=0, lwd=2)
text(test1, 0, labels=c(6:1), pos=1, col=3)
text(test2[-c(1:2)], 1/6, labels=c(1:6), pos=3, col=5)
text(test1[c(3:6)], 1/12, labels=rep(5,4), col=2)
```

Collecting all the probabilities after such sliding, we can visualise the PMF of $z$ (convolution of two uniform distributions), which is shown in Figure \@ref(fig:uniformPMFCombined), where we decided not to simplify the ratios for simplicity.

```{r uniformPMFCombined, fig.cap="Probability Mass Function of two Uniform distributions for 1d6.", echo=FALSE}
apply(expand.grid(1:6,1:6), 1, sum) |> table() -> z
test <- barplot(z/sum(z),
                ylim=c(0,0.3),
                col=9, axes=F, names.arg="")
barplot(z/sum(z),
        ylim=c(0,0.3),
        ylab="Probability",xlab="z", col=11, add=T)
text(test, z/sum(z), labels=TeX(paste0("$\\frac{",z,"}{",sum(z),"}$")), pos=3)
```

### Convolution of two continuous distributions {#distributionsConvolutionContinuous}
The logic with convolution of two continuous distributions is similar to the one for the discreet, but instead of adding bars, we need to calculate the sizes of the intersected surfaces. Mathematically, this is done by taking an integral (see details in Subsection \@ref(distributionsConvolutionMaths)).

Visually, this is shown in Figure \@ref(fig:normalConvolution) for the sum of two variables following the identical Standard Normal distributions (zero mean and unit variance).

```{r normalConvolution, fig.cap="Convolution of two standard Normal distributions.", fig.width=7, fig.height=10, echo=FALSE}
mu1 <- c(-3,-1.5,0,1.5,3)
mu2 <- 0
x <- seq(-10,10,0.01)
mains <- c("A","B","C","D","E")

layout(matrix(1:(5*2), 5, 2, byrow=T),
       widths=c(7,3)/10)
for(i in 1:length(mu1)){
    par(mar=c(3,2,2,0))
    plot(seq(-3,3,0.1),dnorm(seq(-3,3,0.1), mu1[i], 1),
         type="l",ylab="Density",xlab="y",main="",
         xlim=c(-5,5), ylim=c(0,0.4), axes=F)
    polygon(c(x,rev(x)),
            c(dnorm(x, mu1[i], 1), rep(0,length(x))), col=9)
    lines(seq(-3,3,0.1),dnorm(seq(-3,3,0.1), mu2))
    polygon(c(x,rev(x)),
            c(dnorm(x, mu2, 1), rep(0,length(x))), col=11)
    text(4.5, 0.35, mains[i], cex=1.25)
    axis(2)
    box()
    
    z <- seq(-10,mu1[i],0.1)
    par(mar=c(3,1,2,2))
    plot(z, dnorm(z, mu2, sqrt(2)),
         type="l",ylab="Density",xlab="y",main="",
         xlim=c(-5,5), ylim=c(0,0.4), axes=F)
    polygon(c(z,rev(z)),
            c(dnorm(z, mu2, sqrt(2)), rep(0,length(z))), col=18)
    lines(c(mu1[i],mu1[i]), c(0,dnorm(mu1[i], mu2, sqrt(2))), lwd=3, lty=1,
          col=rgb((0.9*0.5+0.6*0.5), (0.6*0.5+0.8*0.5), (0.6*0.5+1*0.5), 0.75))
    axis(1)
    axis(4)
    box()
}
```

Figure \@ref(fig:normalConvolution) shows five special cases of the sliding with the red distribution moving over the blue one. The purple area where distributions intersect corresponds to the probability density for each specific value of $z=x+y$, which on the right of each plot is shown with a vertical purple line. We can see that this area reaches its maximum when two distributions coincide. This corresponds to the peak in the new distribution.

There is a mathematical proof that a sum of two variables $x$ and $y$ following Normal distributions with some means $\mu_x$ and $\mu_y$ and variances $\sigma_x^2$ and $\sigma_y^2$ is another Normal distribution with $\mu_z=\mu_x+\mu_y$ and $\sigma_z^2=\sigma_x^2+\sigma_y^2 + 2 \sigma_{x,y}$, where $\sigma_{x,y}$ is the covariance between them. If the two distributions are independent, the covariance becomes equal to zero. If the two distributions are identical (as in Figure \@ref(fig:normalConvolution)) with zero means and unit variances, the resulting distribution will be $\mathcal{N}(0, 2)$, which we can see in the right-hand sides of the image.

So far, we looked at the convolution of the symmetric distributions. But the situation would be a bit different for the asymmetric ones, because in reality the distribution that we slide over needs to be flipped vertically to reflect the idea we discussed in case of the convolution of discrete distributions in Subsection \@ref(distributionsConvolutionDiscrete): there is a multitude of values of $x$ and $y$ that give a specific value of $z$. For the continuous distribution, for example, the number $z=4$ can be achieved by $x=2$ and $y=2$, by $x=2.1$, $y=1.9$, by $x=2.0000001$ and $y=1.9999999$, by $x=10$, $y=-6$, etc - the number of possible combinations is infinite. But not all of them are equally possible, which is reflected by the PDFs of $x$ and $y$.

To see the idea of flipping clearer, consider two Exponential distributions (discussed in Section \@ref(distributionsExponential)) with $\lambda=1$. Three specific points during the sliding would look like those shown in Figure \@ref(fig:expConvolution). They correspond to situations when $z=1$, $z=3$ and $z=5$.

```{r expConvolution, fig.cap="Convolution of two Exponential distributions.", fig.width=7, fig.height=6, echo=FALSE}
mu2 <- 1
mu1 <- c(1, 3, 5)
x <- seq(0,10,0.01)

layout(matrix(1:(length(mu1)*2), length(mu1), 2, byrow=T),
       widths=c(7,3)/10)
for(i in 1:length(mu1)){
    par(mar=c(4,2,2,0))
    plot(x,dexp(x, mu2),
         type="l",ylab="Density",xlab="",main="",
         xlim=c(-2,5), ylim=c(0,1), axes=F)
    polygon(c(x,rev(x)),
            c(dexp(x, mu2), rep(0,length(x))), col=9)
    lines(mu1[i]-x,dexp(x, mu2))
    polygon(c(mu1[i]-x,rev(mu1[i]-x)),
            c(dexp(x, mu2), rep(0,length(x))), col=11)
    axis(2)
    text(4, 0.85, paste0("z=",mu1[i]), cex=1.25)
    box()
    
    z <- seq(0,mu1[i],0.1)
    par(mar=c(4,1,2,2))
    plot(z, dgamma(z, shape=2, rate=1),
         type="l",ylab="Density",xlab="z",main="",
         xlim=c(0,10), ylim=c(0,0.4), axes=F)
    polygon(c(z,rev(z)),
            c(dgamma(z, shape=2, rate=1), rep(0,length(z))), col=18)
    lines(c(mu1[i],mu1[i]), c(0,dgamma(mu1[i], shape=2, rate=1)), lwd=3, lty=1,
          col=rgb((0.9*0.5+0.6*0.5), (0.6*0.5+0.8*0.5), (0.6*0.5+1*0.5), 0.75))
    axis(1)
    axis(4)
    text(mu1[i], 0.015, mu1[i], pos=4, cex=1.25)
    box()
}
```

For that case, the resulting distribution would not be symmetric. It would have values starting from zero (because there is no intersection between the red and the blue areas for negative values), have a long right tail and a hump, not too far from zero.

Similar mechanism is used for convolutions of other distributions. The interesting point, which we will come back to in Chapter \@ref(PopulationSampling), is that if many identical distributions are convolved (so we take sum of many random variables that follow those distributions), the resulting distribution would be Normal. In the examples in this Chapter, this becomes apparent even with the uniform distribution: if we convolve the distribution $z$ with another discrete uniform distribution, the resulting one would be symmetric and it would start looking closer and closer to the Normal distribution. This, for example, means, that if you roll 30 6-sided dice, the distribution of the outcome could be approximated by the Normal one.


### Mathematics behind convolutions {#distributionsConvolutionMaths}
If we have two random variables $x$ and $y$ following continuous distributions $f(x)$ and $g(y)$ respectively, their sum $z=x+y$ (convolution) would follow a distribution with PDF $h(z)$, which can be written as:
\begin{equation}
    h(z) = (f * g)(z) := \int_{-\infty}^{\infty}f(x)g(z-x) dx .
    (\#eq:convolution)
\end{equation}
The sliding that we discussed in the previous subsections, in this formula is done by the element $g(z-x)$, which takes values from the right tail of distribution due to $z-x$.


