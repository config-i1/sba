# Discrete distributions {#countDistributions}
In this chapter we discuss the idea of discrete distributions, their properties and then move to the discussion of specific examples: Bernoulli, Binomial, Poisson, Geometric and Negative Binomial distributions.


## What is discrete distribution?
A random variable discussed in Section \@ref(whatIsRandomVariable) can take a variety of values. For now we focus on discrete random variable, which means that it can take one of the several possible values with some probabilities. For example, if we roll two six-sided dices, we will have a variety of outcomes from 2 (both having score of one) to 12 (both having score of six), but not all outcomes will have equal probability. For example, there are different ways of obtaining score of 7: 1+6, 2+5, 3+4, 4+3, 5+2 and 6+1 - but there is only one way of obtaining 2: 1+1. In this case, we are dealing with a distribution of values from 2 to 12, where each value has its own probability of occurrence. This situation is shown in Figure \@ref(fig:distributionDice12).

```{r distributionDice12, echo=FALSE, fig.cap="Distribution of outcomes for scores based on two dices."}
y <- c(1:6,5:1)/36
names(y) <- c(2:12)
yBarplot <- barplot(y, xlab="Dice score", ylab="Probability of outcome", col="grey95")
axis(1, at=yBarplot, labels=FALSE)
```

As can be seen from Figure \@ref(fig:distributionDice12), the distribution of probabilities in this case is symmetric, the chances of having very low and very high scores are lower than the chance of having something closer to the middle. The probability of having 7 is the highest and is $\frac{6}{36}=\frac{1}{6}$, which means that it will occur more often than other values if we repeat the experiment and roll the dices many times.

Any discrete distribution can be characterised using the following functions:

1. Probability Mass Function (PMF);
2. Cumulative Distribution Function (CDF);
4. Moment Generation Function (MMF);
5. Characteristic function (CF).

PMF is the function of probability of occurrence from specific values of random variable. An example of PMF is shown in Figure \@ref(fig:distributionDice12). Based on it, we can say what the probability of a specific outcome is for the random variable.

CDF shows the probability of the event lower than the specified one. For example, the probability of getting the score lower than 4 is $\frac{1}{36}+\frac{2}{36}=\frac{1}{12}$, which corresponds to the sum of the first two bars in Figure \@ref(fig:distributionDice12). The CDF for our example is shown in Figure \@ref(fig:distributionDice12CDF).

```{r distributionDice12CDF, echo=FALSE, fig.cap="Cumulative distribution of outcomes for scores based on two dices."}
yBarplot <- barplot(cumsum(y), xlab="Cumulative dice score", ylab="Probability of outcome", col="grey95")
axis(1, at=yBarplot, labels=FALSE)
```

Any CDF is equal to zero for the values below possible (e.g. it is impossible to get score of 1 rolling two dices) and is equal to one for the values at and above the maximum (if we roll two dices, the score will be below 13). Given that CDF shows probabilities, it can never be greater than one or lower than zero.

Finally, MGF and CF are the functions that allow obtaining the moments of distributions, such as mean, variance, skewness etc. We do not discuss these functions in detail in this textbook, and we will discuss the moments later in the Section \@ref(dataAnalysisNumerical).

Because we considered the discrete random variable, the distribution shown in Figure \@ref(fig:distributionDice12) is discrete as well.


## Tossing a coin -- Bernoulli distribution {#distributionBernoulli}
Consider a case when we track whether it rains at Lancaster or not and want to understand based on this information whether it will rain today or not. In this example, we have two values for a random variable (1 - it rains, 0 - it does not), and if we do not have any additional information, we assume that the probability that it rains is fixed. This situation can be modelled using the Bernoulli distribution.

It is one of the simplest distributions. It can be used to characterise the situation, when there are only two outcomes of event, like the classical example with coin tossing. In this special case, according to this distribution, the random variable can only take two values: zero (e.g. for heads) and one (e.g. tails) with a probability of having tails equal to $p=0.5$. It is a useful distribution not only for the coin experiment, but for any other experiment with two outcomes and some probability $p$. For example, consumers behaviour when making a choice whether to buy a product or not can be modelled using Bernoulli distribution: we do not know what a consumer will choose and why, so based on some external information we can assign them a probability of purchase $p$.

In general, the distribution can be characterised with the following PMF:
\begin{equation}
    f(y, p) = p^y (1-p)^{1-y},
    (\#eq:BernoulliPMF)
\end{equation}
where $y$ can only take values of 0 and 1. Figure \@ref(fig:bernoulliPMF) demonstrates how the PMF \@ref(eq:BernoulliPMF) looks for several probabilities.

```{r bernoulliPMF, fig.cap="Probability Mass Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(dbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8", col="grey95")
axis(side=1,at=test,labels=c(0,1))
```

The mean of this distribution equals to $p$, which is in practice used in the estimation of the probability of occurrence $p$: collecting a vector of zeroes and ones and then taking the mean will give the empirical probability of occurrence $\hat{p}$. The variance of Bernoulli distribution is $p\times (1-p)$.

Finally, the CDF of the distribution is:
\begin{equation}
    F(y, p) = \left\{ \begin{aligned}
                            1-p, & \text{ for } y=0 \\
                            1,   & \text{ for } y=1 .
                    \end{aligned} \right.
    (\#eq:BernoulliCDF)
\end{equation}
which can be plotted as shown in Figure \@ref(fig:bernoulliCDF).

```{r bernoulliCDF, fig.cap="Cumulative Distribution Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(pbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8", col="grey95")
axis(side=1,at=test,labels=c(0,1))
```

The CDF of Bernoulli distribution is seldom used in practice and is provided here for completeness.

::: task
While sitting at home during the COVID pandemic isolation, Vasiliy conducted an experiment: he threw paper balls in a rubbish bin located in the far corner of his room. He counted how many times he missed and how many times he got the balls in the bin. It was 36 to 64.

1. What is the probability distribution that describes this experiment?
2. What is the probability that Vasiliy will miss when he throws the next ball?
3. What is the variance of his throws?
:::

::: solution
This is an example of Bernoulli distribution: it has two outcomes and a probability of success.

We will encode the miss as zero and the score as one. Based on that, taking the mean of the outcomes, we can estimate the mean of Bernoulli probability of miss:
\begin{equation*}
    \bar{y} = \hat{p} = \frac{36}{100} = 0.36.
\end{equation*}
So, when Vasiliy throws the next ball in the bin, he will miss with the probability of 0.36.

The variance is $p \times (1-p) = 0.36 \times 0.64 = 0.2304$.
:::

In R, this distribution is implemented in `stats` package via `dbinom(size=1)`, `pbinom(size=1)`, `qbinom(size=1)` and `rbinom(size=1)` for PDF, CDF, QF and random generator respectively. The important parameter in this case is `size=1`, which will be discussed in Section \@ref(distributionBinomial).


::: remark
A nice example of a task using Bernoulli distribution is the Saint Petersburg Paradox [@Kotz2005, page 8318]. The idea of it is as follows. Imagine that I offer you to play a game. I will toss the coin as many times as needed to get first heads. We will calculate how many tails I had in that tossing and I will pay you an amount of money, depending on that number. If I toss tails once, I will pay you £2. If I toss it twice, I will pay £$2^2=4$. In general, I will pay £$2^n$ if I toss consecutively $n$ tails before getting heads. The question of this task, is how much you will be ready to pay to enter such game (i.e. what is the fair price?). Daniel Bernoulli proposed that the fair price can be calculated via the expectation of prize, which in this case is:
\begin{equation*}
    \mathrm{E}(\text{tails }n\text{ times and heads on }n+1) = \sum_{j=1}^\infty 2^{j} \left(\frac{1}{2}\right)^{j+1} 
\end{equation*}
The logic behind this formula is that mathematically, we assume that we can have infinite number of experiments, and each prize has its outcome. For example, the probability to get just £2 is $\frac{1}{2}$, while the probability to get £4 is $\frac{1}{4}$ etc. But the values cancel each other out in this formula leaving us with:
\begin{equation*}
    \mathrm{E}(\text{tails }n\text{ times and heads on }n+1) = \sum_{j=1}^\infty \frac{1}{2} = \infty .
\end{equation*}
So, although it is unrealistic to expect in real life that the streak of tails will continue indefinitely, the statistics theory tells us that the fair price for the game is infinity. Practically speaking, the infinite amount of tails will never happen, so we should have a finite number for the price. But mathematics assumes that the experiment can be repeated infinite amount of times, and in this case it is entirely possible that we will observe an infinite streak of tails. This is the Saint Petersburg paradox, which demonstrates how sometimes the asymptotic properties relate to reality. I think that it provides a good demonstration of what statistics typically works with.
:::
Finally, coming back to the rain example, we cannot say for sure whether it will rain tomorrow or not, but based on the collected sample, we can calculate the probability of that event. But remember that even if the probability is low, it does not mean that you do not need to bring an umbrella with you.


## Multiple coin tosses -- Binomial distribution {#distributionBinomial}
In the previous example with coin tossing we focused on the experiment that contained only one step: toss a coin, see what score you got (zero or one). However, we could make the game more complicated and do it in, let us say, 10 trials. In this more complicated experiment we would sum up the scores to see what we get after those 10 trials. In theory, we can have any integer number from zero to ten, but the resulting score will be random and its chance of appearance will vary with the score. For example, we will get 0 only if in all 10 trials we get heads, while we can get 1 in a variety of ways: 1. first trial is one and then the rest are zero; 2. second trial is one and the rest are zero, etc. This means that the score in this experiment will have a distribution of its own. But can we describe it somehow?

Yes, we can. The distribution that underlies this situation is called "Binomial". For the coin tossing experiment with 10 trials and $p=0.5$ it looks like one shown in Figure \@ref(fig:binomialPMF05).

```{r binomialPMF05, fig.cap="Probability Mass Function for Binomial distribution with p=0.5 and n=10.", echo=FALSE}
test <- barplot(dbinom(c(0:10), size=10, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:10))
```

From the Figure \@ref(fig:binomialPMF05), we can see that the most probable outcome is the score of 5. This is because there are more ways of getting 5 than getting 4 in our example. In fact the number of ways can be calculated using **Binomial coefficient**, which is defined as:
\begin{equation}
    \begin{pmatrix} n \\ k \end{pmatrix} = \frac{n!}{k!(n-k)!},
    (\#eq:BinomialCoefficient)
\end{equation}
where $k$ is the score of interest, $n$ is the number of trials and $!$ is the symbol for factorial. In our example, $k=5$ and $n=10$, meaning that we have:
\begin{equation*}
    \begin{pmatrix} 10 \\ 5 \end{pmatrix} = \frac{10!}{5!(10-5)!} ,
\end{equation*}
which can be calculated in R as:
```{r eval=FALSE}
factorial(10)/(factorial(5)^2)
```
and is equal to 252. Using this formula, we can calculate other scores for comparison:
```{r}
factorial(10)/(factorial(c(0:10))*factorial(10-c(0:10))) |>
    setNames(nm=c(0:10))
```
Given that we throw the coin 10 times, there are overall $2^{10}=1024$ theoretical of outcomes of this experiment, which allows calculating the probability of each outcome:
```{r}
round((factorial(10) /
           (factorial(c(0:10))*factorial(10-c(0:10)))) /
          1024,3) |>
    setNames(nm=c(0:10))
```
These probabilities can be obtained via the PMF of Binomial distribution:
\begin{equation}
    f(k, n, p) = \begin{pmatrix} n \\ k \end{pmatrix} p^k (1-p)^{n-k} .
    (\#eq:BinomialPMF)
\end{equation}
We will denote this distribution as $\mathcal{Bi}(n, p)$. In R, this is implemented in the `dbinom()` function from `stats` package. Note that so far we assumed that $p=0.5$, however in real life this is not necessarily the case.

Consider a problem of demand forecasting for expensive medical equipment in the UK. These are not typically bought in large quantities, and each hospital might need only one machine for their purposes, and that machine can last for many years. For demonstration purposes, assume that there is a fixed probability $p=0.1$ that any given hospital decides to bye such machine. It is safe to assume that the probability that machine is needed in a hospital is independent of the probability in the other one. In this case, the distribution of demand for machines in the UK per year will be Binomial. For completeness of our example, assume that there are 20 hospitals in the country that might need such machine, then this will be characterised as $\mathcal{Bi}(20, 0.1)$ and will be an asymmetric distribution, as shown in Figure \@ref(fig:binomialPMF01).

```{r binomialPMF01, fig.cap="Probability Mass Function for Binomial distribution with p=0.1 and n=20.", echo=FALSE}
test <- barplot(dbinom(c(0:20), size=20, prob=0.1),
                ylab="Probability",xlab="y", ylim=c(0,0.25),
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:20))
```

From the Figure \@ref(fig:binomialPMF01), it is clear that there is a high probability that only a few machines will be bought: 0, 1, 2 or 3. The probability that we will sell 10 is almost zero. We can also say that with the highest probability, the company will sell 2 machines. We could also use mean, saying how much the company will sell on average, which in Binomial distribution is calculated as $n \times p$ and in our case is $20 \times 0.1 = 2$. However, producing expensive medical equipment typically takes time and should be done in advance. So, if we told the company that they should produce only two, we might then face a situation, when the demand was higher (for example, 6) and they lost the sales. 

So, while we already have a useful information about the distribution of demand in this situation, it is not helpful for decision making. What we could do is consider a case of satisfying, let us say, 99% of demand on machines. In our example, we should sum up the probabilities and find for which number of cases we get to 99%. Formally, this can be written as $P(y<k)=0.99$ and we need to find $k$. One way of doing this is by looking at cumulative distribution function of Binomial distribution, which is mathematically represented as:
\begin{equation}
    F(k, n, p) = \sum_{i=0}^k \begin{pmatrix} n \\ i \end{pmatrix} p^i (1-p)^{n-i} .
    (\#eq:BinomialCDF)
\end{equation}

This CDF in our example will have the shape shown in Figure \@ref(fig:binomialCDF01).

```{r binomialCDF01, fig.cap="Cumulative Distribution Function for Binomial distribution with p=0.1 and n=20.", echo=FALSE}
test <- barplot(pbinom(c(0:20), size=20, prob=0.1),
                ylab="Probability",xlab="y", ylim=c(0,1),
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:20))
```

We can see from Figure \@ref(fig:binomialCDF01) that the cumulative probability reaches value close to 1 after the outcome of 6. Numerically, for the $y=6$, we have probability:

```{r}
pbinom(6, size=20, prob=0.1)
```

while for 5 we will have approximately `r round(pbinom(5, size=20, prob=0.1),3)`. So, for our example, we should recommend the company to produce 6 machines - in this case in 99% of the cases the company will not loose the demand. Yes, in some cases, only 2 or 3 machines will be bought instead of 6, but at least the company will avoid a scenario, when their product is unavailable for clients.

We can get the same result by using Quantile Function, which is implemented in R as `qbinom()`:

```{r eval=FALSE}
qbinom(0.99, 20, 0.1)
```

In some cases, we might need to know the mean and variance of the distribution. For the Binomial distribution, they can be calculated via the following formulae:
\begin{equation}
    \mathrm{E}(y) = p \times n ,
    (\#eq:BinomialMean)
\end{equation}
and
\begin{equation}
    \mathrm{V}(y) = p \times (1-p) \times n .
    (\#eq:BinomialVariance)
\end{equation}
So, in our situation the mean is $20 \times 0.1 = 2$, while the variance is $20 \times 0.1 \times 0.9 = 1.8$.

From the example above, we can take away several important characteristics ofthe Binomial distribution:

1. There are many trials: we have many hospitals, each one of which can be considered as a ``trial'';
2. In each trial, there are only two outcomes: buy or do not buy;
3. The probability of success (someone decides to buy the machine) is fixed between the trials: we assume that the probability that a hospital A decides to buy the machine is the same as for the hospital B;
4. The trials are independent: if one hospital buys a machine, this should not impact the decision of another one;
5. The number of trials $n$ is fixed and known.

Final thing to note about the Binomial distribution is that with the growth of the number of trials it converges to the Normal one, no matter what the probability of success is. We can use this property to get answers about the Binomial distribution in those cases. We will discuss this in Section \@ref(distributionsNormal).

In R, the Binomial distribution is implemented via `dbinom()`, `pbinom()`, `qbinom()` and `rbinom()` from `stats` package. The functions have parameter `size` which is the number of trials $n$, and parameter `prob`, which is a probability of success $p$. If `size=1`, the distribution functions revert to the ones from Bernoulli distribution (Section \@ref(distributionBernoulli)).


## Modelling arrivals -- Poisson distribution {#distributionPoisson}
Consider a situation, when we want to model an arrival of patients in a hospital for each hour of day. In that case, they arrive for different reasons: some have fractures, others have headaches, some came because they sneezed and the others are there because they are seriously ill. There is potentially a lot of people that could come to the hospital, but typically only few of them will show up in each specific hour. Furthermore, all these people are typically not related (unless there was an event that caused a specific condition to a group of people) and arrive at random. If we consider this example of arrival of patients to the hospital, then we could argue that this process is memoryless, because the arrival of different patients is not related. For the probability theory, this implies that the probability that a patient arrives in a specific period of time should be independent of when the previous patient arrived. Mathematically, this is represented as:
\begin{equation}
    \mathrm{P}(t > \tau_1 + \tau_2) = \mathrm{P}(t > \tau_1)\mathrm{P}(t > \tau_2),
    (\#eq:PoissonTimes)
\end{equation}
where $t$ is the waiting time until the next arrival, and $\tau_1$ and $\tau_2$ are waiting times. The formula \@ref(eq:PoissonTimes) shows that the probability that we will wait more than $\tau_1$ and $\tau_2$ is just equal to the product of probabilities of waiting for more than $\tau_1$ and more than $\tau_2$. The formula relies on the independence of probabilities (discussed in Chapter \@ref(probabilityTheory)). From the mathematical point of view, there is a function that supports the property \@ref(eq:PoissonTimes) - it is exponent:
\begin{equation}
    e^{\tau_1 + \tau_2} = e^{\tau_1} e^{\tau_2} .
    (\#eq:PoissonTimesExp)
\end{equation}
Based on this principle of memorylessness (and based on the Exponential distribution, which we will discuss in Section \@ref(distributionsExponential)), Poisson distribution is derived. It is a discrete distribution, which is used for modelling number of patients arriving at specific time intervals, based on the average number of arrivals $\lambda$. Its PMF has the shape shown in Figure \@ref(fig:poissonPMF).

```{r poissonPMF, fig.cap="Probability Mass Function of Poisson distribution with different values of $\\lambda$.", echo=FALSE}
par(mfrow=c(2,3))
dpois(c(0:10),lambda=0.1) |> setNames(0:10) |>
barplot(ylim=c(0,1),
        ylab="Probability",xlab="y", col="grey95", main=TeX("$\\lambda=0.1$"))
dpois(c(0:10),lambda=0.5) |> setNames(0:10) |>
barplot(ylim=c(0,1),
        ylab="Probability",xlab="y", col="grey95", main=TeX("$\\lambda=0.5$"))
dpois(c(0:10),lambda=1) |> setNames(0:10) |>
barplot(ylim=c(0,1),
        ylab="Probability",xlab="y", col="grey95", main=TeX("$\\lambda=1$"))

dpois(c(0:10),lambda=2) |> setNames(0:10) |>
barplot(ylim=c(0,1),
        ylab="Probability",xlab="y", col="grey95", main=TeX("$\\lambda=2$"))
dpois(c(0:10),lambda=3) |> setNames(0:10) |>
barplot(ylim=c(0,1),
        ylab="Probability",xlab="y", col="grey95", main=TeX("$\\lambda=3$"))
dpois(c(0:22),lambda=10) |> setNames(0:22) |>
barplot(ylab="Probability", xlab="y", col="grey95", main=TeX("$\\lambda=10$"))
```

From the graphs in Figure \@ref(fig:poissonPMF), we can make several observations:

1. Zeroes are natural in Poisson distribution. This means that there is a chance that nobody will come in the next hour.
2. With the increase of the average number of arrivals $\lambda$, the chance to have more patients arriving increases. For example, with $\lambda=0.1$, the probability of having no patients is approximately 0.9, while in case of $\lambda=1$, it is approximately 0.4.
3. With the increase of $\lambda$, the shape of distribution becomes closer to the Normal one (discuss in Section \@ref(distributionsNormal)).

Mathematically, the PMF of Poisson distribution is represented as:
\begin{equation}
    f(y, \lambda) = \frac{\lambda^y e^{-\lambda}}{y!} ,
    (\#eq:PoissonPMF)
\end{equation}
where $e$ is the Euler's constant. The part $e^{-\lambda}$ represents the memoryless arrivals of patients. The Poisson distribution is characterised as $\mathcal{Pois}(\lambda)$ and has an additional property that its expectation is equal to variance:
\begin{equation}
    \mathrm{E}(y) = \mathrm{V}(y) = \lambda ,
    (\#eq:PoissonMean)
\end{equation}
which makes it convenient to work with and easy to estimate. If we have the parameters of the Poisson distribution, we can calculate probabilities for a variety of situations or generate quantiles - depending on what we need specifically. For example, for scheduling purposes we might need to understand what is the probability of having more than zero but up to four patients in any specific hour. Assume that based on the available data we estimated that $\hat{\lambda}=2$. In this situation we need to either use PMF or the CDF to calculate the following probability:
\begin{equation}
    \begin{aligned}
        \mathrm{P}(0 < y \leq 4) = & \mathrm{P}(y=1) + \mathrm{P}(y=2) + \mathrm{P}(y=3) + \mathrm{P}(y=4) = \\
                                   & \mathrm{P}(y \leq 4) - \mathrm{P}(y=0)
    \end{aligned} .
    (\#eq:PoissonProbabilityExample)
\end{equation}

The first line in \@ref(eq:PoissonProbabilityExample) shows how the probability can be calculated via the PMF, while the second one is for the CDF. They both can be calculated in R via:
```{r}
sum(dpois(1:4,2))
ppois(4,2) - ppois(0,2)
```

where `dpois()` is the PMF and `ppois()` is the CDF of the Poisson distribution. Mathematically, the CDF of the Poisson distributions is written as:
\begin{equation}
    F(y, \lambda) = e^{-\lambda} \sum_{j=0}^{y} \frac{\lambda^j }{j!} ,
    (\#eq:PoissonCDF)
\end{equation}
for integer values of $y$. Finally, we could get quantiles of the distribution for the specified probability. In R, this is done via the `qpois()` function. For example, here is the 0.95 quantile for the Poisson distribution with $\hat{\lambda}=2$:

```{r}
qpois(0.95, 2)
```

:::remark
When identifying a suitable distribution, Poisson has several distinct characteristics:

1. There are many trials (in our example with the hospital, many people live in the area).
2. There is a small chance that each specific patient will arrive in a specific hour (number of patients arriving at the hospital each hour is small in comparison with the population).
3. Each specific event is independent of the others (memorylessness property: a patient arrives independent of another one).

These three criteria can be used to identify Poisson distribution in practice.
:::

<!-- ::: task -->
<!-- ::: -->

<!-- ::: solution -->
<!-- ::: -->

<!-- Connection with Binomial -->


## Rolling a dice -- Discrete Uniform distribution {#distributionUniform}
In the second world war, the UK faced a problem of understanding how many tanks Nazi Germany had. Some tank components had serial numbers, and when some of tanks were captured or destroyed, it was possible to track these numbers. But was it possible to say how many enemy tanks there were overall? As it appears, it was. From the stand point of the UK, it was equally possible to have a serial number 0001, 0042, 0500 or, say, 1984. The distribution of these serial numbers could be assumed uniform, which was then used to get an estimate of the maximum number of tanks that the enemy had. We will come back to the solution of this problem at the end of this section.

This distribution is one of the basic ones in the probability theory. For now we focus on the discrete version of it, keeping in mind that there also exists the continuous one (see Section \@ref(distributionsUniformContinuous)).

The classical example of application of this distribution is dice rolling. The conventional dice has 6 sides and when rolled can give a value of 1 to 6. If the dice is fair then the probability of getting a score on it is the same for all the sides. This means that the PMF of the distribution can be written as:
\begin{equation}
    f(y, k) = \frac{1}{k},
    (\#eq:UniformPMF)
\end{equation}
where $k$ is the number of outcomes (sides of the dice). The more outcomes there are, the lower the probability of having a specific outcome is. For example, on a dice with 10 sides, the probability of getting the score 5 is $\frac{1}{10}$, while on the 6-sided version it is $\frac{1}{6}$.

The PMF of the Uniform distribution is shown visually in Figure \@ref(fig:uniformPMF) on example of 1d6.

```{r uniformPMF, fig.cap="Probability Mass Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(rep(1/6,6), ylim=c(0,1),
                ylab="Probability",xlab="y", col="grey95")
axis(side=1,at=test,labels=c(1:6))
```

The mean of this distribution is calculated as $\frac{a+b}{2}$, where $a$ is the lowest and $b$ is the highest possible values. So, for the 1d6, the mean is $\frac{1+6}{2}=3.5$. This means that if we roll the dice many times the average score will be 3.5.

The variance of the uniform distribution depends on the number of outcomes and is calculated as:
\begin{equation}
    \sigma^2(y, k) = \frac{k^2-1}{12} .
    (\#eq:UniformVariance)
\end{equation}
As can be seen from the formula, the variance of Uniform distribution is proportional to the number of outcomes.

Coming to the CDF of the Uniform distribution, it is calculated as:
\begin{equation}
    f(y, k) = \frac{y-a+1}{k},
    (\#eq:UniformCDF)
\end{equation}
where $a$ is the lowest possible value and $k$ is the number of outcomes. This CDF can be visualised as shown in Figure \@ref(fig:uniformCDF).

```{r uniformCDF, fig.cap="Cumulative Distribution Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(cumsum(rep(1/6,6)), ylim=c(0,1),
                ylab="Probability",xlab="y", col="grey95")
axis(side=1,at=test,labels=c(1:6))
```

Given that the probability of each separate outcome in the Uniform distribution is always $\frac{1}{k}$, the CDF demonstrates a linear growth, reaching 1 at the highest point, which can be interpreted as rolling 1d6, we will always get a value up to 6 (less than or equal to 6). The CDF can be used to get probabilities of several events at the same time. For example, we can say that when rolling 1d6 the probability of getting 1 or 2 is $\frac{2-1+1}{6}=\frac{1}{3}$.

Bernoulli distribution (Section \@ref(distributionBernoulli)) with $p=0.5$ can be considered as a special case of the Uniform distribution (with only two outcomes).

::: task
A company produces headphones, putting serial numbers on them. So far, it has produced 9,990 of them. If a customer buys headphones, what is the probability that they will get a serial number with three digits?
:::

::: solution
This is the task on Uniform distribution, because serial numbers do not repeat and we can assume that the probability of getting any of them is the same. In terms of parameters, $a=1$ and $b=9990$. To get a serial number with three digits, a customer needs to have anything between 100 and 999. This can be formulated as:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \mathrm{P}(y \leq 999) - \mathrm{P}(y \leq 99).
\end{equation*}
Inserting the values in the CDF of the Uniform distribution \@ref(eq:UniformCDF) we get:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \frac{999}{9990} - \frac{99}{9990} \approx 0.1 - 0.01 = 0.09.
\end{equation*}
:::

::: remark
Similarly how Binomial distribution is a generalisation of the Bernoulli, there is distribution describing the multiple dice rolls. It is called the Multinomial distribution. While we do not discuss it here, we note that this is a distribution, which is, for example, used to model respondents choices in survey, when the variable of interest is in a categorical scale and the probabilities for different options are not equal.
:::

Coming back to the example with the German tanks, this problem was solved by estimating the maximum of the uniform distribution, i.e. getting the estimate of $b$. @Goodman1954 provides a solution. He showed how to find the maximum for a set of serial numbers: 83, 135, 274, 380, 668, 895, 955, 964, 1113, 1174, 1210, 1344, 1387, 1414, 1610, 1668, 1689, 1756, 1865, 1874, 1880, 1936, 2005, 2006, 2065, 2157, 2220, 2224, 2396, 2543 and 2787. An estimate of the maximum value can be calculated using the formula [@Goodman1954]:
\begin{equation*}
    % \hat{b} = m + \frac{m}{k}-1,
    \hat{b} = (\mathrm{max} - \mathrm{min}) \frac{k+1}{k-1}
\end{equation*}
which in our example gives
\begin{equation*}
    %\hat{b} = 2787 + \frac{2787}{31}-1 \approx 2876,
    \hat{b} = (2787-83) \frac{32}{30} \approx 2884.
\end{equation*}
The real maximum number in that example was 2885, making the estimate above quite accurate. There are several solutions to this problem, and in the Second World War, statistical estimates were shown to be much more accurate than the ones obtained via intel.


<!-- ## Geometric -->


<!-- ## Negative Binomial -->
<!-- Read Boylan & Syntetos -->
