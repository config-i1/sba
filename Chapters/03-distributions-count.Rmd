# Discrete distributions {#countDistributions}
In this chapter we discuss the idea of discrete distributions, their properties and then move to the discussion of specific examples: Bernoulli, Binomial, Poisson, Geometric and Negative Binomial distributions.


## What is discrete distribution?
A random variable discussed in Section \@ref(whatIsRandomVariable) can take a variety of values. For now we focus on discrete random variable, which means that it can take one of the several possible values with some probabilities. For example, if we throw two six-sided dices, we will have a variety of outcomes from 2 (both having score of one) to 12 (both having score of six), but not all outcomes will have equal probability. For example, there are different ways of obtaining score of 7: 1+6, 2+5, 3+4, 4+3, 5+2 and 6+1 - but there is only one way of obtaining 2: 1+1. In this case, we are dealing with a distribution of values from 2 to 12, where each value has its own probability of occurrence. This situation is shown in Figure \@ref(fig:distributionDice12).

```{r distributionDice12, echo=FALSE, fig.cap="Distribution of outcomes for scores based on two dices."}
y <- c(1:6,5:1)/36
names(y) <- c(2:12)
yBarplot <- barplot(y, xlab="Dice score", ylab="Probability of outcome")
axis(1, at=yBarplot, labels=FALSE)
```

As can be seen from Figure \@ref(fig:distributionDice12), the distribution of probabilities in this case is symmetric, the chances of having very low and very high scores are lower than the chance of having something closer to the middle. The probability of having 7 is the highest and is $\frac{6}{36}=\frac{1}{6}$, which means that it will occur more often than other values if we repeat the experiment and throw the dices many times.

Any discrete distribution can be characterised using the following functions:

1. Probability Mass Function (PMF);
2. Cumulative Distribution Function (CDF);
4. Moment Generation Function (MMF);
5. Characteristic function (CF).

PMF is the function of probability of occurrence from specific values of random variable. An example of PMF is shown in Figure \@ref(fig:distributionDice12). Based on it, we can say what the probability of a specific outcome is for the random variable.

CDF shows the probability of the event lower than the specified one. For example, the probability of getting the score lower than 4 is $\frac{1}{36}+\frac{2}{36}=\frac{1}{12}$, which corresponds to the sum of the first two bars in Figure \@ref(fig:distributionDice12). The CDF for our example is shown in Figure \@ref(fig:distributionDice12CDF).

```{r distributionDice12CDF, echo=FALSE, fig.cap="Cumulative distribution of outcomes for scores based on two dices."}
yBarplot <- barplot(cumsum(y), xlab="Cumulative dice score", ylab="Probability of outcome")
axis(1, at=yBarplot, labels=FALSE)
```

Any CDF is equal to zero for the values below possible (e.g. it is impossible to get score of 1 throwing two dices) and is equal to one for the values at and above the maximum (if we throw two dices, the score will be below 13). Given that CDF shows probabilities, it can never be greater than one or lower than zero.

Finally, MGF and CF are the functions that allow obtaining the moments of distributions, such as mean, variance, skewness etc. We do not discuss these functions in detail in this textbook, and we will discuss the moments later in the Section \@ref(dataAnalysisNumerical).

Because we considered the discrete random variable, the distribution shown in Figure \@ref(fig:distributionDice12) is discrete as well.


## Tossing a coin -- Bernoulli distribution {#distributionBernoulli}
The simplest distribution that arises from one of the commonly used example in statistics is Bernoulli distribution. It can be used to characterise the outcomes of coin tossing for many times. In this special case, according to this distribution, the random variable can only take two values: zero (e.g. for heads) and one (e.g. tails) with a probability of having tails equal to $p=0.5$. It is a useful distribution not only for the coin experiment, but for any other experiment with two outcomes and some probability $p$. For example, consumers behaviour when making a choice whether to buy a product or not can be modelled using Bernoulli distribution: we do not know what a consumer will choose and why, so based on some external information we can assign them a probability of purchase $p$.

In general, the distribution can be characterised with the following PMF:
\begin{equation}
    f(y, p) = p^y (1-p)^(1-y),
    (\#eq:Bernoulli)
\end{equation}
where $y$ can only take values of 0 and 1. Figure \@ref(fig:bernoulliPMF) demonstrates how the PMF \@ref(eq:Bernoulli) looks for several probabilities.

```{r bernoulliPMF, fig.cap="Probability Mass Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(dbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8")
axis(side=1,at=test,labels=c(0,1))
```

The mean of this distribution equals to $p$, which is in practice used in the estimation of the probability of occurrence $p$: collecting a vector of zeroes and ones and then taking the mean will give the empirical probability of occurrence $\hat{p}$. The variance of Bernoulli distribution is $p\times (1-p)$.

Finally, the CDF of the distribution is:
\begin{equation}
    F(y, p) = \left\{ \begin{aligned}
                            1-p, & \text{ for } y=0 \\
                            1,   & \text{ for } y=1 .
                    \end{aligned} \right.
    (\#eq:Bernoulli)
\end{equation}
which can be plotted as shown in Figure \@ref(fig:bernoulliCDF).

```{r bernoulliCDF, fig.cap="Cumulative Distribution Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(pbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8")
axis(side=1,at=test,labels=c(0,1))
```

The CDF of Bernoulli distribution is seldom used in practice and is provided here for completeness.

::: task
While sitting at home during the COVID pandemic isolation, Vasiliy conducted an experiment: he threw paper balls in a rubbish bin located in the far corner of his room. He counted how many times he missed and how many times he got the balls in the bin. It was 36 to 64.

1. What is the probability distribution that describes this experiment?
2. What is the probability that Vasiliy will miss when he throws the next ball?
3. What is the variance of his throws?
:::

::: solution
This is an example of Bernoulli distribution: it has two outcomes and a probability of success.

We will encode the miss as zero and the score as one. Based on that, taking the mean of the outcomes, we can estimate the mean of Bernoulli probability of miss:
\begin{equation*}
    \bar{y} = \hat{p} = \frac{36}{100} = 0.36.
\end{equation*}
So, when Vasiliy throws the next ball in the bin, he will miss with the probability of 0.36.

The variance is $p \times (1-p) = 0.36 \times 0.64 = 0.2304$.
:::


<!-- Uniform discrete -->


<!-- ## Outcomes of multiple trials -- Binomial distribution -->


<!-- ## Poisson distribution  -->


<!-- ## Geometric -->


<!-- ## Negative Binomial -->

