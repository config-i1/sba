# Discrete distributions {#countDistributions}
In this chapter we discuss the idea of discrete distributions, their properties and then move to the discussion of specific examples: Bernoulli, Binomial, Poisson, Geometric and Negative Binomial distributions.


## What is discrete distribution?
A random variable discussed in Section \@ref(whatIsRandomVariable) can take a variety of values. For now we focus on discrete random variable, which means that it can take one of the several possible values with some probabilities. For example, if we roll two six-sided dices, we will have a variety of outcomes from 2 (both having score of one) to 12 (both having score of six), but not all outcomes will have equal probability. For example, there are different ways of obtaining score of 7: 1+6, 2+5, 3+4, 4+3, 5+2 and 6+1 - but there is only one way of obtaining 2: 1+1. In this case, we are dealing with a distribution of values from 2 to 12, where each value has its own probability of occurrence. This situation is shown in Figure \@ref(fig:distributionDice12).

```{r distributionDice12, echo=FALSE, fig.cap="Distribution of outcomes for scores based on two dices."}
y <- c(1:6,5:1)/36
names(y) <- c(2:12)
yBarplot <- barplot(y, xlab="Dice score", ylab="Probability of outcome", col="grey95")
axis(1, at=yBarplot, labels=FALSE)
```

As can be seen from Figure \@ref(fig:distributionDice12), the distribution of probabilities in this case is symmetric, the chances of having very low and very high scores are lower than the chance of having something closer to the middle. The probability of having 7 is the highest and is $\frac{6}{36}=\frac{1}{6}$, which means that it will occur more often than other values if we repeat the experiment and roll the dices many times.

Any discrete distribution can be characterised using the following functions:

1. Probability Mass Function (PMF);
2. Cumulative Distribution Function (CDF);
4. Moment Generation Function (MMF);
5. Characteristic function (CF).

PMF is the function of probability of occurrence from specific values of random variable. An example of PMF is shown in Figure \@ref(fig:distributionDice12). Based on it, we can say what the probability of a specific outcome is for the random variable.

CDF shows the probability of the event lower than the specified one. For example, the probability of getting the score lower than 4 is $\frac{1}{36}+\frac{2}{36}=\frac{1}{12}$, which corresponds to the sum of the first two bars in Figure \@ref(fig:distributionDice12). The CDF for our example is shown in Figure \@ref(fig:distributionDice12CDF).

```{r distributionDice12CDF, echo=FALSE, fig.cap="Cumulative distribution of outcomes for scores based on two dices."}
yBarplot <- barplot(cumsum(y), xlab="Cumulative dice score", ylab="Probability of outcome", col="grey95")
axis(1, at=yBarplot, labels=FALSE)
```

Any CDF is equal to zero for the values below possible (e.g. it is impossible to get score of 1 rolling two dices) and is equal to one for the values at and above the maximum (if we roll two dices, the score will be below 13). Given that CDF shows probabilities, it can never be greater than one or lower than zero.

Finally, MGF and CF are the functions that allow obtaining the moments of distributions, such as mean, variance, skewness etc. We do not discuss these functions in detail in this textbook, and we will discuss the moments later in the Section \@ref(dataAnalysisNumerical).

Because we considered the discrete random variable, the distribution shown in Figure \@ref(fig:distributionDice12) is discrete as well.


## Tossing a coin -- Bernoulli distribution {#distributionBernoulli}
The simplest distribution that arises from one of the commonly used example in probability theory is Bernoulli distribution. It can be used to characterise the outcomes of coin tossing for many times. In this special case, according to this distribution, the random variable can only take two values: zero (e.g. for heads) and one (e.g. tails) with a probability of having tails equal to $p=0.5$. It is a useful distribution not only for the coin experiment, but for any other experiment with two outcomes and some probability $p$. For example, consumers behaviour when making a choice whether to buy a product or not can be modelled using Bernoulli distribution: we do not know what a consumer will choose and why, so based on some external information we can assign them a probability of purchase $p$.

In general, the distribution can be characterised with the following PMF:
\begin{equation}
    f(y, p) = p^y (1-p)^{1-y},
    (\#eq:BernoulliPMF)
\end{equation}
where $y$ can only take values of 0 and 1. Figure \@ref(fig:bernoulliPMF) demonstrates how the PMF \@ref(eq:Bernoulli) looks for several probabilities.

```{r bernoulliPMF, fig.cap="Probability Mass Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(dbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8", col="grey95")
axis(side=1,at=test,labels=c(0,1))
```

The mean of this distribution equals to $p$, which is in practice used in the estimation of the probability of occurrence $p$: collecting a vector of zeroes and ones and then taking the mean will give the empirical probability of occurrence $\hat{p}$. The variance of Bernoulli distribution is $p\times (1-p)$.

Finally, the CDF of the distribution is:
\begin{equation}
    F(y, p) = \left\{ \begin{aligned}
                            1-p, & \text{ for } y=0 \\
                            1,   & \text{ for } y=1 .
                    \end{aligned} \right.
    (\#eq:BernoulliCDF)
\end{equation}
which can be plotted as shown in Figure \@ref(fig:bernoulliCDF).

```{r bernoulliCDF, fig.cap="Cumulative Distribution Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(pbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5", col="grey95")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8", col="grey95")
axis(side=1,at=test,labels=c(0,1))
```

The CDF of Bernoulli distribution is seldom used in practice and is provided here for completeness.

::: task
While sitting at home during the COVID pandemic isolation, Vasiliy conducted an experiment: he threw paper balls in a rubbish bin located in the far corner of his room. He counted how many times he missed and how many times he got the balls in the bin. It was 36 to 64.

1. What is the probability distribution that describes this experiment?
2. What is the probability that Vasiliy will miss when he throws the next ball?
3. What is the variance of his throws?
:::

::: solution
This is an example of Bernoulli distribution: it has two outcomes and a probability of success.

We will encode the miss as zero and the score as one. Based on that, taking the mean of the outcomes, we can estimate the mean of Bernoulli probability of miss:
\begin{equation*}
    \bar{y} = \hat{p} = \frac{36}{100} = 0.36.
\end{equation*}
So, when Vasiliy throws the next ball in the bin, he will miss with the probability of 0.36.

The variance is $p \times (1-p) = 0.36 \times 0.64 = 0.2304$.
:::

In R, this distribution is implemented in `stats` package via `dbinom(size=1)`, `pbinom(size=1)`, `qbinom(size=1)` and `rbinom(size=1)` for PDF, CDF, QF and random generator respectively. The important parameter in this case is `size=1`, which will be discussed in Section \@ref(distributionBinomial).


## Multiple coin tosses -- Binomial distribution {#distributionBinomial}
In the previous example with coin tossing we focused on the experiment that contained only one step: toss a coin, see what score you got (zero or one). However, we could make the game more complicated and do it in, let us say, 10 trials. In this more complicated experiment we would sum up the scores to see what we get after those 10 trials. In theory, we can have any integer number from zero to ten, but the resulting score will be random and its chance of appearance will vary with the score. For example, we will get 0 only if in all 10 trials we get heads, while we can get 1 in a variety of ways: 1. first trial is one and then the rest are zero; 2. second trial is one and the rest are zero, etc. This means that the score in this experiment will have a distribution of its own. But can we describe it somehow?

Yes, we can. The distribution that underlies this situation is called "Binomial". For the coin tossing experiment with 10 trials and $p=0.5$ it looks like one shown in Figure \@ref(fig:binomialPMF05).

```{r binomialPMF05, fig.cap="Probability Mass Function for Binomial distribution with p=0.5 and n=10.", echo=FALSE}
test <- barplot(dbinom(c(0:10), size=10, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:10))
```

From the Figure \@ref(fig:binomialPMF05), we can see that the most probable outcome is the score of 5. This is because there are more ways of getting 5 than getting 4 in our example. In fact the number of ways can be calculated using **Binomial coefficient**, which is defined as:
\begin{equation}
    \begin{pmatrix} n \\ k \end{pmatrix} = \frac{n!}{k!(n-k)!},
    (\#eq:BinomialCoefficient)
\end{equation}
where $k$ is the score of interest, $n$ is the number of trials and $!$ is the symbol for factorial. In our example, $k=5$ and $n=10$, meaning that we have:
\begin{equation*}
    \begin{pmatrix} 10 \\ 5 \end{pmatrix} = \frac{10!}{5!(10-5)!} ,
\end{equation*}
which can be calculated in R as:
```{r eval=FALSE}
factorial(10)/(factorial(5)^2)
```
and is equal to 252. Using this formula, we can calculate other scores for comparison:
```{r}
factorial(10)/(factorial(c(0:10))*factorial(10-c(0:10))) |>
    setNames(nm=c(0:10))
```
Given that we throw the coin 10 times, there are overall $2^{10}=1024$ theoretical of outcomes of this experiment, which allows calculating the probability of each outcome:
```{r}
round((factorial(10) /
           (factorial(c(0:10))*factorial(10-c(0:10)))) /
          1024,3) |>
    setNames(nm=c(0:10))
```
These probabilities can be obtained via the PMF of Binomial distribution:
\begin{equation}
    f(k, n, p) = \begin{pmatrix} n \\ k \end{pmatrix} p^k (1-p)^{n-k} .
    (\#eq:BernoulliPMF)
\end{equation}
We will denote this distribution as $\mathcal{Bi}(n, p)$. In R, this is implemented in the `dbinom()` function from `stats` package. Note that so far we assumed that $p=0.5$, however in real life this is not necessarily the case.

Consider a problem of demand forecasting for expensive medical equipment in the UK. These are not typically bought in large quantities, and each hospital might need only one machine for their purposes, and that machine can last for many years. If we then assume that there is a fixed probability $p=0.1$ that any given hospital decides to by such machine. It is safe to assume that the probability that machine is needed in one hospital is independent of the probability in the other one. In this case, the distribution of demand for machines in the UK per year will be Binomial. For completeness of our example, assume that there are 20 hospitals in the country that might need such machine, then this will be characterised as $\mathcal{Bi}(20, 0.1)$ and will be an asymmetric distribution, as shown in Figure \@ref(fig:binomialPMF01).

```{r binomialPMF01, fig.cap="Probability Mass Function for Binomial distribution with p=0.1 and n=20.", echo=FALSE}
test <- barplot(dbinom(c(0:20), size=20, prob=0.1),
                ylab="Probability",xlab="y", ylim=c(0,0.25),
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:20))
```

From the Figure \@ref(fig:binomialPMF01), it is clear that there is a high probability that only a few machines will be bought: 0, 1, 2 or 3. The probability that we will sell 10 is almost zero. We can also say that with the highest probability, the company will sell 2 machines. We could also use mean, saying how much the company will sell on average, which in Binomial distribution is calculated as $n \times p$ and in our case is $20 \times 0.1 = 2$. However, producing expensive medical equipment typically takes time and should be done in advance. So, if we told the company that they should produce only two, we might then face a situation, when the demand was higher (for example, 6) and they lost the sales. 

So, while we already have a useful information about the distribution of demand in this situation, it is not helpful for decision making. What we could do is consider a case of satisfying, let us say, 99% of demand on machines. In our example, we should sum up the probabilities and find for which number of cases we get to 99%. Formally, this can be written as $P(y<k)=0.99$ and we need to find $k$. One way of doing this is by looking at cumulative distribution function of Binomial distribution, which is mathematically represented as:
\begin{equation}
    F(k, n, p) = \sum_{i=0}^k \begin{pmatrix} n \\ i \end{pmatrix} p^i (1-p)^{n-i} .
    (\#eq:BernoulliPMF)
\end{equation}

This CDF in our example will have the shape shown in Figure \@ref(fig:binomialCDF01).

```{r binomialCDF01, fig.cap="Cumulative Distribution Function for Binomial distribution with p=0.1 and n=20.", echo=FALSE}
test <- barplot(pbinom(c(0:20), size=20, prob=0.1),
                ylab="Probability",xlab="y", ylim=c(0,1),
                main="", col="grey95")
axis(side=1,at=test,labels=c(0:20))
```

We can see from Figure \@ref(fig:binomialCDF01) that the cumulative probability reaches value close to 1 after the outcome of 6. Numerically, for the $y=6$, we have probability:

```{r}
pbinom(6, size=20, prob=0.1)
```

while for 5 we will have approximately `r round(pbinom(5, size=20, prob=0.1),3)`. So, for our example, we should recommend the company to produce 6 machines - in this case in 99% of the cases the company will not loose the demand. Yes, in some cases, only 2 or 3 machines will be bought instead of 6, but at least the company will avoid a scenario, when their product is unavailable for clients.

We can get the same result by using Quantile Function, which is implemented in R as `qbinom()`:

```{r eval=FALSE}
qbinom(0.99, 20, 0.1)
```

In some cases, we might need to know the mean and variance of the distribution. For the Binomial distribution, they can be calculated via the following formulae:
\begin{equation}
    \mathrm{E}(y) = p \times n ,
    (\#eq:BinomialMean)
\end{equation}
and
\begin{equation}
    \mathrm{V}(y) = p \times (1-p) \times n .
    (\#eq:BinomialVariance)
\end{equation}
So, in our situation the mean is $20 \times 0.1 = 2$, while the variance is $20 \times 0.1 \times 0.9 = 1.8$.

From the example above, we can take away several important characteristics ofthe Binomial distribution:

1. There are many trials: we have many hospitals, each one of which can be considered as a ``trial'';
2. In each trial, there are only two outcomes: buy or do not buy;
3. The probability of success (someone decides to buy the machine) is fixed between the trials: we assume that the probability that a hospital A decides to buy the machine is the same as for the hospital B;
4. The trials are independent: if one hospital buys a machine, this should not impact the decision of another one;
5. The number of trials $n$ is fixed and known.

Final thing to note about the Binomial distribution is that with the growth of the number of trials it converges to the Normal one, no matter what the probability of success is. We can use this property to get answers about the Binomial distribution in those cases. We will discuss this in Section \@ref(distributionsNormal).

In R, the Binomial distribution is implemented via `dbinom()`, `pbinom()`, `qbinom()` and `rbinom()` from `stats` package. The functions have parameter `size` which is the number of trials $n$, and parameter `prob`, which is a probability of success $p$. If `size=1`, the distribution functions revert to the ones from Bernoulli distribution (Section \@ref(distributionBernoulli)).


## Rolling a dice -- Discrete Uniform distribution {#distributionUniform}
Another simple distribution, arising from a classical probability theory examples, is the Uniform distribution. For now we focus on the discrete version of it, keeping in mind that there also exists the continuous one (see Section \@ref(distributionsUniformContinuous)).

The classical example of application of this distribution is dice rolling. The conventional dice has 6 sides and when rolled can give a value of 1 to 6. If the dice is fair then the probability of getting a score on it is the same for all the sides. This means that the PMF of the distribution can be written as:
\begin{equation}
    f(y, k) = \frac{1}{k},
    (\#eq:BernoulliPMF)
\end{equation}
where $k$ is the number of outcomes (sides of the dice). The more outcomes there are, the lower the probability of having a specific outcome is. For example, on a dice with 10 sides, the probability of getting the score 5 is $\frac{1}{10}$, while on the 6-sided version it is $\frac{1}{6}$.

::: remark
In some tabletop games, the number of dices and their outcomes are encoded as $a \mathrm{d} b$, where $a$ is the number of dices, $b$ is the number of sides and d stands for the word "dice". In our example, the 10-sided dice can be encoded as 1d10, while the classical 6-sided one is 1d6.
:::

The PMF of the Uniform distribution is shown visually in Figure \@ref(fig:uniformPMF) on example of 1d6.

```{r uniformPMF, fig.cap="Probability Mass Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(rep(1/6,6), ylim=c(0,1),
                ylab="Probability",xlab="y", col="grey95")
axis(side=1,at=test,labels=c(1:6))
```

The mean of this distribution is calculated as $\frac{a+b}{2}$, where $a$ is the lowest and $b$ is the highest possible values. So, for the 1d6, the mean is $\frac{1+6}{2}=3.5$. This means that if we roll the dice many times the average score will be 3.5.

The variance of the uniform distribution depends on the number of outcomes and is calculated as:
\begin{equation}
    \sigma^2(y, k) = \frac{k^2-1}{12} .
    (\#eq:BernoulliVariance)
\end{equation}
As can be seen from the formula, the variance of Uniform distribution is proportional to the number of outcomes.

Coming to the CDF of the Uniform distribution, it is calculated as:
\begin{equation}
    f(y, k) = \frac{y-a+1}{k},
    (\#eq:BernoulliCDF)
\end{equation}
where $a$ is the lowest possible value and $k$ is the number of outcomes. This CDF can be visualised as shown in Figure \@ref(fig:uniformCDF).

```{r uniformCDF, fig.cap="Cumulative Distribution Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(cumsum(rep(1/6,6)), ylim=c(0,1),
                ylab="Probability",xlab="y", col="grey95")
axis(side=1,at=test,labels=c(1:6))
```

Given that the probability of each separate outcome in the Uniform distribution is always $\frac{1}{k}$, the CDF demonstrates a linear growth, reaching 1 at the highest point, which can be interpreted as rolling 1d6, we will always get a value up to 6 (less than or equal to 6). The CDF can be used to get probabilities of several events at the same time. For example, we can say that when rolling 1d6 the probability of getting 1 or 2 is $\frac{2-1+1}{6}=\frac{1}{3}$.

Bernoulli distribution (Section \@ref(distributionBernoulli)) with $p=0.5$ can be considered as a special case of the Uniform distribution (with only two outcomes).

::: task
A company produces headphones, putting serial numbers on them. So far, it has produced 9,990 of them. If a customer buys headphones, what is the probability that they will get a serial number with three digits?
:::

::: solution
This is the task on Uniform distribution, because serial numbers do not repeat and we can assume that the probability of getting any of them is the same. In terms of parameters, $a=1$ and $b=9990$. To get a serial number with three digits, a customer needs to have anything between 100 and 999. This can be formulated as:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \mathrm{P}(y \leq 999) - \mathrm{P}(y \leq 99).
\end{equation*}
Inserting the values in the CDF of the Uniform distribution \@ref(eq:BernoulliCDF) we get:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \frac{999}{9990} - \frac{99}{9990} \approx 0.1 - 0.01 = 0.09.
\end{equation*}
:::

::: remark
Similarly how Binomial distribution is a generalisation of the Bernoulli, there is distribution describing the multiple dice rolls. It is called the Multinomial distribution. While we do not discuss it here, we note that this is a distribution, which is, for example, used to model respondents choices in survey, when the variable of interest is in a categorical scale and the probabilities for different options are not equal.
:::


<!-- ## Poisson distribution  -->


<!-- ## Geometric -->


<!-- ## Negative Binomial -->
<!-- Read Boylan & Syntetos -->
