# Discrete distributions {#countDistributions}
In this chapter we discuss the idea of discrete distributions, their properties and then move to the discussion of specific examples: Bernoulli, Binomial, Poisson, Geometric and Negative Binomial distributions.


## What is discrete distribution?
A random variable discussed in Section \@ref(whatIsRandomVariable) can take a variety of values. For now we focus on discrete random variable, which means that it can take one of the several possible values with some probabilities. For example, if we throw two six-sided dices, we will have a variety of outcomes from 2 (both having score of one) to 12 (both having score of six), but not all outcomes will have equal probability. For example, there are different ways of obtaining score of 7: 1+6, 2+5, 3+4, 4+3, 5+2 and 6+1 - but there is only one way of obtaining 2: 1+1. In this case, we are dealing with a distribution of values from 2 to 12, where each value has its own probability of occurrence. This situation is shown in Figure \@ref(fig:distributionDice12).

```{r distributionDice12, echo=FALSE, fig.cap="Distribution of outcomes for scores based on two dices."}
y <- c(1:6,5:1)/36
names(y) <- c(2:12)
yBarplot <- barplot(y, xlab="Dice score", ylab="Probability of outcome")
axis(1, at=yBarplot, labels=FALSE)
```

As can be seen from Figure \@ref(fig:distributionDice12), the distribution of probabilities in this case is symmetric, the chances of having very low and very high scores are lower than the chance of having something closer to the middle. The probability of having 7 is the highest and is $\frac{6}{36}=\frac{1}{6}$, which means that it will occur more often than other values if we repeat the experiment and throw the dices many times.

Any discrete distribution can be characterised using the following functions:

1. Probability Mass Function (PMF);
2. Cumulative Distribution Function (CDF);
4. Moment Generation Function (MMF);
5. Characteristic function (CF).

PMF is the function of probability of occurrence from specific values of random variable. An example of PMF is shown in Figure \@ref(fig:distributionDice12). Based on it, we can say what the probability of a specific outcome is for the random variable.

CDF shows the probability of the event lower than the specified one. For example, the probability of getting the score lower than 4 is $\frac{1}{36}+\frac{2}{36}=\frac{1}{12}$, which corresponds to the sum of the first two bars in Figure \@ref(fig:distributionDice12). The CDF for our example is shown in Figure \@ref(fig:distributionDice12CDF).

```{r distributionDice12CDF, echo=FALSE, fig.cap="Cumulative distribution of outcomes for scores based on two dices."}
yBarplot <- barplot(cumsum(y), xlab="Cumulative dice score", ylab="Probability of outcome")
axis(1, at=yBarplot, labels=FALSE)
```

Any CDF is equal to zero for the values below possible (e.g. it is impossible to get score of 1 throwing two dices) and is equal to one for the values at and above the maximum (if we throw two dices, the score will be below 13). Given that CDF shows probabilities, it can never be greater than one or lower than zero.

Finally, MGF and CF are the functions that allow obtaining the moments of distributions, such as mean, variance, skewness etc. We do not discuss these functions in detail in this textbook, and we will discuss the moments later in the Section \@ref(dataAnalysisNumerical).

Because we considered the discrete random variable, the distribution shown in Figure \@ref(fig:distributionDice12) is discrete as well.


## Tossing a coin -- Bernoulli distribution {#distributionBernoulli}
The simplest distribution that arises from one of the commonly used example in probability theory is Bernoulli distribution. It can be used to characterise the outcomes of coin tossing for many times. In this special case, according to this distribution, the random variable can only take two values: zero (e.g. for heads) and one (e.g. tails) with a probability of having tails equal to $p=0.5$. It is a useful distribution not only for the coin experiment, but for any other experiment with two outcomes and some probability $p$. For example, consumers behaviour when making a choice whether to buy a product or not can be modelled using Bernoulli distribution: we do not know what a consumer will choose and why, so based on some external information we can assign them a probability of purchase $p$.

In general, the distribution can be characterised with the following PMF:
\begin{equation}
    f(y, p) = p^y (1-p)^{1-y},
    (\#eq:BernoulliPMF)
\end{equation}
where $y$ can only take values of 0 and 1. Figure \@ref(fig:bernoulliPMF) demonstrates how the PMF \@ref(eq:Bernoulli) looks for several probabilities.

```{r bernoulliPMF, fig.cap="Probability Mass Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(dbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(dbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8")
axis(side=1,at=test,labels=c(0,1))
```

The mean of this distribution equals to $p$, which is in practice used in the estimation of the probability of occurrence $p$: collecting a vector of zeroes and ones and then taking the mean will give the empirical probability of occurrence $\hat{p}$. The variance of Bernoulli distribution is $p\times (1-p)$.

Finally, the CDF of the distribution is:
\begin{equation}
    F(y, p) = \left\{ \begin{aligned}
                            1-p, & \text{ for } y=0 \\
                            1,   & \text{ for } y=1 .
                    \end{aligned} \right.
    (\#eq:BernoulliCDF)
\end{equation}
which can be plotted as shown in Figure \@ref(fig:bernoulliCDF).

```{r bernoulliCDF, fig.cap="Cumulative Distribution Function of Bernoulli distribution with probabilities of 0.2, 0.5 and 0.8", echo=FALSE}
par(mfcol=c(1,3))
test <- barplot(pbinom(c(0,1), size=1, prob=0.2), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.2")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.5), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.5")
axis(side=1,at=test,labels=c(0,1))

test <- barplot(pbinom(c(0,1), size=1, prob=0.8), ylim=c(0,1),
                ylab="Probability",xlab="y",
                main="p=0.8")
axis(side=1,at=test,labels=c(0,1))
```

The CDF of Bernoulli distribution is seldom used in practice and is provided here for completeness.

::: task
While sitting at home during the COVID pandemic isolation, Vasiliy conducted an experiment: he threw paper balls in a rubbish bin located in the far corner of his room. He counted how many times he missed and how many times he got the balls in the bin. It was 36 to 64.

1. What is the probability distribution that describes this experiment?
2. What is the probability that Vasiliy will miss when he throws the next ball?
3. What is the variance of his throws?
:::

::: solution
This is an example of Bernoulli distribution: it has two outcomes and a probability of success.

We will encode the miss as zero and the score as one. Based on that, taking the mean of the outcomes, we can estimate the mean of Bernoulli probability of miss:
\begin{equation*}
    \bar{y} = \hat{p} = \frac{36}{100} = 0.36.
\end{equation*}
So, when Vasiliy throws the next ball in the bin, he will miss with the probability of 0.36.

The variance is $p \times (1-p) = 0.36 \times 0.64 = 0.2304$.
:::


## Throwing a dice -- Uniform distribution {#distributionUniform}
Another simple distribution, arising from a classical probability theory examples, is the Uniform distribution. For now we focus on the discrete version of it, keeping in mind that there also exists the continuous one (see Section \@ref(distributionsUniformContinuous)).

The classical example of application of this distribution is dice throwing. The conventional dice has 6 sides and when thrown can give a value of 1 to 6. If the dice is fair then the probability of getting a score on it is the same for all the sides. This means that the PMF of the distribution can be written as:
\begin{equation}
    f(y, k) = \frac{1}{k},
    (\#eq:BernoulliPMF)
\end{equation}
where $k$ is the number of outcomes (sides of the dice). The more outcomes there are, the lower the probability of having a specific outcome is. For example, on a dice with 10 sides, the probability of getting the score 5 is $\frac{1}{10}$, while on the 6-sided version it is $\frac{1}{6}$.

::: remark
In some tabletop games, the number of dices and their outcomes are encoded as $a \mathrm{d} b$, where $a$ is the number of dices, $b$ is the number of sides and d stands for the word "dice". In our example, the 10-sided dice can be encoded as 1d10, while the classical 6-sided one is 1d6.
:::

The PMF of the Uniform distribution is shown visually in Figure \@ref(fig:uniformPMF) on example of 1d6.

```{r uniformPMF, fig.cap="Probability Mass Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(rep(1/6,6), ylim=c(0,1),
                ylab="Probability",xlab="y")
axis(side=1,at=test,labels=c(1:6))
```

The mean of this distribution is calculated as $\frac{a+b}{2}$, where $a$ is the lowest and $b$ is the highest possible values. So, for the 1d6, the mean is $\frac{1+6}{2}=3.5$. This means that if we throw the dice many times the average score will be 3.5.

The variance of the uniform distribution depends on the number of outcomes and is calculated as:
\begin{equation}
    \sigma^2(y, k) = \frac{k^2-1}{12} .
    (\#eq:BernoulliVariance)
\end{equation}
As can be seen from the formula, the variance of Uniform distribution is proportional to the number of outcomes.

Coming to the CDF of the Uniform distribution, it is calculated as:
\begin{equation}
    f(y, k) = \frac{y-a+1}{k},
    (\#eq:BernoulliCDF)
\end{equation}
where $a$ is the lowest possible value and $k$ is the number of outcomes. This CDF can be visualised as shown in Figure \@ref(fig:uniformCDF).

```{r uniformCDF, fig.cap="Cumulative Distribution Function of Uniform distribution for 1d6.", echo=FALSE}
test <- barplot(cumsum(rep(1/6,6)), ylim=c(0,1),
                ylab="Probability",xlab="y")
axis(side=1,at=test,labels=c(1:6))
```

Given that the probability of each separate outcome in the Uniform distribution is always $\frac{1}{k}$, the CDF demonstrates a linear growth, reaching 1 at the highest point, which can be interpreted as throwing 1d6, we will always get a value up to 6 (less than or equal to 6). The CDF can be used to get probabilities of several events at the same time. For example, we can say that when throwing 1d6 the probability of getting 1 or 2 is $\frac{2-1+1}{6}=\frac{1}{3}$.

Bernoulli distribution (Section \@ref(distributionBernoulli)) with $p=0.5$ can be considered as a special case of the Uniform distribution (with only two outcomes).

::: task
A company produces headphones, putting serial numbers on them. So far, it has produced 9,990 of them. If a customer buys headphones, what is the probability that they will get a serial number with three digits?
:::

::: solution
This is the task on Uniform distribution, because serial numbers do not repeat and we can assume that the probability of getting any of them is the same. In terms of parameters, $a=1$ and $b=9990$. To get a serial number with three digits, a customer needs to have anything between 100 and 999. This can be formulated as:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \mathrm{P}(y \leq 999) - \mathrm{P}(y \leq 99).
\end{equation*}
Inserting the values in the CDF of the Uniform distribution \@ref(eq:BernoulliCDF) we get:
\begin{equation*}
    \mathrm{P}(100 \leq y \leq 999) = \frac{999}{9990} - \frac{99}{9990} \approx 0.1 - 0.01 = 0.09.
\end{equation*}
:::


<!-- ## Outcomes of multiple trials -- Binomial distribution -->


<!-- ## Poisson distribution  -->


<!-- ## Geometric -->


<!-- ## Negative Binomial -->

