# Population and sampling {#PopulationSampling}
Consider a case, when you want to understand what is the average height of teenagers living in your town. It is very expensive and time consuming to go from one house to another and ask every single teenager (if you find one), what their hight is. If we could do that, we would get the true mean, true average height of teenagers living in the town. But in reality, it is more practical to ask a sample of teenagers and make conclusions about the "population" (all teenagers in the town) based on this sample. Indeed, you will spend much less time collecting the information about the height of 100 people rather than 100,000. However, when we take a sample of something, the statistics we work with will always differ from the truth: sample mean will never be equal to the true mean, but it can be shown mathematically that it will converge to the truth, when some specific conditions are met and when the sample size increases. If we set up the experiment correctly, then we can expect our statistics to follow some laws. In this chapter, we discuss these laws, how they work and what they imply.

## Law of Large Numbers {#LLN}
The first law is called the **Law of Large Numbers** (LLN). It is the theorem saying that (under wide conditions) the average of a variable obtained over the large number of trials will be close to its expected value and will get closer to it with the increase of the sample size. This can be demonstrated with the following example:

```{r histY30Y1000, fig.cap="Histograms of samples of data from variable y.", }
obs <- 10000
# Generate data from normal distribution
y <- rnorm(obs,100,100)
# Create sub-samples of 50 and 100 observations
y30 <- sample(y, 30)
y1000 <- sample(y, 1000)
par(mfcol=c(1,2))
hist(y30, xlab="y")
abline(v=mean(y30), col="red")
hist(y1000, xlab="y")
abline(v=mean(y1000), col="red")
```

What we will typically see on the plots above is that the mean (red line) on the left plot will be further away from the true mean of 100 than in the case of the right plot. Given that this is randomly generated, the situation might differ, but the idea would be that with the increase of the sample size the estimated sample mean will converge to the true one. We can even produce a plot showing how this happens:

```{r eval=FALSE}
yMean <- vector("numeric",obs)
for(i in 1:obs){
    yMean[i] <- mean(sample(y,i))
}
plot(yMean, type="l", xlab="Sample size", ylab="Sample mean")
```

```{r statsSampleMean, fig.cap="Demonstration of Law of Large Numbers.", echo=FALSE}
knitr::include_graphics("images/02-statistics-LLN.png")
```

We can see from the plot above that with the increase of the sample size the sample mean reaches the true value of 100. This is a graphical demonstration of the Law of Large Numbers: it only tells us about what will happen when the sample size increases. But it is still useful, because it used for many statistical inferences and if it does not work, then the estimate of mean would be incorrect, meaning that we cannot make conclusions about the behaviour in population.

In order for LLN to work, the distribution of variable needs to have finite mean and variance. This is discussed in some detail in the next subsection. 

In summary, what LLN tells us is that if we average things out over a large number of observations, then that average starts looking very similar to the population value. However, this does not say anything about the performance of estimators on small samples.


## Central Limit Theorem {#CLT}
As we have already seen on Figure \@ref(fig:statsSampleMean), the sample mean is not exactly equal to the population mean even when the sample size is very large (thousands of observations). There is always some sort of variability around the population mean. In order to understand how this variability looks like, we could conduct a simple experiment. We could take a random sample of, for instance, 1000 observations several times and record each of the obtained means. We then can see how the variable will be distributed to see if there are any patterns in the behaviour of the estimator (Figure \@ref(fig:histyMean)):

```{r histyMean, fig.cap="Histogram of the estimates of mean of the variable y."}
# Set seed for reproducibility
set.seed(41)
# Define number of iterations
nIterations <- 1000
sampleSize <- 1000
yMean <- vector("numeric",nIterations)
for(i in 1:nIterations){
    yMean[i] <- mean(sample(y, sampleSize))
}
hist(yMean, xlab="Sample mean", main="", probability=TRUE)
# Add the density curve for the normal distribution
lines(seq(90,110,0.1),
      dnorm(seq(90,110,0.1), 100, 100/sqrt(sampleSize)), 
      col=3, lwd=2)
```

Figure \@ref(fig:histyMean) shows the distribution of the estimates of mean and also has a density curve of the normal distribution with some known parameters above it. We can see that the empirical distribution on the histogram looks similar to the normal one. Why is that?

The answer to this is given by the so called "Central Limit Theorem". It is a group of theorems proving that when many variables are added, the distribution of their normalised sum asymptotically would follow the Normal distribution. There are many different versions of this theorem, which prove that it works under different conditions. One of the basic ones, tells that the sum of independent identically distributed random variables converges to the Normal distribution with the increase of the elements that are summed up, even if the distributions of each variable are not normal.

We do not prove the CLT here, but we provide a demonstration that explains how it holds. To do that, we need to refer back to the convolutions of distributions (Section \@ref(distributionsConvolution)). To understand why we refer to convolution, recall the formula for the sample mean:
\begin{equation*}
    \bar{y} = \frac{1}{n} \sum_{j=1}^n y_j .
\end{equation*}
In the core of the calculation of the mean, there lies the sum of actual values $y_j$. If each one of them has some distribution (e.g. Normal) then their mean would be the normalised (using $n$) sum of random variables $y_j$.

::: remark
The CLT is the theorem about *what happens with the estimate* (usually mean), *not with individual observations*. This means that the error term might follow, for example, Gamma distribution, but the estimate of its mean (under some conditions) will follow the Normal distribution.
:::

Now, consider a simple case, where the random variable $y$ follows the discrete Uniform distribution assuming values in the region [1, 6]. This corresponds to rolling a 6-sided die. If we now want to calculate the score of throwing several similar dice, we would be talking about the value $z=y_1 + y_2 + y_3 + \dots + y_n = \sum_{j=1}^n y_j $, i.e. the convolution of $n$ Uniform distributions. Figure \@ref(fig:uniformPMFCombinedN) shows how the convolved distributions look for $n=1$ (one die), $n=2$ (two dice), $n=4$ and $n=8$.

```{r uniformPMFCombinedN, fig.cap="Probability Mass Function of the sum of n Uniform distributions for 1d6.", fig.width=7, fig.height=8, echo=FALSE}

load("./data/dice_Convolution.Rdata")
# z <- vector("list", 4)
# for(i in 0:3){
#     z[[i+1]] <- table(apply(expand.grid(asplit(replicate(2^i,1:6), 2)), 1, sum))
# }
# save(z, file="./data/dice_Convolution.Rdata")

par(mfcol=c(4,1), mar=c(2,1,3,0))
for(i in c(1:4)){
    test <- barplot(z[[i]], main=paste0(2^(i-1), " dice"), col=10)
}
```

We see that the convolution of 8 distributions already starts looking similar to the normal one (Figure \@ref(fig:uniformPMFCombinedN)). The more dice we combine, the closer that distribution would be to the bell shape of the Normal distribution. This works for both discrete and continuous distributions, no matter what the original shape of the distribution is. The only thing that changes is the number of samples needed for the convolution to converge to the Normal one.

There are several factors that impact the speed of convergence of the convolution to the Normal distribution:

1. Asymmetry of distribution - the more asymmetric the distribution of $y$ is, the longer the sample size needs to be for the convolution to converge to the Normal distribution;
2. Fatness of tails (or the sharpness of the peak) - the fatter the tails are, the longer the samples size needs to be for the convergence;
3. Identical distribution - if all $y_1$, $y_2$, $\dots$, $y_n$ follow the same distribution, the speed of convergence will be higher;
4. Independence of variables - if the variables are independent, the speed of convergence would be higher than in the case of dependent variables.

The fastest distribution, convolutions of which converge to the Normal one, is the Normal distribution itself. But even if the distribution of $y_j$ is symmetric, without the large fat tails and the random variables $y_j$ are i.i.d. (independent and identically distributed), then the speed of convergence would be high enough, so that even with the sample of 30 observations the empirical distribution of the sum $\sum_{j=1}^n y_j$ would be close to the Normal one.

Now, consider several examples of distributions, violating the factors above. We will produce the empirical PDFs of the sum of $n$ random variables and compared it with the Normal distribution.

Figure \@ref(fig:logNormalPDFCombinedN) demonstrates how probability density functions of the convolutions of the Log-Normal distributions would look, depending on how many variables $y_j$ we sum up (for $n=1, 2, 4, \dots 128$). The distribution itself is asymmetric, which is apparent from the first image in Figure \@ref(fig:logNormalPDFCombinedN).

```{r logNormalPDFCombinedN, fig.cap="Probability Density Function of the convolution of n Log-Normal distributions, $y \\sim \\mathrm{log}\\mathcal{N}(0, 0.5)$.", fig.width=8, fig.height=8, echo=FALSE}
set.seed(141)
n <- 2^c(0:7)
y <- replicate(max(n), rlnorm(10000,0,0.5))
z <- vector("list", length(n))
z[[1]] <- y[,1]

for(i in 2:length(n)){
    z[[i]] <- apply(y[,1:n[i]],1,sum)
}

par(mfrow=c(4,2), mar=c(2,1,3,0))
for(i in c(1:length(n))){
    # hist(z[[i]], main=n[i], col=10,
    #      probability=T, axes=F)
    if(i==1){
        mainTitle <- " observation"
    }
    else{
        mainTitle <- " observations"
    }
    plot(density(z[[i]]), col=4, lwd=2, main=paste0(n[i], mainTitle), axes=F)
    lines(seq(-min(z[[i]]), max(z[[i]])*1.5, 0.1),
          dnorm(seq(-min(z[[i]]), max(z[[i]])*1.5, 0.1), mean=mean(z[[i]]), sd=sd(z[[i]])),
          col=3, lwd=2, lty=2)
    axis(1)
    box()
}
```

The empirical density functions are shown in solid green line, while the normal distribution is shown in the dark red dashed lines. We can see that with the increase of the number of observations $n$, the empirical density converges to the theoretical normal one. Given that the Log-Normal distribution is asymmetric, this convergence happens slowly, but even with the 64 samples, it looks already close to the Normal one.

::: remark
The skewness of the Log-Normal distribution we considered in the example above is `r round((exp(0.5^2)+2)*(sqrt(exp(0.5^2)-1)), digits)`. If the asymmetry of the distribution was higher, it would take more observations for the convolved distribution to converge to the Normal one.
:::

Next, we see the effect of the fatness of tails. Figure \@ref(fig:LaplacePDFCombinedN) shows the convolution of the Laplace distributions. This distribution is known to have **fatter tails and higher peak than the Normal one**, which becomes apparent for the $n=1$ in the figure.

```{r LaplacePDFCombinedN, fig.cap="Probability Density Function of the sum of n Laplace distributions, $y \\sim \\mathcal{Laplace}(0, 1)$.", fig.width=8, fig.height=6, echo=FALSE}
set.seed(41)
n <- 2^c(0:5)
y <- replicate(max(n),rlaplace(10000,0,1))
z <- vector("list", length(n))
z[[1]] <- y[,1]

for(i in 2:length(n)){
    z[[i]] <- apply(y[,1:n[i]],1,sum)
}

par(mfrow=c(3,2), mar=c(2,1,3,0))
for(i in c(1:length(n))){
    # hist(z[[i]], main=n[i], col=10,
    #      probability=T, axes=F)
    if(i==1){
        mainTitle <- " observation"
    }
    else{
        mainTitle <- " observations"
    }
    plot(density(z[[i]]), col=4, lwd=2, main=paste0(n[i], mainTitle), axes=F)
    lines(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1),
          dnorm(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1), mean=mean(z[[i]]), sd=sd(z[[i]])),
          col=3, lwd=2, lty=2)
    axis(1)
    box()
}
```

But similarly to the example with the Log-Normal distribution, we see that the convolution of the Laplace distributions converges to the Normal one with the increase of the sample size. In fact, this happens much faster than in the case of the Log-Normal, and for the sample of 16 observations, the distributions already look very similar.

::: remark
The excess kurtosis of the Laplace distribution is 3 (the Normal one has 0). The higher it is, the longer it would take (i.e. larger sample size) for the convolved distribution to converge to the Normal one.
:::

Next, we consider the situation where the distributions are **not identical**. Figure \@ref(fig:nonidentPDFCombinedN) shows the convolution of several continuous distributions (with randomly selected parameters):

- Normal,
- Laplace,
- Log-Normal,
- Uniform,
- Gamma.

These are then normalised to have the same minimum and maximum values and are selected at random, after which their convolutions are plotted against the Normal density function.

::: remark
While CLT works well for i.i.d. variables, for it to work in case of the violation of i.i.d., we need to normalise the random variables. Otherwise, some of them would dominate the pool and it would take many more samples for the convolved distribution to converge to the Normal one.
:::

```{r nonidentPDFCombinedN, fig.cap="Probability Density Function of the sum of n non-identically distributed random variables.", fig.width=8, fig.height=8, echo=FALSE}
set.seed(41)
obs <- 10000
ddist <- function(obs){
    distributions <- matrix(c(rnorm(obs, runif(1,-10,10), runif(1,1,10)),
                              rlaplace(obs, runif(1,-10,10), runif(1,1,5)),
                              rlnorm(obs, runif(1,-10,1), runif(1,0,0.5)),
                              runif(obs, runif(1,-10,0),runif(1,0,10)),
                              rgamma(obs, runif(1,1,10), runif(1,1,10))),
                            obs, 5)
    distributions
    return((distributions - matrix(apply(distributions, 2, min), obs, 5, byrow=TRUE)) / matrix(apply(distributions, 2, max), obs, 5, byrow=TRUE))
}

n <- (1:8)*5
y <- matrix(replicate(8, ddist(obs)), obs, max(n))
z <- vector("list", length(n))

for(i in 1:length(n)){
    z[[i]] <- apply(y[,sample(1:max(n), n[i])], 1, sum)
}

par(mfrow=c(4,2), mar=c(2,1,3,0))
for(i in c(1:length(n))){
    mainTitle <- " observations"
    x <- density(z[[i]])
    y <- dnorm(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1), mean=mean(z[[i]]), sd=sd(z[[i]]))
    plot(x, col=4, lwd=2, main=paste0(n[i], mainTitle), axes=F, ylim=c(min(x$y,y), max(x$y,y)))
    lines(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1),
          y,
          col=3, lwd=2, lty=2)
    axis(1)
    box()
}
```

As we see from Figure \@ref(fig:nonidentPDFCombinedN), the convolved distribution still converges to the Normal one, but this time the convergence happens non-linearly: adding asymmetric distributions to the pool leads to divergences from Normality on some of large samples (e.g. sample of 35 observations). But even in that case, the convolved distribution looks similar to the Normal one on larger samples.

The speed of convergence in this situation highly depends on the asymmetry and the excess of the convolved distributions (see two examples above).

Finally, we consider the violation of the independence. In this example, the variable $y_j$ follows the normal distribution, but it's parameters depend on the parameters of $y_{j-1}$. Each value we generate is standardised to make sure that CLT works.

```{r nonindepPDFCombinedN, fig.cap="Probability Density Function of the sum of n dependent random variables.", fig.width=8, fig.height=8, echo=FALSE}
set.seed(41)
obs <- 10000
ddist <- function(obs, times){
    distributions <- matrix(NA, obs, times)
    meanValue <- 100
    sdValue <- 10
    x <- rnorm(obs, meanValue, sdValue)
    distributions[,1] <- x;
    for(i in 2:times){
        # distributions[,i] <- (distributions[,1:(i-1),drop=FALSE] %*% rlnorm(i-1, 0, 0.5) + rnorm(obs, 0, 1))
        distributions[,i] <- distributions[,i-1,drop=FALSE] %*% rlnorm(1, 0.5, 1) + rnorm(obs, 0, 0.1)
        distributions[,i] <- (distributions[,i] - mean(distributions[,i]))/sd(distributions[,i])
    }
    return(distributions)
}

n <- 2^(1:6)
y <- ddist(obs, max(n))
# y <- matrix(replicate(8, ddist(obs)), obs, max(n))
z <- vector("list", length(n))

for(i in 1:length(n)){
    z[[i]] <- apply(y[,sample(1:max(n), n[i])], 1, sum)
}

par(mfrow=c(3,2), mar=c(2,1,3,0))
for(i in c(1:length(n))){
    mainTitle <- " observations"
    x <- density(z[[i]])
    y <- dnorm(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1), mean=mean(z[[i]]), sd=sd(z[[i]]))
    plot(x, col=4, lwd=2, main=paste0(n[i], mainTitle), axes=F, ylim=c(min(x$y,y), max(x$y,y)))
    lines(seq(-max(z[[i]]), max(z[[i]])*1.5, 0.1),
          y,
          col=3, lwd=2, lty=2)
    axis(1)
    box()
}
```

Because the base distributions used in this example, are Normal, it does not take too much time for their convolution to get close to the Normal as well. Still, we see the violation of independence leads to shapes that deviate from normality even on the sample of 64 observations. The speed of convergence in this example would be lower if the convolved distributions are more linearly related.

In general, we should note that the more factors listed above are violated, the more observations you need to have before the Central Limit Theorem starts working and the convolved distribution becomes close to the Normal one. And as we saw from the examples above, the asymmetry is one of the critical factors influencing the speed of convergence of the convolution of distributions to the Normal one. But there are also three assumptions that need to be satisfied for the CLT to work at all:

1. **The true value of parameter is not near a bound**. e.g. if the variable follows uniform distribution on (0, $a$) and we want to estimate $a$, then its distribution will not be Normal (because in this case the true value is always approached from below). This assumption is important in contexts, where parameter estimates are restricted to make models obey to some rules (e.g. AR parameters of ARIMA model that guarantee that it is stationary).
2. **The mean and variance of the distribution are finite**. This might seem as a weird assumption, but some distributions do not have finite moments, so the CLT will not hold if a variable follows them, just because the sample mean will be all over the plane due to randomness and will not converge to the "true" value. Cauchy distribution is one of such examples.
3. **The random variables are independent identically distributed** (i.i.d.). We saw what the violation of this assumption implies in milder cases, but in more serious ones, this might lead to more substantial problems, so that the convolved distribution would not converge to the Normal one.

If these assumptions hold, then CLT will work for the estimate of a parameter, no matter what the distribution of the random variable is. This becomes especially useful, when we want to test a hypothesis or construct a confidence interval for an estimate of a parameter.


## Properties of estimators {#estimatesProperties}
Before we move further, we need to agree what the term "estimator" means, which will be used several times further in this textbook:

- **Estimate** of a parameter is a result of application of a statistical procedure to the sample of data for obtaining some coefficients of a model. The value that we get when using the arithmetic mean on a sample of data is an estimate of the population mean;
- **Estimator** is the rule for calculating estimates of parameters based on a sample of data. For example, arithmetic mean itself is an estimator of the population mean. Another example would be method of Ordinary Least Squares, which is a rule for producing estimates of parameters of a regression model and thus an estimator.

In this section, we discuss such terms as **bias**, **efficiency** and **consistency** of estimates of parameters, which are directly related to [LLN](#LLN) and [CLT](#CLT). Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), we want to discuss them only in a general way, explaining the ideas behind them.

Note that all the discussions in this chapter relate to **the estimates of parameters**, not to the distribution of a random variable itself. A common mistake that students make when studying statistics, is that they think that the properties apply to the variable $y_j$ instead of the estimate of its parameters (e.g. mean of $y_j$).


### Bias {#estimatesPropertiesBias}
**Bias** refers to the expected difference between the estimated value of parameter (on a specific sample) and the "[true](#intro)" one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be.

The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample:
\begin{equation}
    \mathrm{V}(y) = \frac{1}{n} \sum_{j=1}^n \left( y_j - \bar{y} \right)^2,
    (\#eq:varianceBiased)
\end{equation}
where $n$ is the sample size and $\bar{y} = \frac{1}{n} \sum_{j=1}^n y_j$ is the mean of the data. There is a lot of proofs in the literature of this issue (even @WikipediaVarianceBias2020 has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment:
```{r}
mu <- 100
sigma <- 10
nIterations <- 1000
# Generate data from normal distribution, 10,000 observations
y <- rnorm(10000,mu,sigma)
# This is the function, which will calculate the two variances
varFunction <- function(y){
   return(c(var(y), mean((y-mean(y))^2)))
}
# Calculate biased and unbiased variances for the sample of 30 observations,
# repeat nIterations times
varValues <- replicate(nIterations, varFunction(sample(y,30)))
```
This way we have generated 1000 samples with 30 observations and calculated variances using the formulae \@ref(eq:varianceBiased) and the corrected one for each step. Now we can plot it in order to see how it worked out:

```{r fig.cap="Histograms for biased and unbiased estimates of variance."}
par(mfcol=c(1,2))
# Histogram of the biased estimate
hist(varValues[2,], xlab="V(y)", ylab="y", main="Biased estimate of V(y)")
abline(v=mean(varValues[2,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[2,]),2))),lwd=1,col="red")

# Histogram of unbiased estimate
hist(varValues[1,], xlab="V(y)", ylab="y", main="Unbiased estimate of V(y)")
abline(v=mean(varValues[1,]), col="red")
legend("topright",legend=TeX(paste0("E$\\left(V(y)\\right)$=",round(mean(varValues[1,]),2))),lwd=1,col="red")

```

Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the histogram on the right hand side of the plot) will have lower mean than the unbiased one. This is the graphical example of the effect of not taking the number of estimated parameters into account. The correct formula for the unbiased estimate of variance is:
\begin{equation}
    s^2 = \frac{1}{n-k} \sum_{j=1}^n \left( y_j - \bar{y} \right)^2,
    (\#eq:varianceUnBiased)
\end{equation}
where $k$ is the number of all independent estimated parameters. In this simple example $k=1$, because we only estimate mean (the variance is based on it). Analysing the formulae \@ref(eq:varianceBiased) and \@ref(eq:varianceUnBiased), we can say that with the increase of the sample size, the bias will disappear and the two formulae will give almost the same results: when the sample size $n$ becomes big enough, the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator.


### Efficiency {#estimatesPropertiesEfficiency}
**Efficiency** means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, letâ€™s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters.

Another classical example of not efficient estimator is the median, when used on the data that follows Normal distribution. Here is a simple experiment demonstrating the idea:
```{r eval=FALSE}
mu <- 100
sigma <- 10
nIterations <- 500
obs <- 100
varMeanValues <- vector("numeric",obs)
varMedianValues <- vector("numeric",obs)
y <- rnorm(100000,mu,sigma)
for(i in 1:obs){
    ySample <- replicate(nIterations,sample(y,i*100))
    varMeanValues[i] <- var(apply(ySample,2,mean))
    varMedianValues[i] <- var(apply(ySample,2,median))
}
```

In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one:

```{r eval=FALSE}
options(scipen=6)
plot(1:100*100,varMeanValues/varMedianValues, type="l", xlab="Sample size",ylab="Relative efficiency")
abline(h=1, col="red")
```

```{r statsEfficiecny, fig.cap="An example of a relatively inefficient estimator.", echo=FALSE}
knitr::include_graphics("images/02-statistics-efficiency.png")
```

What we should typically see on this graph, is that the black line should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution $\mu$ than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median [@WikipediaMedianEfficiency2020] (so, the line should converge approximately to the value of 0.64).


### Consistency {#estimatesPropertiesConsistency}
**Consistency** means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This follows directly from [LLN](#LLNandCLT) and is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better.

An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way:
\begin{equation}
    \mathrm{LMax} = \max \left(|y_1-\hat{y}|, |y_2-\hat{y}|, \dots, |y_n-\hat{y}| \right).
    (\#eq:chebyshevNorm)
\end{equation}
Minimising this norm, we can get an estimate $\hat{y}$ of the location parameter $\mu$. The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter:
```{r eval=FALSE}
LMax <- function(y){
    estimator <- function(par){
        return(max(abs(y-par)));
    }
    
    return(optim(mean(y), fn=estimator, method="Brent", lower=min(y), upper=max(y)));
}

mu <- 100
sigma <- 10
nIterations <- 1000
y <- rnorm(10000, mu, sigma)
LMaxEstimates <- vector("numeric", nIterations)
for(i in 1:nIterations){
    LMaxEstimates[i] <- LMax(y[1:(i*10)])$par;
}
```

And here how the estimate looks with the increase of sample size:
```{r eval=FALSE}
plot(1:nIterations*10, LMaxEstimates, type="l", xlab="Sample size",ylab=TeX("Estimate of $\\mu$"))
abline(h=mu, col="red")
```

```{r statsConsistency, fig.cap="An example of inconsistent estimator.", echo=FALSE}
knitr::include_graphics("images/02-statistics-consistency.png")
```

While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters.


::: remark
There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just throw away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data.
:::


### Asymptotic normality {#asymptoticNormality}
Finally, **asymptotic normality** is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to $\mu$) and a fixed variance. This follows directly from [CLT](#LLNandCLT). Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not Normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values.

Another important aspect to cover is what the term **asymptotic**, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index $t \rightarrow \infty$. While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases.


### Why having biased estimate can be better than having the inefficient one? {#efficiencyVSBias}
It might not be clear to everyone why the model with some bias in it might be better than the model with high variance. In order to answer this question, consider the situation, where we want to estimate the value of parameter $\mu$, and we have two methods to do that. Given that we work on a sample of data, the estimates will have some sorts of distributions, shown in Figure \@ref(fig:biasVarianceEstimate).

```{r biasVarianceEstimate, fig.cap="Example of two estimators of a parameter.", echo=FALSE}
layout(matrix(c(1,2),1,2), widths=c(0.73,0.27))
plot(seq(-10,10,0.1),dnorm(seq(-10,10,0.1),2,2),
     type="l", ylim=c(0,1), col="darkblue",
     xlab=latex2exp::TeX("$m$"), ylab="Density",
     xlim=c(-4,8), lwd=2)
lines(seq(-10,10,0.1),dnorm(seq(-10,10,0.1),3,0.5),
      type="l", col="darkred", lwd=2)
abline(v=2, lwd=2, lty=3)
# The plot for the legend
par(mar=c(0.1,0.1,4.1,0.1), bty="n", xaxt="n", yaxt="n")
plot(0, 0, col="white")
legend("topright",legend=c("Estimator 1","Estimator 2","The true value"),
       col=c("darkblue","darkred","black"),
       lwd=2, lty=c(1,1,3))
```

Which of the two estimators would you prefer: the first one or the second one? The conventional statistician might choose Estimator 1, because it produces the unbiased estimates of parameter, meaning that on average we will have the correct value of the true parameter. However, if we rephrase the question slightly, making it more realistic, the answer would probably change: "Which of the two estimators would you prefer **on small sample**?". In this situation, we understand that we have limited data and need to make a decision based on what we have on hands, we might not be able to rely on asymptotic properties, on LLN and CLT (Chapter \@ref(PopulationSampling)). If we choose Estimator 1, then on our specific sample, we might end up easily with a value for $m$ of -2, 0 or 6, just due to the pure chance - this is how wide the distribution is. On the other hand, if we choose the Estimator 2, we will end up with the value, which will be close to the true one: 2.5, 3 or 4. Yes, this value will be typically higher than needed, but at least it will not lead us to confusing conclusions on the data we have. Having said that, if the bias was too high (e.g. if the distribution of the Estimator 2 was placed around -4), the estimator might become unreliable, so there should be some balance in how much bias one should impose.

::: example
In a computer game Diablo II (by Blizzard North), there are two spells, which might be considered as similar in terms of damage to monsters: Lightning and Glacial Spike. On the first level, the Lightning does random damage from 1 to 43, while the Glacial Spike does randomly 17 to 26. Assuming that the distributions of damage are uniform in both cases, we would conclude that on average the Lightning does slightly more damage than the Glacial Spike: $\frac{1}{2}(43+1)=22$ vs $\frac{1}{2}(17+26)=21.5$. However, the Lightning has much higher variability, and is less efficient in killing monsters than the Glacial Spike: it has variance of $\frac{1}{12}(43-1)^2 = 147$ versus $\frac{1}{12}(26-17)^2 = 6.75$ of the Glacial Spike. This means that each time a player shoots the Lightning, there is a chance that it will do less damage than the Glacial Spike (for example, in $\frac{(17-1)}{(43-1)} \approx 38$% of the cases Lightning will do less damage than the lowest possible damage of the Glacial Spike). This means that if one needs to choose, which of the spells to use in a battle, the Glacial Spike would be a safer option, as each specific shot will not be as weak as it could be in the case of the Lightning. But if a player casts both spells many times, then asymptotically the Lightning will be better than Glacial Spike, as it would do more damage on average.
:::


## Confidence interval {#confidenceInterval}
As mentioned in Section \@ref(sourcesOfUncertainty), we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed [earlier](#estimatesProperties). And, if the [LLN](#LLN) and [CLT](#CLT) hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable $y$ itself. When dealing with the former, we typically work with the **confidence interval** - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the **prediction interval** - the interval constructed for the random variable $y$.

In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have:

1. A random variable $y$, which is assumed to follow some distribution with finite mean $\mu$ and variance $\sigma^2$;
2. A sample of size $n$ from the population of $y$;
3. Estimates of mean $\hat{\mu}=\bar{y}$ and variance $\hat{\sigma}^2 = s^2$, obtained based on the sample of size $n$.

What we want to get by doing this is an idea about the population mean $\mu$. The value $\bar{y}$ does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value $\mu$ can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this (Figure \@ref(fig:normalCurveBasic)).

```{r normalCurveBasic, fig.cap="Distribution of the sample mean.", echo=FALSE}
hist(rnorm(10000), xlab=TeX("$\\bar{y}$"), ylab="Density", xlim=c(-5,5), axes=FALSE, main="")
par(new=TRUE)
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab="", ylab="", type="l")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$")),
       lwd=c(2), col=c("darkblue"))
```

On its own, this distribution just tells us that the variable is random around the true mean $\mu$ and that its density function has a bell-like shape. In order to make this more useful, we can construct the **confidence interval** for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover $(1-\alpha)\times 100$% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this:

```{r normalCurveIntervals, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on the population data."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qnorm(0.025),0.01),rev(seq(-5,qnorm(0.025),0.01))),
        c(dnorm(seq(-5,qnorm(0.025),0.01)), rep(0,length(seq(-5,qnorm(0.025),0.01)))), col="grey")
polygon(c(seq(qnorm(0.975),5,0.01),rev(seq(qnorm(0.975),5,0.01))),
        c(dnorm(seq(qnorm(0.975),5,0.01)), rep(0,length(seq(qnorm(0.975),5,0.01)))), col="grey")
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$"),"bounds",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkblue","darkred","grey95","grey"))
```

Figure \@ref(fig:normalCurveIntervals) shows the classical normal distribution curve around the population mean $\mu$, confidence interval of the level $1-\alpha$ and the cut off tails, the overall surface of which corresponds to $\alpha$. The value $1-\alpha$ is called **confidence level**, while $\alpha$ is the **significance level**. By constructing the interval this way, we expect that in the $(1-\alpha)\times 100$% of cases the value will be inside the bounds, and in $\alpha\times 100$% it will not.

In reality we do not know the true mean $\mu$, so we do a slightly different thing: we construct a confidence interval based on the sample mean $\bar{y}$ and sample variance $s^2$, hoping that due to [LLN](#LLNandCLT) they will converge to the true values. We use Normal distribution, because we expect [CLT](#LLNandCLT) to work. This process looks something like in Figure \@ref(fig:normalCurveIntervalsShades), with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample:

```{r normalCurveIntervalsShades, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on a sample."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l", xlim=c(-5,5),
     lwd=1,lty=2, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),-1),rep(0,length(seq(-5,5,0.1)))),col="grey95",lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=2, lty=1, col="darkblue")
abline(v=-1, col="darkblue", lwd=2, lty=2)
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),0),rep(0,length(seq(-5,5,0.1)))),col="white",lty=0)
lines(c(-5,5), c(0,0), col="black", lwd=1)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=1, lty=2, col="darkblue")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=2, lty=1, col="darkgreen")
lines(c(-1,-1), c(0,dnorm(-1)), col="darkblue", lwd=1, lty=2)
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkgreen", lwd=2, lty=2)
legend("topright",legend=c(TeX("$\\mu$"),"Sample mean","Bounds","True PDF","Assumed PDF"),
       lwd=c(2,2,2,2,2), col=c("darkblue","darkgreen","darkred","darkblue","darkgreen"), lty=c(2,2,1,1,1))
```

So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of $\bar{y}$ and $s^2$. If we construct the confidence interval of the width $1-\alpha$ (e.g. 0.95) for thousands of random samples (thousands of trials), then in $(1-\alpha)\times 100$% of cases (e.g. 95%) the true mean will be covered by the interval, while in $\alpha \times 100$% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials.

Mathematically the red bounds in Figure \@ref(fig:normalCurveIntervalsShades) are represented using the following well-known formula for the confidence interval:
\begin{equation}
    \mu \in (\bar{y} + t_{\alpha/2}(df) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(df) s_{\bar{y}}),
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=n-k$ degrees of freedom ($n$ is the sample size and $k$ is the number of estimated parameters, e.g. $k=1$ in our case) and level $\alpha/2$, and $s_{\bar{y}}=\frac{1}{\sqrt{n}}s$ is the estimate of the standard deviation of the sample mean (see proof below). If we knew for some reason the true variance $\sigma^2$, then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics (see discussion of sample mean tests in Section \@ref(statisticalTestsOneSampleMean)).

::: proof
We are interested in calculating the variance of $\bar{y}=\frac{1}{n} \sum_{j=1}^n y_j$. If $s$ is the standard deviation of the random variable $y$, then $\mathrm{V}(y_j) = s^2$. The following derivations assume that $y_j$ is i.i.d. (specifically, not correlated with each other).
\begin{equation*}
    \begin{aligned}
    \mathrm{V}(\bar{y}) = & \mathrm{V}\left(\frac{1}{n} \sum_{j=1}^n y_j \right) = \frac{1}{n^2} \mathrm{V} \left(\sum_{j=1}^n y_j \right) = \\
                          & \frac{1}{n^2} \sum_{j=1}^n \mathrm{V} \left( y_j \right) = \frac{1}{n^2} \sum_{j=1}^n s^2 = \\
                          & \frac{1}{n} s^2
    \end{aligned}
\end{equation*}
Based on this, we can conclude that $s_{\bar{y}}=\sqrt{\mathrm{V}(\bar{y})}= \frac{1}{\sqrt{n}} s$.
:::

Note, that in order to construct *confidence interval*, we do not care what distribution $y$ follows, as long as LLN and CLT hold.

## Prediction interval {#predictionInterval}
If we are interested in capturing the uncertainty about the random variable $y$, then we should refer to prediction interval. In this case, we typically rely on [LLN](#LLNandCLT) and the [assumed distribution](#distributions) for the random variable $y$. For example, if we know that $y \sim \mathcal{N}(\mu, \sigma^2)$, then based on our sample we can construct a prediction interval of the width $1-\alpha$:
\begin{equation}
    y \in (\bar{y} + z_{\alpha/2} s, \bar{y} + z_{1-\alpha/2} s),
    (\#eq:predictionInterval)
\end{equation}
where $z_{\alpha/2}$ is the z-statistics (quantile of standard normal distribution) for the level $\alpha/2$ and $\bar{y}$ is the sample estimate of $\mu$ and $s$ is the sample estimate of $\sigma$. The graphical presentation of such interval can be shown as in Figure \@ref(fig:predictionInterval).

```{r predictionInterval, fig.cap="Artificial data, mean, confidence and prediction intervals.", echo=FALSE}
y <- rnorm(100,100,10)
yInterval <- qnorm(c(0.025,0.975),mean(y),sd(y))
yMeanInterval <- qnorm(c(0.025,0.975),mean(y),sd(y)/10)

par(mfcol=c(1,2))
plot(y, xlab="Observation", ylab="Value")
points(which(y>yInterval[2] | y<yInterval[1]),
       y[y>yInterval[2] | y<yInterval[1]], pch=16)
abline(h=mean(y), col="darkblue", lwd=2)
abline(h=yInterval, col="darkorange", lwd=2, lty=2)
abline(h=yMeanInterval, col="darkred", lwd=2)

hist(y, xlab="Value", main="")
abline(v=mean(y), col="darkblue", lwd=2)
abline(v=yInterval, col="darkorange", lwd=2, lty=2)
abline(v=yMeanInterval, col="darkred", lwd=2)
```

Figure \@ref(fig:predictionInterval) shows the 95% prediction interval on two plots: the linear plot of values vs observations id and on the histogram. In both cases the prediction intervals are the dashed orange lines, lying further away from the sample mean (the solid blue line). The two solid red lines around the mean represent the 95% confidence intervals for the mean (discussed in Section \@ref(confidenceInterval)). As can be seen, the prediction intervals show, where the 95% of observations are expected to lie. As a result, several observations lie outside the bounds (given the sample of 100 observations, we would expect 5 of them to lie outside, but this will vary from one sample to another). In contrast, confidence interval shows, where the expectation of the population will lie in 95% of the cases, if the interval is constructed many times for random samples.

The formula \@ref(eq:predictionInterval) relies on the assumption of normality. If it does not hold, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the [assumptions](#assumptions) do not hold, we might switch to more advanced methods for prediction interval construction.
