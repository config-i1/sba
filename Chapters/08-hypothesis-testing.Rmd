# Hypothesis testing {#hypothesisTesting}
Hypothesis testing arises naturally from the idea of confidence intervals discussed in Section \@ref(confidenceInterval): instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does.

In this Chapter we will discuss the main mechanism of hypothesis testing, then move to the discussion of type 0, I and II errors that might arise in the process. We then will discuss the idea of a Power of a Test and investigate what it is influenced by. After that we will discuss several basic popular parametric and non-parametric tests and how to select the most appropriate one between them.


## Basic idea
Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null ($\mathrm{H}_0$) and alternative ($\mathrm{H}_1$) hypotheses about the process or a property in the population, and we want to find some evidence to reject the $\mathrm{H}_0$. Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate $\mathrm{H}_0$ that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject $\mathrm{H}_0$ (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than Â£100k per year we would formulate the hypotheses in the following way:
\begin{equation*}
    \mathrm{H}_0: \mu \leq 100, \mathrm{H}_1: \mu > 100.
\end{equation*}
Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps:

1. Formulate null and alternative hypotheses ($\mathrm{H}_0$ and $\mathrm{H}_1$) based on your understanding of the problem;
2. Select the significance level $\alpha$ on which the hypothesis will be tested;
3. Select the test appropriate for the formulated hypotheses (1);
4. Conduct the test (3) and get the calculated value;
5. Compare the value in (4) with the threshold one;
6. Make a conclusion based on (5) on the selected level (2).

Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called "p-hacking", trying to make results look nicer than they really are.

Consider an example, where we want to check, whether the population mean $\mu$ is equal to zero, based on a sample of 36 observations, where $\bar{y}=-0.5$ and $s^2=1$. In this case, we formulate the null and alternative hypotheses:
\begin{equation*}
    \mathrm{H}_0: \mu=0, \mathrm{H}_1: \mu \neq 0.
\end{equation*}
We then select the significance level $\alpha=0.05$ (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula:
\begin{equation}
    t = \frac{\bar{y} - \mu}{s_{\bar{y}}} = \frac{-0.5 - 0}{\frac{1}{\sqrt{36}}} = -3 .
    (\#eq:ttestFormula)
\end{equation}
After that we get the critical value of t with $df=36-1=35$ degrees of freedom and significance level $\alpha/2=0.025$, which is approximately equal to `r round(qt(0.025, 36-1),3)`. We compare this value with the \@ref(eq:ttestFormula) by absolute and reject $\mathrm{H}_0$ if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level.

Visually, the whole process of hypothesis testing explained above can be represented in the following way:

```{r hypothesisTesting01, echo=FALSE, fig.cap="The process of hypothesis testing with t value."}
plot(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), xlab="t", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dt(seq(-5,5,0.1), 36-1),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qt(0.025, 36-1),0.01),rev(seq(-5,qt(0.025, 36-1),0.01))),
        c(dt(seq(-5,qt(0.025, 36-1),0.01),36-1), rep(0,length(seq(-5,qt(0.025, 36-1),0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1),5,0.01),rev(seq(qt(0.975, 36-1),5,0.01))),
        c(dt(seq(qt(0.975, 36-1),5,0.01),36-1), rep(0,length(seq(qt(0.975, 36-1),5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1), col="darkred", lwd=2)
abline(v=-3, col="darkblue", lwd=2)
legend("topright",legend=c("critical values","calculated t",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

If the blue line on Figure \@ref(fig:hypothesisTesting01) would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject $\mathrm{H}_0$. But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using \@ref(eq:confidenceInterval) and checking, whether the interval covers the $\mu=0$:
\begin{equation*}
    \begin{aligned}
        & \mu \in \left(-0.50 -2.03 \frac{1}{\sqrt{36}}, -0.50 + 2.03 \frac{1}{\sqrt{36}} \right), \\
        & \mu \in (-0.84, -0.16).
    \end{aligned}
\end{equation*}
In our case it does not, so we conclude that we reject $\mathrm{H}_0$ on 5% significance level. This can be roughly represented by the graph on  Figure \@ref(fig:hypothesisTesting02):

```{r hypothesisTesting02, echo=FALSE, fig.cap="Confidence interval for for the population mean example."}
plot(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), xlab="Sample mean", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01))),
        c(dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1),rep(0,length(seq(-1.5,0.5,0.01)))), col="grey95", lty=0)
lines(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-1.5,0.5), c(0,0), col="black", lwd=1)
polygon(c(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01),rev(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01))),
        c(dt((seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01),rev(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01))),
        c(dt((seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1)/6-0.5, col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c("bounds",TeX("$\\mu$=0"),TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from \@ref(eq:ttestFormula) to \@ref(eq:confidenceInterval). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject $\mathrm{H}_0$ on the selected significance level.

Also **note** that we never say that we accept $\mathrm{H}_0$, because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds.

Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure \@ref(fig:hypothesisTesting01) (calculated value). In R this can be done using the `pt()` function:
```{r}
pt(-3, 36-1)
```
Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately `r round(pt(-3, 36-1)*2, 4)`. This is the significance level, for which the switch from "reject" to "do not reject" happens. We could compare this value with the pre-selected significance level directly, rejecting $\mathrm{H}_0$ if it is lower than $\alpha$. This value is called "p-value" and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: **p-value** is the smallest significance level at which a null hypothesis can be rejected.

Despite this simplification, we still need to follow the procedure and select $\alpha$ *before conducting the test*! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also *note* that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. *This is still a binary process*: we either reject, or fail to reject $\mathrm{H}_0$, depending on whether p-value is smaller or greater than the selected significance level.

While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean:

1. We reject $\mathrm{H}_0$ because t-value is -3, which is smaller than the critical value of `r round(qt(0.025, 36-1),3)` (or equivalently the absolute of t-value is 3, while the critical is `r round(qt(0.975, 36-1),3)`);
2. We reject $\mathrm{H}_0$ because p-value is `r round(pt(-3, 36-1)*2, 4)`, which is smaller than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It does not include zero, so we reject $\mathrm{H}_0$.

In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject $\mathrm{H}_0$, the approach (3) gives more appropriate information. Consider the case, when we test, whether $\mu=-0.6$ in the example above. We then have the following three approaches to the problem:

1. We fail to reject $\mathrm{H}_0$ because t-value is 0.245, which is smaller than the critical value of `r round(qt(0.975, 36-1),3)`;
2. We fail to reject $\mathrm{H}_0$ because p-value is 0.808, which is greater than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It includes -0.6, so we fail to reject $\mathrm{H}_0$. *This does not mean that the true mean is indeed equal to -0.6*, but it means that the region will cover this number in 95% of cases if we do resampling many times.

In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply.

Furthermore, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values [@Wasserstein2016]. Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below:

1. P-values can indicate how incompatible the data are with a specified statistical model;
2. P-values do not measure:
- the probability that the studied hypothesis is true,
- or the probability that the data were produced by random chance alone;
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold;
4. Proper inference requires full reporting and transparency;
5. A p-value, or statistical significance, does not measure:
- the size of an effect
- or the importance of a result;
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than $\alpha$) is not a good idea [@Wasserstein2016]. Follow the proper procedure if you want to test the hypothesis.

Furthermore, the hypothesis testing mechanism has been criticised by many scientists over the years. For example, @Cohen1994 discussed issues with the procedure, making several important points, some of which are outlined above. He also points out at the fundamental problem with hypothesis testing, which is typically neglected by proponents of the procedure: in practice, null hypothesis is always wrong. In reality, it is not possible for a value to be equal, for example, to zero. Even an unimportant effect of one variable on another would be close to zero, but not equal to it. This means that with the increase of the sample size, $\mathrm{H}_0$ will inevitably be rejected.

::: remark
When formulating the null hypothesis as equality, it is in fact almost always wrong, because it is close to impossible for a parameter to be equal to a specific value. In the light of this, we should understand that the null hypothesis really means that the true value of parameter is in a proximity of the tested value. So, the hypotheses in this case should be understood as:
\begin{equation*}
    \mathrm{H}_0: \mu \approx a, \mathrm{H}_1: \mu \not\approx a ,
\end{equation*}
where $a$ is the value we are comparing our parameter with. Note that in case of one-tailed tests, this is no longer an issue, because the null hypothesis in that case compares the value with a set of values (e.g. $\mathrm{H}_0: \mu\leq a$).
:::

Finally, our mind operates with binary constructs: true / not true - while the hypothesis testing works in the dichotomy "**I know** / **I don't know**", with the latter appearing when there is not enough evidence to reject $\mathrm{H}_0$. As a result of this, people tend to make wrong conclusions, because it is difficult to distinguish "not true" from "I don't know", especially for those who do not know statistics well.

Summarising the discussion above, in my opinion, it makes sense to move away from hypothesis testing if possible and switch to other instruments for uncertainty measurement, such as confidence intervals.

<!-- The significance level is the probability of making a mistake if the hypothesis is true. It is never true (unless it is one sided), so you will make more mistakes than you expect. On the other hand, confidence interval will cover the true value in 1-alpha percent of cases. We don't know it, but we know that it will be there. -->


### Common mistakes related to hypothesis testing {#hypothesisTestingMistakes}
Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently.

1. "Calculated value is lower than the critical one, so the null hypothesis is true".
- This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the $\mathrm{H}_0$ being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the "Black swan in Australia". Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that "We fail to reject $\mathrm{H}_0$ that all swans are white". Uneducated people would be tempted to say that " $\mathrm{H}_0$: All swans are white" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting $\mathrm{H}_0$ and showing that "not all swans are white", which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting $\mathrm{H}_0$, collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly.
2. "Calculated value is lower than the critical one, so we accept the null hypothesis".
- We **never** accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not "accept $\mathrm{H}_0$". This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white).
3. "The parameter in the model is significant, so we can conclude that..."
- You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level $\alpha$. The correct sentence would be "The parameter in the model is significant on 3%, so we can conclude that...", where 3% is the selected significance level $\alpha$.
3. "The parameter in the model is significant because p-value<0.0000"
- Indeed, some statistical software will tell you that `p-value<0.0000`, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, "why bother if it is that low?". Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you.
5. "The parameter is not significant, so we remove the variable from the model".
- This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level ([assumptions](#assumptions) do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject $\mathrm{H}_0$. Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. *You always need to have some fundamental reason to include or remove variables in the model*. Hypothesis testing just gives you additional information that can be helpful for decision making.
6. "The parameter in the new model is more significant than in the old one".
- There is no such thing as "more significant" or "less significant". **Significance is binary** and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level $\alpha$ or not.
7. "The p-value of one variables is higher than the p-value of another, so...".
- p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake.

Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer [confidence intervals](#confidenceIntervals), when possible (as discussed above).


## Errors of types 0, I and II {#typeErrors}
When conducting a conventional statistical test, we can have one of the four situations, depending on what happens in real life and what results we obtain. They are summarised in Table \@ref(tab:typeErrorsTable).

```{r typeErrorsTable, echo=FALSE}
typeErrorsTable <- data.frame(C1 = c(rep("The data tells us",4)),
                              C2 = c(rep("Fail to reject $\\mathrm{H}_0$",2), rep("Reject $\\mathrm{H}_0$",2)),
                              C3 = c("Correct decision,", "Probability is $1-\\alpha$",
                                     "Type I error,", "Probability is $\\alpha$"),
                              C4 = c("Type II error,", "Probability is $\\beta$",
                                     "Correct decision,", "Probability is $1-\\beta$"))
colnames(typeErrorsTable) <- c("","","$\\mathrm{H}_0$ is true","$\\mathrm{H}_0$ is wrong")
typeErrorsTable <- kableExtra::kbl(typeErrorsTable, align="c", caption="Four outcomes in hypothesis testing.")
typeErrorsTable <- kableExtra::kable_paper(typeErrorsTable, full_width=FALSE)
typeErrorsTable <- kableExtra::column_spec(typeErrorsTable, 1, bold=TRUE)
typeErrorsTable <- kableExtra::add_header_above(typeErrorsTable, c(" "=2, "Reality" = 2))
typeErrorsTable <- kableExtra::collapse_rows(typeErrorsTable, columns=1:2)
kableExtra::kable_styling(typeErrorsTable, font_size=12, protect_latex=TRUE)
```

The Table \@ref(tab:typeErrorsTable) shows two hypothetical outcomes in reality (we never know, which one we have) and two outcomes of hypothesis testing. This gives us the $2\times 2$ matrix, where $\alpha$ is the significance level and $1-\beta$ is so called "Power of the Test" (discussed in detail in Subsection \@ref(powerOfTheTest)).

**Type I** error (aka "false positive", i.e. we find a positive effect, when we should not have found it) happens when the null hypothesis is actually true, but we reject it. The probability of this event is equal to $\alpha$. This is one of the definitions of the significance level $\alpha$ (in how many cases we are ready to make mistakes, when the null hypothesis is true).

**Type II** error (aka "false negative", i.e. we do not find effect, while we should have found it) happens when we fail to reject the wrong hypothesis ($\mathrm{H}_0$ is not true). The probability of this event equals to $\beta$, which can be calculated based on the assumed distribution, the critical and calculated values for the hypothesis.

In order to remember what Type I and Type II errors stand for, there is a good mnemonic with a story of a boy who cried "wolf".

::: example
Just as a reminder, in a village, there lived a boy who one day decided to make a practical joke of his fellow villagers. He ran around the main street crying "Wolf!". We should acknowledge that there was no wolf at that stage, so in our terms we would say that the $\mathrm{H}_0$: $\mu=0$ was true. But the villagers who have heard the boy went on the streets to help. They rejected the correct null hypothesis in order to help the boy, and they were surprised to find that there were no wolves on the streets. Thus the villagers made a **Type I error**.

Next week, the boy encountered a wolf on the main street and started crying "Wolf!", calling for help. Alas, this time nobody believed the boy and nobody came out to help, and thus the villagers rejected the correct null hypothesis in favour of the wrong one, $\mathrm{H}_1$: $\mu\neq 0$. By doing so they have made the **Type II error**. If the villagers knew statistics, they would understand that failing to reject $\mathrm{H}_0$ once does not mean that it is true.
:::

While we can regulate the probability of Type I error by changing $\alpha$, the probability of Type II error cannot be controlled directly. Ideally, we want it to be as low as possible. In general, the more information about the "true" parameters and model you can provide to the test, the lower the Type II error will be. For example, if we want to conduct a test to compare mean with a value and the CLT holds (see Section \@ref(CLT)), then you might want to choose between t-test and z-test. The latter assumes that the population standard deviation is known (and you can provide it), and as a result has a lower probability of Type II error than the former. We will discuss specific tests in the Section \@ref(statisticalTests).

All the four situations in Table \@ref(tab:typeErrorsTable) rely on the idea that the reality is somehow known. But in real life, we never know whether the null hypothesis is true or not. However, the Table is still useful because it gives an understanding of what to expect from a statistical test and what test to select in each specific situation.

Finally, sometimes analysts refer to the **Type 0** error (it is sometimes called "type III" error, but it is more fundamental than Type I or Type II, so I prefer "Type 0"). This is the error that arises when an answer is obtained to the wrong question. This does not have any mathematics behind it but is important in general: we need to understand what questions to ask and how to formulate them correctly before doing the test.


## Power of a test {#powerOfTheTest}
Power of a test is one of the important theoretical ideas related to hypothesis testing. It is **the probability of correctly rejecting the wrong hypothesis**. By definition, it is equal to $1-\beta$ in Table \@ref(tab:typeErrorsTable) and thus lies between 0 and 1, where the higher number reflects the higher power. While it is possible to calculate the power of a test, when the true value is known, in practice this does not make sense, because in reality we never know it. But the idea itself is useful because it allows comparing the theoretical properties of different statistical tests.

Given that the power of the test is a probability, it can be calculated for a specific test with specific parameters, assuming that a specific hypothesis is true. In this section, we will explain how to do that.


### Visual explanation
We start the explanation of the idea of the Power of a Test with a visual example. Consider an example of a z-test, which can be used for comparison of means if the population standard deviation is known (see Section \@ref(statisticalTests) for the explanation of the test). We will use an example of a one sided test for the following hypothesis (where number 3 is selected arbitrarily, just for the example):
\begin{equation}
    \mathrm{H}_0: \mu \geq 3, \mathrm{H}_1: \mu < 3.
    (\#eq:powerOfATestHypothesis)
\end{equation}
In this example we will test the hypothesis on 5% significance level. The general process of hypothesis testing can be visualised as shown in Figure \@ref(fig:powerOfTestVisual01).

```{r powerOfTestVisual01, echo=FALSE, fig.cap="The process of hypothesis testing of one sided hypothesis \\@ref(eq:powerOfATestHypothesis)."}
plot(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), xlab="z", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue", xlim=c(-7,4))
polygon(c(seq(-8,10,0.1),rev(seq(-8,10,0.1))),
        c(dnorm(seq(-8,10,0.1)),rep(0,length(seq(-8,10,0.1)))), col="grey95", lty=0)
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), lwd=1, lty=1, col="darkgreen")
lines(c(-8,10), c(0,0), col="black", lwd=1)
polygon(c(seq(-8,qnorm(0.05),0.01),rev(seq(-8,qnorm(0.05),0.01))),
        c(dnorm(seq(-8,qnorm(0.05),0.01)), rep(0,length(seq(-8,qnorm(0.05),0.01)))), col="lightgrey")
abline(v=qnorm(0.05), col="darkred", lwd=2)
abline(v=-2.66, col="darkblue", lwd=2)
legend("topright",legend=c("critical values","calculated z",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","lightgrey"))
```

The plot in Figure \@ref(fig:powerOfTestVisual01) shows the theoretical distribution of z-values, the 5% critical value and the calculated one. On the plot, we see that the calculated value lies in the tail of the distribution, which means that we can reject the null hypothesis on 5% significance level. The situation, when we correctly reject H$_0$ in our example corresponds to the case, when the true distribution lies to the left of the assumed one as shown in Figure \@ref(fig:powerOfTestVisual02).

```{r powerOfTestVisual02, echo=FALSE, fig.cap="Hypothetical \"true\" and the assumed distributions."}
plot(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), xlab="z", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue", xlim=c(-7,4))
polygon(c(seq(-8,10,0.1),rev(seq(-8,10,0.1))),
        c(dnorm(seq(-8,10,0.1)),rep(0,length(seq(-8,10,0.1)))), col="grey95", lty=0)
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), lwd=1, lty=1, col="darkgreen")
lines(c(-8,10), c(0,0), col="black", lwd=1)
polygon(c(seq(-8,qnorm(0.05),0.01),rev(seq(-8,qnorm(0.05),0.01))),
        c(dnorm(seq(-8,qnorm(0.05),0.01)), rep(0,length(seq(-8,qnorm(0.05),0.01)))), col="lightgrey")
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1),-4), lwd=2, lty=2, col="purple")
abline(v=qnorm(0.05), col="darkred", lwd=2)
abline(v=-2.66, col="darkblue", lwd=2)
abline(v=-4, col="purple", lwd=2, lty=2)
legend("topright",legend=c("critical values","calculated z","True distribution",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,2,6,6), col=c("darkred","darkblue","purple","grey95","lightgrey"), lty=c(1,1,2,1,1))
```

In the example of Figure \@ref(fig:powerOfTestVisual02) we consider a hypothetical situation, where the true mean is such that the standard normal distribution is centered around the value -4. In this example we correctly reject the wrong null hypothesis, which corresponds to the correct decision in Table \@ref(tab:typeErrorsTable) and the probability of this case is the "Power of a Test". Visually, it corresponds to the surface of the "true" distribution to the left of the critical value that we have chosen in the beginning - any calculated value below this will lead to the rejection of H$_0$ and thus to the correct decision. This is shown visually in Figure \@ref(fig:powerOfTestVisual03).

```{r powerOfTestVisual03, echo=FALSE, fig.cap="Power of the Test based on the hypothetical true value of $\\mu$."}
plot(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), xlab="z", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue", xlim=c(-7,4))
polygon(c(seq(-8,10,0.1),rev(seq(-8,10,0.1))),
        c(dnorm(seq(-8,10,0.1)),rep(0,length(seq(-8,10,0.1)))), col="grey95", lty=0)
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), lwd=1, lty=1, col="darkgreen")
lines(c(-8,10), c(0,0), col="black", lwd=1)
polygon(c(seq(-8,qnorm(0.05),0.01),rev(seq(-8,qnorm(0.05),0.01))),
        c(dnorm(seq(-8,qnorm(0.05),0.01)), rep(0,length(seq(-8,qnorm(0.05),0.01)))), col="lightgrey")
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1),-4), lwd=2, lty=2, col="purple")
polygon(c(seq(-8,qnorm(0.05),0.01),rev(seq(-8,qnorm(0.05),0.01))),
        c(dnorm(seq(-8,qnorm(0.05),0.01),-4), rep(0,length(seq(-8,qnorm(0.05),0.01)))),
        col=rgb(0.9,0.8,0.9,0.5), lty=2, border="purple", lwd=2)
abline(v=qnorm(0.05), col="darkred", lwd=2)
legend("topright",legend=c("critical values","True distribution",TeX("$1-\\alpha$"),TeX("$\\alpha$"),"Power of the Test"),
       lwd=c(2,2,6,6,6), col=c("darkred","purple","grey95","lightgrey",rgb(0.9,0.8,0.9,0.5)), lty=c(1,2,1,1,1))
```

The surface to the left of the critical value in Figure \@ref(fig:powerOfTestVisual03) is the Power of the Test for the example of a specific value of assumed $\mu$. Given that we never know the true value, we could try other values, which would shift the distribution to the left or to the right, meaning either the increase or the decrease in power of the test. For example, the situation shown in Figure \@ref(fig:powerOfTestVisual04) corresponds to the smaller Power of the Test (because the surface of the distribution to the left is smaller than the surface of the distribution in Figure \@ref(fig:powerOfTestVisual03)).

```{r powerOfTestVisual04, echo=FALSE, fig.cap="Power of the Test based on another hypothetical true value of $\\mu$."}
plot(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), xlab="z", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue", xlim=c(-7,4))
polygon(c(seq(-8,10,0.1),rev(seq(-8,10,0.1))),
        c(dnorm(seq(-8,10,0.1)),rep(0,length(seq(-8,10,0.1)))), col="grey95", lty=0)
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1)), lwd=1, lty=1, col="darkgreen")
lines(c(-8,10), c(0,0), col="black", lwd=1)
lines(seq(-8,10,0.1), dnorm(seq(-8,10,0.1),-2.5), lwd=2, lty=2, col="purple")
polygon(c(seq(-8,qnorm(0.05),0.01),rev(seq(-8,qnorm(0.05),0.01))),
        c(dnorm(seq(-8,qnorm(0.05),0.01),-2.5), rep(0,length(seq(-8,qnorm(0.05),0.01)))),
        col=rgb(0.9,0.8,0.9,0.5), lty=2, border="purple", lwd=2)
abline(v=qnorm(0.05), col="darkred", lwd=2)
legend("topright",legend=c("critical values","True distribution","Power of the Test"),
       lwd=c(2,2,6), col=c("darkred","purple",rgb(0.9,0.8,0.9,0.5)), lty=c(1,2,1))
```

Analysing Figures \@ref(fig:powerOfTestVisual03) and \@ref(fig:powerOfTestVisual04), we can already outline two factors impacting the power of a test:

1. The location of the true mean. The further it is away to the tested one, the easier it is to detect the difference and reject the H$_0$, which implies a higher power of a test;
2. Significance level $\alpha$. With the lower significance level, the critical value (vertical line in Figures \@ref(fig:powerOfTestVisual03) and \@ref(fig:powerOfTestVisual04)) will be further to the left and thus the power of the test will be lower.

There are other factors, which are not as obvious as the two above. For example, if we did not know the true value of $\sigma$, we would need to estimate it, and as a result we would need to use a less powerful t-test instead of the z-test. On smaller samples the critical value of the t-test is typically higher than the one from the z-test by absolute value. For example, in case of 36 observations, on 5% significance level it is equal to `r round(qt(0.05, 35),4)`, which is lower than the similar value of `r round(qnorm(0.05),4)` from the standard normal distribution. This means that if we used the t-test instead of the z-test in the example above, the vertical line on the plots in Figures \@ref(fig:powerOfTestVisual03) and \@ref(fig:powerOfTestVisual04) would be firther to the left of the one that we had. As a result, we would conclude that the power of the t-test is lower than the power of the z-test.

Furthermore, with the increase of the sample size the distribution of means tends to become narrower due to the Law of Large Numbers (see Section \@ref(LLN)) and thus the power of tests grow, because the critical value moves closer to the centre of distribution.


### Mathematical explantion
Moving from the visual explanation to the mathematical one, we can present the Power of a Test as the following probability:
\begin{equation}
    \mathrm{P}(\text{reject H}_0 | \mathrm{H}_0 \text{ is wrong}) = 1 - \mathrm{P}(\text{Type II error}) = 1-\beta.
    (\#eq:powerOfATestConcept)
\end{equation}
As shown in the previous Subsection, the calculation of the Power of the Test is done based on the parameters of the specific test. In this Subsection, we continue with the same example as before and the same hypothesis:
\begin{equation*}
    \mathrm{H}_0: \mu \geq 3, \mathrm{H}_1: \mu < 3.
\end{equation*}
For the example, we assume that the population standard deviation is known and is equal to $\sigma=0.18$, that we work with a sample of 36 observations and that we use a 5% significance level to test the hypothesis. In this case, the test statistics is:
\begin{equation}
    z = \frac{\bar{y}-3}{\sigma/\sqrt{n}} = \frac{\bar{y}-3}{0.03} ,
    (\#eq:powerOfATest01)
\end{equation}
where $\bar{y}$ is the sample mean. The rule for rejecting the null hypothesis in this situation is that if the calculated value $z$ is lower than or equal to the critical one, which in our case (for the chosen 5% significance level) is -1.645:
\begin{equation*}
    z = \frac{\bar{y}-3}{\sigma/\sqrt{n}} = \frac{\bar{y}-3}{0.03} \leq -1.645
\end{equation*}
Solving this inequality for $\bar{y}$, we will reject H$_0$ if the sample mean is
\begin{equation}
    \bar{y} \leq 3 -1.645 \times 0.03 = 2.95 .
    (\#eq:powerOfATest02)
\end{equation}
This can be interpreted as "we will fail to reject the null hypothesis in the cases, when the sample mean is greater than 2.95". Now that we have this value, we can calculate the theoretical power of the test for a variety of cases. For example, we can see how powerful the test is in rejecting the wrong null hypothesis if the true mean is in fact equal to 2.87:
\begin{equation}
    \begin{aligned}
        1-\beta = & \mathrm{P}(\bar{y}\leq 2.95 | \mu=2.87) = \\
                  & \mathrm{P}\left(z \leq \frac{2.95 - 2.87}{0.03} \right) = \\
                  & \mathrm{P}\left(z \leq 2.67 \right) = \\
                  & 0.9962
    \end{aligned}
    (\#eq:powerOfATest03)
\end{equation}
We could do similar calculations for other cases of the true mean and see how powerful the test is in those situations. In fact, we could create a **power curve**, showing how the power of the test changes in a variety of cases of hypothetical true mean. In R, this can be construct in the following way:

```{r powerCurve, fig.cap="Power curve for the z-test in the example."}
# Set all the parameters
yMean <- 3
yMeanSD <- 0.18 / sqrt(36)
levelSignigicance <- 0.05
zValue <- 3 +qnorm(levelSignigicance) * yMeanSD
# Vector of hypothetical population means
muValues <- seq(2.8,3.1,length.out=100)
# Vector of values for power curve
powerValues <- vector("numeric",100)

# Calculate the power values
powerValues <- pnorm((zValue - muValues)/yMeanSD)

# Plot the power curve
plot(muValues, powerValues, type="l",
     xlab=latex2exp::TeX("$\\mu$"),
     ylab="Power of the test")
# Add lines for the case of 1-beta=0.05
lines(rep(3,2),c(0,0.05), lty=3)
lines(c(0,3),rep(0.05,2), lty=3)
# And provide a description
text(3, 0.05+0.05,
     latex2exp::TeX("$\\alpha=1-\\beta=0.05$"),
     pos=4)
```

The plot in Figure \@ref(fig:powerCurve) shows how powerful the z-test is for each specific value of population mean. We can see that the test becomes more powerful the further the true mean is away from the tested value (we chose 3 as the tested value). This means that it is easier for the test to detect the distance of the sample mean from the true value, when the true value is, for instance, 2.8 than in the case, when it is 2.95.

There is one specific point, where the power of the test coincides with the significance level. It is the case, when the population mean is indeed equal to 3. In this situation rejecting the null hypothesis would result in Type I error, which is equivalent to the significance level $\alpha$, which we chose to be equal to 0.05.

In general, there are several things that influence the power of any statistical test:

1. The value of the true parameter;
2. The significance level;
3. The sample size;
4. The amount of information available for the test.

The element (1) is depicted on the plot in Figure \@ref(fig:powerCurve). If we conduct the test about the wrong value of the true mean, then the distance from it will impact the power: the further it is away, the more powerful the test will be, being able to tell the difference between the sample mean and the true mean.

The element (2) will define the critical value of a statistical test, and in general the smaller the significance level is, the less powerful the test will be, as we will not be able to spot small discrepancies from the true mean.

The larger the sample size (element (3)), the more powerful the test becomes in general. In our example, we can see that from the equation \@ref(eq:powerOfATest01), where the sample size $n$ is in the denominator of the denominator. The higher values of $n$ will lead to the higher values of $z$ and as a result, the higher chance of rejecting the H$_0$ if it is wrong. Figure \@ref(fig:powerCurves02) demonstrates how the power curve changes with the increase of the sample size. we see that the power of the test increases much faster with the decrease of the hypothetical value of $\mu$, when the sample size is large (for example, $n=1000$) than in the case of small samples (e.g., $n=25$).


```{r powerCurves02, fig.cap="Power curves with different sample sizes.", echo=FALSE}
muValues <- seq(2.8,3.1,length.out=1000)
plot(muValues, pnorm((zValue - muValues)/(yMeanSD*6/sqrt(1000))), type="l",
     xlab=TeX("$\\mu$"), ylab="Power of the test")
lines(muValues, pnorm((zValue - muValues)/(yMeanSD*6/sqrt(300))), col="blue")
lines(muValues, pnorm((zValue - muValues)/(yMeanSD*6/sqrt(100))), col="purple")
lines(muValues, pnorm((zValue - muValues)/(yMeanSD*6/sqrt(50))), col="red")
lines(muValues, pnorm((zValue - muValues)/(yMeanSD*6/sqrt(25))), col="orange")
legend("topright", legend=c("n=1000","n=300","n=100","n=50","n=25"),
       col=c("black","blue","purple","red","orange"), lwd=1)
```

Finally, the more general point about the "amount of information" applies to the selection between the tests. In the Section \@ref(statisticalTests) we will discuss a variety of tests and we will discuss the conditions under which some of them are more powerful than the others. But in general the rule is: the more a priori information you can provide to the test, the easier it becomes to detect deviations from the tested value, because the uncertainty caused by estimation of additional parameters is decreased in this case.


## Statistical vs practical significance {#significance}
We have already discussed what the significance level means and how it connects with Type I and Type II errors in the previous sections. We have noticed that the significance level $\alpha$ is non-linearly related with the Type II probability $\beta$ and have spotted that there is a relation between them and subsequently relation between significance level and the power of a test. In fact, coming back to Table \@ref(tab:typeErrorsTable), we can say that there is a trade-off between them. If we use a very low significance level then we will make fewer mistakes when testing the correct hypothesis (failing to reject the correct H$_0$), but we will make more mistakes when the null hypothesis is wrong, because the power of the test will be lower. Sometimes, this can be compensated by choosing more powerful statistical tests (see Section \@ref(statisticalTests)) or increasing the sample size, but this is not always possible to do. On the other hand, choosing a high significance level means that we will make more Type I errors, rejecting the null hypothesis when it is correct, but at the cost of making fewer Type II errors, rejecting the H$_0$, when it is wrong even if the difference between the true unknown mean and the sample one is small. This trade-off can be taken into account, when an analyst needs to decide what significance level to use.

If you are unsure what significance level to choose, Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many times in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical tests in various circumstances.

However, there is an important aspect that should also be considered when making decisions based on results of statistical tests - "practical significance". While it is not universally measurable, it is an aspect that is worth considering when making decisions. To better understand it, consider the following artificial example. A company creating helmets for cyclists has collected data about cyclists injuries for two cases: when they wear helmets and when they do not. They found that the cyclists that do not wear helmets get in car accidents less often than the cyclists that do. The probability of the event for them is just 1%, while it is 5% for the latter group (and the difference was statistically significant on 5%). Based on this a cyclist that have only started learning a statistics course can make a conclusion that they should not wear a helmet because it will decrease the chance of accident. However, the company has also analysed the types of injuries that cyclists get in case of accidents, and found that in 2% of the cases, those cyclists that do not wear helmets die in accidents, while this number for those that wear helmets is 1%. The company pointed out that the difference between the two situations was not statistically significant on 5% level. So, a person learning statistics would be inclined to conclude based on that they should not wear a helmet, because the two situations are not statistically different. This would be a wrong decision because it does not consider the practical significance.

Indeed, in the example above, based on the data collected by a company, the mortality in two cases is similar in terms of statistical significance. However, having the two times higher probability of death when not wearing a helmet than in the case with a helmet has serious practical implications. Getting in an accident is unpleasant and is associated with some costs (financial, moral etc), but dying has a much higher cost, incomparable with that. And even though the probability of dying in an accident without helmet is low (only 2%), it does not mean that a person not wearing a helmet will be lucky enough to appear in that 98% of cases, when an accident happens. Even 2% is enough for an event with such a critical outcome - this can happen any time with anyone. And although the probability of death in case of "wearing a helmet" is just 1% lower, given the asymmetry of costs, it is better to wear a helmet and increase the odds of survival by that one percent than to continue gambling. After all, your head is one of the most important parts of your body.

The situation above is artificial, I could not find appropriate data for this example. In reality, the numbers might be different, but the message is the same: you should consider practical implications of statistical analysis, when making decisions. Taking both statistical and practical significance into account, we can crate a table demonstrating the four possible cases for decision making (see Table \@ref(tab:significanceTable))

```{r significanceTable, echo=FALSE}
significanceTable <- data.frame(C1 = c("Practically significant","Practically insignificant"),
                                C2 = c("Make a decision","Think about it and do not make a decision"),
                                C3 = c("Think about it and make a decision","Do not make a decision"))
colnames(significanceTable) <- c("","Statistically significant","Statistically insignificant")
significanceTable <- kableExtra::kbl(significanceTable, align="c", caption="Making decisions in the case of practical vs statistical significance.")
significanceTable <- kableExtra::kable_paper(significanceTable, full_width=FALSE)
significanceTable <- kableExtra::column_spec(significanceTable, 1, bold=TRUE)
kableExtra::kable_styling(significanceTable, font_size=12, protect_latex=TRUE)
```

In Table \@ref(tab:significanceTable), there are two situations, when there is nothing to argue about: when practical and statistical significances agree with each other (either they are both significant or not). However, I argue that the practical significance is in general more important than the statistical one. If you find that a new decision will reduce costs but the reduction will not be statistically significant, then it makes sense to make that decision anyway. On the other hand, if the decision is statistically significant (for instance, it improves the process by 1%, being significant on the selected level), but it is not practically significant (the costs of implementing it are higher than the savings from it) then it should not be made. This is because the statistical outcomes are always associated with potential Type I and Type II errors discussed in Section \@ref(typeErrors) and thus not finding difference could be due to Type II error, while finding one could be due to Type I error. When it comes to making decisions, the results of statistical testing should only help in supporting them, rather than guiding them.
