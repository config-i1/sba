# Statistical tests {#statisticalTests}
Having discussed the idea of hypothesis testing, we can now move to the discussion of specific tests. In this chapter, we will start the discussion with the tests for the means of random variables, then move towards the variance and to the comparison of several variables. The tests in this chapter are introduced based on the needs of an analyst. We finish the chapter with a discussion of how to select the appropriate statistical test for your problem.

Before we proceed, we need to define two terms, which will be used in this chapter. **Parametric statistical test** is the test that fully relies on distributional assumptions about the random variable. For example, we can assume that the variable follows Normal distribution and thus we can use a parametric test. **Non-parametric statistical test** is the test that does not rely on distributional assumptions. These types of test are typically conducted on ranked data rather than on the original one, reducing the scale of information to the ordinal one (see Section \@ref(scales)). The parametric ones are typically more powerful than their non-parametric counterparts if the assumptions hold. If they do not, the non-parametric ones become more powerful.


## One-sample tests about mean {#statisticalTestsOneSampleMean}
Consider an example, where we collected the data of height of 100 males across England for the 2021. Based on that sample we want to see if the average height is 175.3 cm, as it was claimed by NHS back in 2012. Based on our sample, we found that the mean height was 176.1 cm. Can we say that the height has increased or did we get this value just because of the pure randomness? How do we formulate and test such hypothesis? We need to follow the procedure described in Section \@ref(hypothesisTestingBasics). We start by formulating null and alternative hypotheses:
\begin{equation*}
    \mathrm{H}_0: \mu = 175.3, \mathrm{H}_1: \mu \neq 175.3 .
\end{equation*}
Next, we select the significance level. If the level is not given to us, then we need to choose one based on our preferences. I like 1%, because I am risk averse.

After that, we select the appropriate test. The task described above focuses on investigating the hypothesis about the mean. If we can assume that the CLT holds (see Section \@ref(CLT)), then we can use a parametric statistical test, because we know that the sample mean will follow Normal distribution in that case. In our example, we can indeed assume that it holds, because we deal with a sample of 100 observations, and we can also assume that the distribution of height across England is symmetric (we do not expect people to have extreme heights of, let's say, 5 meters or 0.5 meters). The next thing to consider is whether the population standard deviation of the mean height is known or not. Based on that, we would choose either z-test or t-test.


### z-test {#statisticalTestsOneSampleMeanZ}
Consider the situation, where we know from NHS that the population standard deviation of height is $\sigma=5$ (typically, we do not know it, except for some very rare cases, when this is given by design, e.g. due to how some machine is calibrated).

::: remark
As shown in Section \@ref(confidenceInterval), if the standard deviation of $y$ is $\sigma$, then the standard deviation of $\bar{y}$ is $\frac{1}{\sqrt{n}} \sigma$. This means that in our case $\sigma_{\bar{y}}=\frac{1}{\sqrt{100}} \times 5 =0.5$.
:::

If we assume that the mean follows Normal distribution and know the population standard deviation, i.e. $\bar{y} \sim \mathcal{N}\left(\mu, \sigma^2_{\bar{y}}\right)$, then the standardised value z:
\begin{equation}
    z = \frac{\bar{y}-\mu}{\sigma_{\bar{y}}} .
    (\#eq:zTestFormula)
\end{equation}
will follow standard normal distribution: $z \sim \mathcal{N}\left(0, 1\right)$. Knowing these properties, we can conduct the test using one of the three approaches discussed in Section \@ref(hypothesisTestingBasics). First, we could calculate the z value based on formula \@ref(eq:zTestFormula):
\begin{equation*}
    z = \frac{176.1 -175.3}{0.5} = 1.6 
\end{equation*}
and compare it with the critical one. Given that we test the two-sided hypothesis (because the alternative is "inequality"), he critical value should be split into two parts, to have $\frac{\alpha}{2}$ in one tail and $\frac{\alpha}{2}$ in the other one. The critical values can be calculated, for example, in R using the code below:

```{r}
# I have previously selected significance level of 1%
alpha <- 0.01
qnorm(c(alpha/2, 1-alpha/2), 0, 1)
```

We see that the calculated value lies inside the interval, so we fail to reject the null hypothesis on 1% significance level. The simplified version for this procedure is to compare the absolute of the calculated value with the absolute of the critical one. If the calculated is greater than the critical, then we reject H$_0$. In our case 1.6 < 2.58, so we fail to reject H$_0$.

Another way of testing the hypothesis is by calculating the p-value and comparing it with the significance level. In R, this can be done using the command:

```{r}
(1-pnorm(abs(1.6)))*2
```

In the R code above we are calculating the surface in the tails of distribution. Thus the appearance of the number 2, to add the surfaces in two tails. This procedure is shown in Figure \@ref(fig:hypothesisTestingZTestPvalue).

```{r hypothesisTestingZTestPvalue, echo=FALSE, fig.cap="The process of p-value calculation for the z-test."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab="z", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkgreen")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qnorm(1-pnorm(abs(1.6))),0.01),rev(seq(-5,qnorm(1-pnorm(abs(1.6))),0.01))),
        c(dnorm(seq(-5,qnorm(1-pnorm(abs(1.6))),0.01)), rep(0,length(seq(-5,qnorm(1-pnorm(abs(1.6))),0.01)))), col="lightblue")
polygon(c(seq(qnorm(pnorm(abs(1.6))),5,0.01),rev(seq(qnorm(pnorm(abs(1.6))),5,0.01))),
        c(dnorm(seq(qnorm(pnorm(abs(1.6))),5,0.01)), rep(0,length(seq(qnorm(pnorm(abs(1.6))),5,0.01)))), col="lightblue")
# abline(v=qnorm(c(1-pnorm(abs(1.6)),pnorm(abs(1.6)))), col="darkred", lwd=2)
abline(v=c(-1.6,1.6), col="darkblue", lwd=2)
text(-1.65,0.02,"-1.6",pos=4)
text(1.65,0.02,"1.6",pos=2)
legend("topright",legend=c("calculated z","p-value"),
       lwd=c(2,6), col=c("darkblue","lightblue"))
```

Comparing the p-value of `r round((1-pnorm(abs(1.6)))*2,4)` with the significance level of 0.01, we can conclude that we fail to reject the null hypothesis. This means that based on the collected sample, we cannot tell the difference between the population mean height of 175.3 and the sample height of 176.1.


### t-test {#statisticalTestsOneSampleMeanT}
The population standard deviation is rarely known. The more practical test is the t-test, which relies on the relation between the normal distribution and the Student's distribution. If $y \sim \mathcal{N}(\mu ,\sigma^2)$ and $s=\sqrt{\frac{1}{n}\sum_{j=1}^n{\left(y_j-\bar{y}\right)^2}}$ is the estimate of the sample standard deviation, then $t \sim \mathcal{t}(n-1)$ where
\begin{equation}
    t = \frac{\bar{y}-\mu}{s_{\bar{y}}} 
    (\#eq:tTestFormulaAgain)
\end{equation}
and $n-1$ is the number of degrees of freedom. The reason why we need to use Student's distribution in this case rather than the Normal one is because of the uncertainty arising from the estimation of the standard deviation $s$. The hypothesis testing procedure in this case is exactly the same as in the case of z-test. We insert the values in \@ref(eq:tTestFormulaAgain) to get the calculated t, then compare it with the critical and make a conclusion. Consider an example, where the estimated standard deviation $s=4$. We then get:
\begin{equation*}
    t = \frac{176.1-175.3}{0.4} = 2 ,
\end{equation*}
while the critical value for the chosen 1% significance level is:

```{r}
qt(c(alpha/2, 1-alpha/2), 100-1)
```

Given that the calculated value of 2 is lower than `r round(qt(1-alpha/2, 100-1),3)`, we fail to reject the null hypothesis on 1% significance level. So, we again conclude that we cannot tell the difference between the mean in the data and the assumed population mean.

In R, the same procedures can be done using the `t.test()` function from `stats` package. Here is an example, demonstrating how the test can be done for the generated data:

```{r echo=FALSE}
set.seed(41)
```

```{r}
y <- rnorm(100, 175.3, 5)
# Note that our significance level is 1%,
# so we ask to produce 99% confidence interval
t.test(y, mu=175.3, alternative="two.sided", conf.level=0.99)
```

```{r echo=FALSE}
test <- t.test(y, mu=175.3, alternative="two.sided",conf.level=0.99)
```

::: remark
If we were to test a one-sided hypothesis (e.g. $\mathrm{H}_0: \mu < 175.3, \mathrm{H}_1: \mu \geq 175.3$), then we would need to change the `alternative` parameters in the `t.test()` function to correspond to the formulated H$_1$.
:::

The output above shows the calculated t (`r format(test$statistic,digits=5)`), the number of degrees of freedom and the p-value. It also constructs the 99% confidence interval (`r paste0(format(test$conf.int[1:2],digits=7),collapse=", ")`). The conclusions can be made using one of the three approaches, discussed above and in Section \@ref(hypothesisTestingBasics):

1. The calculated value is `r format(test$statistic, digits=5)`, which is lower than the critical one of `r format(qt(1-alpha/2, 100-1), digits=5)` as discussed earlier, so we fail to reject H$_0$;
2. The p-value is `r format(test$p.value, digits=3)`, which is greater than the selected significance level of 1%, so we fail to reject H$_0$;
3. The 99% confidence interval includes the tested value of 175.3, so we fail to reject the H$_0$ on the 99% confidence level.

As mentioned in Section \@ref(hypothesisTestingBasics), I personally prefer the last approach of the three because it gives more information about the uncertainty around the estimate of the sample mean.


### Non-parametric, one-sample Wilcoxon test
In some situations, the CLT might not hold due to violation of some of assumptions. For example, the distribution of the random variable is expected to be asymmetric with a long tail. In this case, the mean might not be finite and thus the CLT would not hold. Alternatively, the sample size might be too small to assume that CLT has started working. In these cases, the parametric tests for the mean will not be powerful enough (see discussion in Section \@ref(powerOfTheTest)) and would lead to wrong conclusions about the tested hypothesis.

One of the solutions in this situation is a non-parametric test that does not have distributional assumptions. In the case of the hypothesis about the mean of a random variable, we could use Wilcoxon test. The null hypothesis in this test can be formulated in a variety of ways, one of which is the following:
\begin{equation*}
    \begin{aligned}
        &\mathrm{H}_0: \text{ distribution is symmetric around } \mu = 175.3, \\
        &\mathrm{H}_1: \text{ distribution is not symmetric around } \mu = 175.3 .
    \end{aligned}
\end{equation*}
If the H$_0$ is true in this case, then it means that the mean will coincide with the centre of distribution, which should be around the tested value. If it is not symmetric, then possibly the centre of distribution is not around the tested value. The test is done on the ranked data, sorting the values of $y$ from the lowest to the highest and assigning the numerical values to them. After that the test values is calculated.

Given that the test does not rely on distributional assumptions, it is less powerful than the parametric tests on large samples, but it is also more powerful on the small ones.

In R, the test can be conducted using `wilcox.test()` function from `stats` package:

```{r}
wilcox.test(y, mu=175.3, alternative="two.sided")
```

```{r echo=FALSE}
test <- wilcox.test(y, mu=175.3, alternative="two.sided")
```

Similar to how we have done that with t-test, we can compare the p-value with the significance level (reminder: we have chosen 1%) and make a conclusion. Based on the output above, we fail to reject H$_0$ because `r format(test$p.value,digits=3)`>0.01.This means that once again, we cannot tell the difference between the sample mean and the population mean of 175.3.


## One-sample test about variance {#statisticalTestsOneSampleVariance}
Another example of a situation that could be potentially interesting in practice is when we are not sure about the estimated variance of a distribution. We might want to understand, whether the variance in population is similar to the value we obtained based on our sample. For illustrative purposes, consider the continued example with the height of humans. After collecting the sample of 30 observations, it was found that the variance of height is 100. However, based on previous survey, we know that the population variance of the height is 121. We want to know whether the in-sample estimate of variance is significantly lower from the population one on 1% significance level. This hypothesis can be formulated as:
\begin{equation*}
    \mathrm{H}_0: \sigma^2 \geq 121, \mathrm{H}_1: \sigma^2 < 121 .
\end{equation*}
where $\sigma^2$ is the population variance. The conventional test for this hypothesis is the Chi-squared test ($\chi^2$). This is a parametric test, which means that it relies on distributional assumptions. However, unlike the test about the mean from the Section \@ref(statisticalTestsOneSampleMean) (which assumed that CLT holds, see Section \@ref(CLT)), the Chi-squared test relies on the assumption about the random variable itself:
\begin{equation*}
    y_j \sim \mathcal{N}(\mu, \sigma^2) ,
\end{equation*}
which means that (as discussed in Section \@ref(statisticalTestsOneSampleMeanZ)):
\begin{equation*}
    z_j = \frac{y_j - \mu}{\sigma} \sim \mathcal{N}(0, 1) .
\end{equation*}
Coming back to the variance, we would typically use the following formula to estimate it in sample:
\begin{equation*}
    \mathrm{V}\left( y \right) = \frac{1}{n-1} \sum_{j=1}^n \left(y_j - \bar{y} \right)^2,
\end{equation*}
where $\bar{y}$ is an unbiased, efficient and consistent estimate of $\mu$ (Section \@ref(estimatesProperties)). If we divide the variance by the true value of the population one, we will get the following:
\begin{equation*}
    \frac{\mathrm{V}\left( y \right)}{\sigma^2} = \frac{1}{n-1} \sum_{j=1}^n \frac{\left(y_j - \bar{y} \right)^2}{\sigma^2},
\end{equation*}
or after multiplying it by $n-1$:
\begin{equation}
    \chi^2 = (n-1) \frac{\mathrm{V}\left( y \right)}{\sigma^2} = \sum_{j=1}^n z_j^2 .
    (\#eq:chisqFormula)
\end{equation}
Given the assumption of normality of the random variable $z_j$, the variable \@ref(eq:chisqFormula) will follow the Chi-squared distribution with $n-1$ degrees of freedom (this is the definition of the Chi-squared distribution). This property can be used to test a statistical hypothesis about the variance.

The hypothesis testing itself is done using the same procedure as before (Section \@ref(hypothesisTestingBasics)). After formulating the hypothesis and selecting the significance level, we use the formula \@ref(eq:chisqFormula) to get the calculated value:
\begin{equation*}
    \chi^2 = (30-1) \times \frac{100}{121} \approx 23.97
\end{equation*}
Given that the alternative hypothesis in our example is "lower than", we should compare the obtained value with the critical one from the left tail of the distribution, which on 1% significance level is:

```{r}
qchisq(0.01,29)
```
Comparing the critical and calculated values ($23.97>14.26$), we conclude that we fail to reject the null hypothesis. This means that the sample variance is not statistically different from the population one. The test of this hypothesis is shown visually in Figure \@ref(fig:hypothesisTestingChiSquared), which shows that the calculated value is not in the tail of the distribution (thus "fail to reject").

```{r hypothesisTestingChiSquared, echo=FALSE, fig.cap="The process of hypothesis testing in Chi-squared distribution (one-tailed case)."}
plot(seq(0,100,0.1), dchisq(seq(0,100,0.1),29), xlab=TeX("$\\chi^2$$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(0,100,0.1),rev(seq(0,100,0.1))),
        c(dchisq(seq(0,100,0.1),29),rep(0,length(seq(0,100,0.1)))), col="grey95", lty=0)
lines(seq(0,100,0.1), dchisq(seq(0,100,0.1),29), lwd=1, lty=1, col="darkgreen")
lines(c(0,100), c(0,0), col="black", lwd=1)
polygon(c(seq(0,qchisq(0.01,29),0.01),rev(seq(0,qchisq(0.01,29),0.01))),
        c(dchisq(seq(0,qchisq(0.01,29),0.01),29), rep(0,length(seq(0,qchisq(0.01,29),0.01)))), col="grey")
abline(v=qchisq(0.01,29), col="darkblue", lwd=2)
text(14.2,0.006,"14.26",pos=4)
abline(v=23.97, col="darkred", lwd=2)
text(24,0.004,"23.97",pos=4)
legend("topright",legend=c(TeX("critical $\\chi^2$$"),TeX("calculated $\\chi^2$$"),TeX("$\\alpha =0.01$")),
       lwd=c(2,2,6), col=c("darkblue","darkred","grey"))
```

A thing to keep in mind about the Chi-square distribution is that it is in general asymmetric (it converges to normal with the increase of the sample size, see discussion in Chapter \@ref(distributions)). This means that if the alternative hypothesis is formulated as "not equal", the absolutes of critical values for the left and right tails will differ. This situation is shown in Figure \@ref(fig:hypothesisTestingChiSquared2) with the example of 5% significance level, which is split into two equal parts of 2.5% with critical values of 16.05 and 45.72.

```{r hypothesisTestingChiSquared2, echo=FALSE, fig.cap="The process of hypothesis testing in Chi-squared distribution (two-tailed case)."}
plot(seq(0,100,0.1), dchisq(seq(0,100,0.1),29), xlab=TeX("$\\chi^2$$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(0,100,0.1),rev(seq(0,100,0.1))),
        c(dchisq(seq(0,100,0.1),29),rep(0,length(seq(0,100,0.1)))), col="grey95", lty=0)
lines(seq(0,100,0.1), dchisq(seq(0,100,0.1),29), lwd=1, lty=1, col="darkgreen")
lines(c(0,100), c(0,0), col="black", lwd=1)
polygon(c(seq(0,qchisq(0.025,29),0.01),rev(seq(0,qchisq(0.025,29),0.01))),
        c(dchisq(seq(0,qchisq(0.025,29),0.01),29), rep(0,length(seq(0,qchisq(0.025,29),0.01)))), col="grey")
abline(v=qchisq(0.025,29), col="darkblue", lwd=2)
polygon(c(seq(100,qchisq(0.975,29),-0.01),rev(seq(100,qchisq(0.975,29),-0.01))),
        c(dchisq(seq(100,qchisq(0.975,29),-0.01),29), rep(0,length(seq(100,qchisq(0.975,29),-0.01)))), col="lightgrey")
abline(v=qchisq(0.975,29), col="darkblue", lwd=2)
legend("topright",legend=c("critical values","95%","left 2.5%", "right 2.5%"),
       lwd=c(2,6,6,6), col=c("darkblue","grey95","grey","lightgrey"))
```

The surfaces in the tails in Figure \@ref(fig:hypothesisTestingChiSquared2) are equal, but because of the asymmetry of distribution they look different.
<!-- Furthermore, Chi-squared distribution supports positive values only, which is why its left tail starts from zero, while the right one converges to zero asymptotically. -->

::: remark
If an analyst needs to conduct the test about the variance of the mean, then only CLT is required to hold, no additional assumptions about the distribution of the random variable $y_j$ are required.
:::

Chi-squared test is also used in many other situations, for example to test the relation between categorical variables (see Section \@ref(correlationsNominal)) or to test the "goodness of fit". We do not discuss them in this Chapter.

Finally, there are non-parametric analogues of the Chi-squared test, but their discussion lies outside of the scope of this textbook.


<!-- ## Two-sample tests for mean -->
<!-- ### t-test -->
<!-- ### Wilcoxon -->
<!-- ### Mann-Whitney -->

<!-- ## Two-sample test for variance -->
<!-- ### F-test -->
<!-- ### Conover squared ranks test -->
<!-- Anything else? -->

<!-- ## Multiple sample tests -->
<!-- ### F-test -->
<!-- ### Kruskal-Wallis -->
<!-- ### Friedman -->
<!-- ### Nemenyi / MCB -->

<!-- ## Test to check the distribution -->
<!-- ### Shapiro-Wilk -->
<!-- ### Kolmogorov-Smirnov -->


<!-- The group of tests united by the term "Parametric statistical tests" includes those of them that have some distributional assumptions and typically rely on moments of distributions. These tests are typically more powerful than their non-parametrical counterparts, because they rely on more assumptions. In this Section we discuss the most popular tests that are used in practice. -->


<!-- ## Selecting appropriate statistical test {#statisticalTestsSelection} -->

