# Simple Linear Regression {#simpleLinearRegression}
::: example
A timber harvesting company "Timber Lend" needs to measure the volume of trees they cut. While they could measure the volume using physics principles, this is time consuming and they want to speed up the process. They have collected data of 31 trees, which includes:

1. `volume` measured manually by a special group of tree surgeon,
2. `height` of the tree, measured from the bottom to the top of the cut trunk,
3. `diameter` of the trunk, measured at the bottom.

They want to improve their timber harvesting process by speeding up the volume measurement. How can they do that based on the available data?
:::

```{r echo=FALSE}
load("data/SBA_Chapter_10_Trees.Rdata")
```

The data in this example is available online from [here](https://github.com/config-i1/sba/blob/master/data/SBA_Chapter_10_Trees.Rdata) and can be loaded in R the following way:

```{r eval=FALSE}
load(url("https://github.com/config-i1/sba/raw/refs/heads/master/data/SBA_Chapter_10_Trees.Rdata"))
```

To answer the question of the company "Timber Lend", we need to understand how we can capture relations between different variables numerically, so that we would be able to say what volume the company can expect based on the height and/or diameter of each trunk. Yes, we already know how to do [graphical](#dataAnalysisGraphical) and [correlations](#correlations) analysis. But this will not provide us sufficient information to answer the question in the beginning of this chapter. Still, the first thing to do is to plot the relation between the variables. We start with analysing the relation between height and volume, which is plotted in Figure \@ref(fig:TreesLine).

```{r TreesLine, fig.cap="Scatterplot matrix of the trees volume and height.", echo=FALSE}
slmTrees <- lm(volume~height, SBA_Chapter_10_Trees)

plot(SBA_Chapter_10_Trees$height, SBA_Chapter_10_Trees$volume,
     xlab="Height", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))
abline(slmTrees, col=5, lwd=2, lty=2)
# text(14,700,paste0(c("volume = ",round(coef(slmTrees)[1],2), " + ",
#                      round(coef(slmTrees)[2],2)," height + e"),
#                    collapse=""))
```

We can see that there is a relation between the height and volume (Figure \@ref(fig:TreesLine)), which is mildly linear: with the increase of the height, volume of trees tends to increase on average. In addition to that, we can spot that there is a higher variability in the volume of trees with larger height in comparison with the ones with the lower one. This effect is called "heteroscedasticity" and we will come back to it in Section \@ref(assumptions). But the important question for us now is whether we can quantify this relation between the variables, so that we could say, for example, that a tree that has a height of 75 is expected to have some specific volume?

To answer this question, we need to mathematically describe this relation. This can be done by finding coefficients of the line going through the cloud of points in Figure \@ref(fig:TreesLine). The general mathematical form of this line (called "regression line") is:
\begin{equation}
    \hat{y}_j = \beta_0 + \beta_1 x_j,
    (\#eq:SLRLineFormula)
\end{equation}
where $\hat{y}_j$ is the expected value of the response variable (expected volume in the example above), $\beta_0$ is the intercept (constant term), showing where the line intersects the y-ays, and $\beta_1$ is the coefficient for the slope parameter, which regulates how fast the expected volume increases with the increase of height. If $\beta_1$ is negative, the line would go down, showing that with the increase of one variable, the other tends to decrease. $\beta_1$ is also the tangent of the angle $\phi$ between the line drawn through the cloud of points and the x-ays. The two parameters are visualised in Figure \@ref(fig:regressionVisuals) for an example of some artificial data.

```{r regressionVisuals, fig.cap="Visualisation of regression line drawn for some artificial data.", fig.width=6, fig.height=6, echo=FALSE}
# Generate the data
set.seed(41)
x <- rnorm(1000,5,10)
error <- rnorm(1000,0,10)
b1 <- 1.5
b0 <- 5
y <- b0 + b1*x + error

plot(0, 0, col=1,
     xlim=c(-20,20), ylim=c(-20,20),
     xlab="x", ylab="y")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
draw.arc(-2.5,0,10,0,atan(b1), lwd=2, col=3)
text(8, 5, TeX("$\\phi=\\arctan (\\beta_1)$"), pos=4, col=3)
arrows(-7, b0, -0.1, b0, col=3, lty=1, lwd=2, angle=10)
text(-8, b0, TeX("$\\beta_0$"), pos=NULL, col=3)
```

Based on this regression line, we could explain every observation in sample as:
\begin{equation}
    {y}_j = \hat{y}_j + \epsilon_j = \beta_0 + \beta_1 x_j + \epsilon_j,
    (\#eq:SLRFormula)
\end{equation}
where $\epsilon_j$ is the deviation of each specific point from the line. This variable is also called the "error term" and can be shown visually as in Figure \@ref(fig:TreesModel), where each error corresponds to the size of each vertical line. In that figure, we only showed three errors for observations 17, 18 and 31, which all have large heights of 85, 86 and 87 respectively. But we could calculate such errors for all the other points in Figure \@ref(fig:TreesModel).

```{r TreesModel, fig.cap="Scatterplot diagram between height and volume, together with an error term", echo=FALSE}
pointsFunction <- function(id, pos=NULL, error="\\epsilon", text=TRUE){
    lines(rep(SBA_Chapter_10_Trees$height[id],2),c(fitted(slmTrees)[id],SBA_Chapter_10_Trees$volume[id]), lty=2)
    if(text){
        text(SBA_Chapter_10_Trees$height[id]+0.15,mean(c(fitted(slmTrees)[id],SBA_Chapter_10_Trees$volume[id])),
             TeX(paste0("$",error,"_{",id,"}$")), pos=pos)
    }
    points(SBA_Chapter_10_Trees$height[id],SBA_Chapter_10_Trees$volume[id], pch=16)
    points(SBA_Chapter_10_Trees$height[id],fitted(slmTrees)[id], pch=3)
}

plot(SBA_Chapter_10_Trees$height, SBA_Chapter_10_Trees$volume,
     xlab="Height", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))
abline(slmTrees, col=5, lwd=2, lty=2)

# Error on the last obs
pointsFunction(31, 2)
pointsFunction(18, 4)
pointsFunction(17, 2)
```

The mathematical formula \@ref(eq:SLRFormula) is called "simple regression model", and is one of the basic *statistical model* (discussed in Subsection \@ref(modelsMethods)) that captures the relation between an explanatory variable $x_j$ and the response variable $y_j$ and explains what composes the response variable. In our example, the volume is impacted by the height and some individual errors that happen due to randomness.

::: remark
The line in Figure \@ref(fig:TreesModel) captures the averaged-our relation between the height and the volume. We might find some specific points, where the increase of height would not increase volume (e.g. switch from the observation 17 to 18 at the right-hand side of the image), but this can be considered as a random fluctuation. But overall, the average tendency is described by the increasing line.
:::

Now the question is how to capture and quantify this relation correctly, so that we could help the "Timber Lend" company with its problem. One of the simplest techniques for this is called "Ordinary Least Squares".


## Ordinary Least Squares (OLS) {#OLS}
For obvious reasons, we do not have the values of parameters from the population: it would be simply impossible to measure heights, diameters and volumes of all eysting trees in the world. This means that we will never know what the true intercept and slope are. But we can get some estimates of these parameters based on the sample of data we have. There are different ways of doing that, and the most popular one is called "Ordinary Least Squares" method. This is the method that was used in the estimation of the model in Figure \@ref(fig:TreesModel). So, how does it work?

Having the sample of data, we can draw a line through the cloud of points and then change the parameters for the intercept and slope until we are satisfied with how the line looks like. This would not be a reliable approach, but what we would be doing in this case is probably just making sure that the line goes somehow in the middle of data. To make this more rigorous, we could use the following simple method:

::: remark
This is not how OLS works, but this gives an idea what it implies.
:::

1. Sort all values in ascending order;
2. Split the sample in two halves based on the middle of the explanatory variable (in our case that would be `height=76`);
3. Calculate mean height and volume in the first half of the data;
4. Calculate mean height and volume of the second half;
5. Draw the line through the points on the plane.

The resulting line is shown in Figure \@ref(fig:TreesModelSegments)

```{r TreesModelSegments, fig.cap="Scatterplot diagram between height and volume and the line drawn through two middle points of the data.", echo=FALSE}
x1 <- mean(SBA_Chapter_10_Trees$height[SBA_Chapter_10_Trees$height<76])
x2 <- mean(SBA_Chapter_10_Trees$height[SBA_Chapter_10_Trees$height>=76])
y1 <- mean(SBA_Chapter_10_Trees$volume[SBA_Chapter_10_Trees$height<76])
y2 <- mean(SBA_Chapter_10_Trees$volume[SBA_Chapter_10_Trees$height>=76])
b <- (y2-y1)/(x2-x1)
a <- y1-b*x1

plot(SBA_Chapter_10_Trees$height, SBA_Chapter_10_Trees$volume,
     xlab="Height", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))

abline(v=mean(SBA_Chapter_10_Trees$height), lty=5, col=6)
abline(a=a, b=b, col=5, lwd=2, lty=2)

points(x1, y1, col=3, pch=4, lwd=2)
points(x2, y2, col=3, pch=4, lwd=2)
lines(c(x1,x1), c(0,y1), lty=3, col=15, lwd=2)
lines(c(0,x1), c(y1,y1), lty=3, col=15, lwd=2)
lines(c(x2,x2), c(0,y2), lty=3, col=15, lwd=2)
lines(c(0,x2), c(y2,y2), lty=3, col=15, lwd=2)
```

The line in the figure represents the average change of volume with the increase of height of trunks. The red crosses show the middle points, the vertical line in the middle shows where the sample is split into two halves. We could improve this method by splitting each half into two halves again, and calculating points for them, or even further splitting each resulting half in halves etc. This method might not be reasonable for the specific sample, but if we had the population data, eventually we would be able to get a collection of points, each one of them representing the mean volume of trees given specific height.

But there is an easier way to do something similar but more practical. We could draw an arbitrary line, picking some estimates of parameters $b_0$ and $b_1$ and ending up with an approximation of the true model:

\begin{equation}
    {y}_j = b_0 + b_1 x_j + e_j,
    (\#eq:SLRFormulaEstimated)
\end{equation}
where $b_0$ is the estimate of the true intercept, $b_1$ is the estimate of the true slope and $e_j$ is the estimate of the true error term, which we usually call "residuals".

::: remark
We never know the true values of $\beta_0$ and $\beta_1$, which is why when we estimate a model, we should substitute them with $b_0$ and $b_1$. This way we show that we deal with just an approximation of the true model.
:::

After that we  can calculate errors for each of observations, as we did before, but this time, because we do not know the true line, and we are only trying to get the best possible estimates of parameters, we should denote each error as $e_j$ instead of $\epsilon_j$, which in general can be calculated as $e_j = y_j - \hat{y}_j$, where $\hat{y}_j$ is the value of the regression line (aka "fitted" value) for each specific value of explanatory variable. 

For example, for the height of tree of `r SBA_Chapter_10_Trees$height[20]` meters, the actual volume is `r SBA_Chapter_10_Trees$volume[20]`, while the fitted value would be `r round(fitted(slmTrees)[20],3)`. The resulting error (or residual of model) is `r SBA_Chapter_10_Trees$volume[20]` - `r round(fitted(slmTrees)[20],3)` = `r round(residuals(slmTrees)[20],3)`. We could collect all these errors of the model for all available trees based on their heights and this would result in a vector of positive and negative values like this:

```{r echo=FALSE}
residuals(slmTrees)
```

These residuals are obtained from the following mathematical formula, given some values of $b_0$ and $b_1$:
\begin{equation}
    e_j = y_j - {b}_0 - {b}_1 x_j.
    (\#eq:SLRFormulaEstimatedError)
\end{equation}
If we needed to estimate parameters ${b}_0$ and ${b}_1$ of the model, we would want to minimise those distances by changing the parameters of the model. This would correspond to drawing a line going through the middle of the series, in a way connecting all the possible mean points in the data. Visually this is shown in Figure \@ref(fig:TreesModelOLS), where the line somehow goes through the data, and we calculate errors from it.

```{r TreesModelOLS, fig.cap="Scatterplot diagram between height and volume and the OLS line.", echo=FALSE}
plot(SBA_Chapter_10_Trees$height, SBA_Chapter_10_Trees$volume,
     xlab="Height", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))
abline(slmTrees, col=3, lwd=2, lty=2)

for(i in 1:31){
    pointsFunction(i, 2, error="e")
}
```

The problem is that some errors are positive, while the others are negative (see the middle image in Figure \@ref(fig:TreesModelOLSResid)). If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error, as shown in the bottom image in Figure \@ref(fig:TreesModelOLSResid).

```{r TreesModelOLSResid, fig.cap="Volume, residuals and their squared values plotted agains the height of trees.", fig.width=10, fig.height=10, echo=FALSE}
par(mfcol=c(3,1), mar=c(4,4,1,1))
plot(SBA_Chapter_10_Trees$height, SBA_Chapter_10_Trees$volume,
     xlab="Height", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))
abline(slmTrees, col=3, lwd=2, lty=2)

for(i in 1:31){
    pointsFunction(i, 2, error="e", text=FALSE)
}

plot(SBA_Chapter_10_Trees$height, residuals(slmTrees), type="h",
     xlab="Height", ylab="Residuals", lty=2)
points(SBA_Chapter_10_Trees$height, residuals(slmTrees), pch=16)
abline(h=0, col=17, lty=2, lwd=2)

plot(SBA_Chapter_10_Trees$height, residuals(slmTrees)^2, type="h",
     xlab="Height", ylab="Squared residuals", lty=2)
points(SBA_Chapter_10_Trees$height, residuals(slmTrees)^2, pch=16)
abline(h=0, col=17, lty=2, lwd=2)
```

If we then sum up all the squared residuals, we will end up with something called "Sum of Squared Errors":
\begin{equation}
    \mathrm{SSE} = \sum_{j=1}^n e_j^2 .
    (\#eq:OLSCriterion)
\end{equation}
If we now minimise SSE by changing values of parameters ${b}_0$ and ${b}_1$, we will find the parameters that would guarantee that the line goes through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as there is an analytical solution to this:
\begin{equation}
    \begin{aligned}
        {b}_1 = & \frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \\
        {b}_0 = & \bar{y} - {b}_1 \bar{x}
    \end{aligned} ,
    (\#eq:OLSSLREstimates)
\end{equation}
where $\bar{x}$ is the mean of the explanatory variable $x_j$ (height in our example) and $\bar{y}$ is the mean of the response variables $y_j$ (volume).

::: proof
In order to get \@ref(eq:OLSSLREstimates), we should first insert \@ref(eq:SLRFormulaEstimatedError) in \@ref(eq:OLSCriterion) to get:
\begin{equation*}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - {b}_0 - {b}_1 x_j)^2 .
\end{equation*}
This can be expanded to:
\begin{equation*}
    \begin{aligned}
        \mathrm{SSE} = & \sum_{j=1}^n y_j^2 - 2 b_0 \sum_{j=1}^n y_j - 2 b_1 \sum_{j=1}^n y_j x_j + \\
                       & n b_0^2 + 2 b_0 b_1 \sum_{j=1}^n x_j + b_1^2 \sum_{j=1}^n x_j^2
    \end{aligned}
\end{equation*}
Given that we need to find the values of parameters $b_0$ and $b_1$ minimising SSE, we can take a derivative of SSE with respect to $b_0$ and $b_1$, equating them to zero to get the following **System of Normal Equations**:
\begin{equation*}
    \begin{aligned}
        & \frac{d \mathrm{SSE}}{d b_0} = -2 \sum_{j=1}^n y_j + 2 n b_0 + 2 b_1 \sum_{j=1}^n x_j = 0 \\
        & \frac{d \mathrm{SSE}}{d b_1} = -2 \sum_{j=1}^n y_j x_j  + 2 b_0 \sum_{j=1}^n x_j + 2 b_1 \sum_{j=1}^n x_j^2 = 0
    \end{aligned}
\end{equation*}

Solving this system of equations for $b_0$ and $b_1$ we get:
\begin{equation}
    \begin{aligned}
        & b_0 = \frac{1}{n}\sum_{j=1}^n y_j - b_1 \frac{1}{n}\sum_{j=1}^n x_j \\
        & b_1 = \frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \sum_{j=1}^n x_j^2 - \left(\sum_{j=1}^n x_j \right)^2}
    \end{aligned}
    (\#eq:OLSSLREstimatesProof)
\end{equation}
In the system of equations \@ref(eq:OLSSLREstimatesProof), we have the following elements:

1. $\bar{y}=\frac{1}{n}\sum_{j=1}^n y_j$,
2. $\bar{x}=\frac{1}{n}\sum_{j=1}^n x_j$,
3. $\mathrm{cov}(x,y) = \frac{1}{n}\sum_{j=1}^n y_j x_j - \frac{1}{n^2}\sum_{j=1}^n y_j \sum_{j=1}^n x_j$,
4. $\mathrm{V}(x) = \frac{1}{n}\sum_{j=1}^n x_j^2 - \left(\frac{1}{n} \sum_{j=1}^n x_j \right)^2$,

which after inserting in \@ref(eq:OLSSLREstimatesProof) lead to \@ref(eq:OLSSLREstimates).
:::

::: remark
If for some reason ${b}_1=0$ in \@ref(eq:OLSSLREstimates) (for example, because the covariance between $x$ and $y$ is zero, implying that they are not correlated), then the intercept ${b}_0 = \bar{y}$, meaning that the global average of the data would be the best predictor of the variable $y_j$.
:::

This method of estimation of parameters based on the minimisation of SSE, is called "Ordinary Least Squares", because by using this method we get the least possible squares of errors for the data. The word "ordinary" means that this is one of the basic estimation techniques. There are other least squares techniques, which we are not yet discussing in this book. The method is simple and does not require any specific assumptions: we just minimise the overall distance between the line and the points by changing the values of parameters.

::: example
For the problem with trees, we can use the `lm()` function from the `stats` package in R to get the OLS estimates of parameters. This is done in the following way:
```{r}
slmTrees <- lm(volume~height, SBA_Chapter_10_Trees)
slmTrees
```

The syntax of the function implies that we use volume as the response variable and height as the explanatory one. The resulting model in our notations has $b_0=$ `r round(coef(slmTrees)[1],2)` and $b_1=$ `r round(coef(slmTrees)[2],2)`, its equation can be written as: $volume_j=$ `r round(coef(slmTrees)[1],2)` $+$ `r round(coef(slmTrees)[2],2)` $height_j+e_j$.
:::

While we can make some conclusions based on the simple linear regression, we know that in real life we rarely see bivariate relations - typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we only discuss the simple linear regression for educational purpose and generally, do not recommend to use it in real life situations.


## Covariance, correlation and SLR {#SLRCovariance}
Now that we have introduced a simple linear regression, we can take a step back to better understand some statistics related to it, which we discussed in previous chapters.

### Covariance
Covariance is one of the most complicated things to explain to general audience. We will need to use a bit of mathematics that we introduced in Section \@ref(OLS), specifically the formula \@ref(eq:OLSSLREstimates), where $b_1$ is calculated as:
\begin{equation*}
    {b}_1 = \frac{\mathrm{cov}(x,y)}{\hat{\sigma}_x^2} ,
\end{equation*}
where $\hat{\sigma}_x$ is the in-sample estimate of standard deviation. Using a simple manipulation, we can express cov$(x,y)$ as:
\begin{equation*}
    \mathrm{cov}(x,y) = {b}_1 \hat{\sigma}_x \hat{\sigma}_x .
\end{equation*}
Visually, this can be represented as the areas of a rectangular shown on right pane in Figure \@ref(fig:covarianceVisual).

```{r covarianceVisual, fig.cap="Visualisation of covariance between two random variables, $x$ and $y$.", fig.width=10, fig.height=5, echo=FALSE}
set.seed(41)
x <- rnorm(1000,5,10)
error <- rnorm(1000,0,10)
b1 <- 1.5
b0 <- 0
y <- b0 + b1*x + error

par(mfcol=c(1,2), mar=c(4,4,2,1))
plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
draw.arc(0,0,8,0,atan(b1), lwd=2, col=3)
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4)

plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4)
```

In Figure \@ref(fig:covarianceVisual), the data is centred around the means of $x$ and $y$. We draw a regression line with the angle $\arctan (b_1)$ through the cloud of points (the left-hand side image). After that we draw a segment of the length $\hat{\sigma}_x$ parallel to the x-ays (righ-hand side image). The multiplication of $b_1$ by $\hat{\sigma}_x$ gives the side denoted as $b_1 \hat{\sigma}_x$ (because $b_1$ equals to tangent of the angle of the line to the x-ays). And finally, the $b_1 \hat{\sigma}_x \times \hat{\sigma}_x = \mathrm{cov}(x,y)$ is the area of the rectangular in the right-hand side image of Figure \@ref(fig:covarianceVisual). The higher the standard deviation of $x$ is, the bigger the area will be, implying that the covariance becomes larger. At the same time, for the same values of $\hat{\sigma}_x$, the higher $b_1$ is, the larger the area becomes, increasing the covariance as well. In this interpretation the covariance becomes equal to zero in one of the two cases:
\begin{equation*}
    \begin{aligned}
        & {b}_1 = 0 \\
        & \hat{\sigma}_x =0 ,
    \end{aligned}
\end{equation*}
which implies that either the angle of the regression line is zero, i.e. there is no linear relation between $x$ and $y$, or there is no variability in the variable $x$.

Similar visualisations can be done if the axes are swapped and the regression $x = a_0 + a_1 y + u$ is constructed. The logic would be similar, only changing the value of the slope parameter $b_1$ by $a_1$ and substituting $\hat{\sigma}_x$ with $\hat{\sigma}_y$. Finally, in case of negative relation between the variables, the rectangular area will be drawn below the zero line and thus could be considered as being negative (although the surface of the area itself cannot be negative). Still, the logic explained for the positive case above could be transferred on the negative case as well.


### Correlation {#SLRCovarianceCor}
Another thing to discuss is the connection between the parameter ${b}_1$ and the correlation coefficient. We have already briefly mentioned that in Section \@ref(correlationCoefficient), but here we can now spend more time on it. First, we could estimate two models given the pair of variable $x$ and $y$:

1. Model \@ref(eq:SLRFormulaEstimated) $y_j = b_0 + b_1 x_j + e_j$;
2. The inverse model $x_j = a_0 + a_1 y_j + u_j$.

We could then extract the slope parameters of the two models via \@ref(eq:OLSSLREstimates) and get the value of correlation coefficient as a geometric mean of the two:
\begin{equation}
    r_{x,y} = \mathrm{sign}(b_1) \sqrt{{b}_1 a_1} = \mathrm{sign}(\mathrm{cov}(x,y)) \sqrt{\frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \frac{\mathrm{cov}(x,y)}{\mathrm{V}(y)}} = \frac{\mathrm{cov}(x,y)}{\sqrt{V(x)V(y)}} ,
    (\#eq:correlationDerivationPearson)
\end{equation}
which is the formula \@ref(eq:measuresAssociationPearson). This is how the correlation coefficient was originally derived by Karl Pearson [@PearsonPaper]. Visually this is shown in Figure \@ref(fig:correlationVisual), where the two lines are drawn for several examples of artificial data. 
```{r correlationVisual, fig.cap="Visualisation of correlations between two random variables, $x$ and $y$ for four cases: positive, negative, perfect and zero correlation.", fig.width=8, fig.height=8, echo=FALSE}
b1 <- cov(x,y) / var(x)
a1 <- cov(x,y) / var(y)

par(mfrow=c(2,2), mar=c(4,4,4,1))

plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Positive correlation")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
draw.arc(1,0,7,0,atan(b1), lwd=2, col=3)
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4, col=3)
abline(a=0, b=1/a1, col=5, lwd=2, lty=2)
draw.arc(0,0,8,atan(1/a1),pi/2, lwd=2, col=5)
text(6, 12, TeX("$\\arctan (a_1)$"), pos=2, col=5)

y <- -y
b1 <- cov(x,y) / var(x)
a1 <- cov(x,y) / var(y)
plot(x-mean(x),y-mean(y), col=11,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Negative correlation")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
abline(a=0, b=b1, col=3, lwd=2, lty=2)
draw.arc(1,0,7,0,atan(b1), lwd=2, col=3)
text(8, -5, TeX("$\\arctan (b_1)$"), pos=4, col=3)
abline(a=0, b=1/a1, col=5, lwd=2, lty=2)
draw.arc(0,0,8,atan(1/a1)+pi,pi/2, lwd=2, col=5)
text(9, 14, TeX("$\\arctan (a_1)$"), pos=2, col=5)

plot(x-mean(x),x-mean(x), col=11,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Perfect correlation")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
abline(a=0, b=1, col=3, lwd=2, lty=2)
draw.arc(0,0,8,0,pi/4, lwd=2, col=3)
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4, col=3)
abline(a=0, b=1, col=5, lwd=2, lty=2)
draw.arc(0,0,8,pi/4,pi/2, lwd=2, col=5)
text(6, 12, TeX("$\\arctan (a_1)$"), pos=2, col=5)

plot(x-mean(x),rnorm(1000,0,10), col=11,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Zero correlation")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)
abline(a=0, b=0, col=3, lwd=2, lty=2)
abline(v=0, col=5, lwd=2, lty=2)
y <- -y
```

If the lines have positive slopes (as shown in the left top pane in Figure \@ref(fig:correlationVisual)) then the resulting coefficient of correlation will be positive. If they are both negative, the correlation will be negative as well (right top pane in Figure \@ref(fig:correlationVisual)). If the lines coincide then the product of tangents of their angles will be equal to 1 (thus we would have a perfect correlation of 1, left bottom pane in Figure \@ref(fig:correlationVisual)). Finally, in the case, when there is no linear relation between variables, the lines will coincide with the x- and y- axes respectively, producing $a_1=b_1=0$ and thus leading to the zero correlation coefficient (right bottom pane in Figure \@ref(fig:correlationVisual)).

Finally, another way to look at the correlation is to consider the visualisation of covariance from Figure \@ref(fig:covarianceVisual) and to expand it to the correlation coefficient, for which the part $\hat{\sigma}_x \times \hat{\sigma}_y$, corresponds to the denominator in the formula \@ref(eq:correlationDerivationPearson), and is shown in Figure \@ref(fig:correlationVisual2) as a red area.

```{r correlationVisual2, fig.cap="Visualisation of correlation between two random variables, $x$ and $y$.", fig.width=5, fig.height=5, echo=FALSE}
b1 <- cov(x,y) / var(x)

plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2, col=5)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(sd(y),2)),
        density=10, lwd=2, col=3, angle=-45)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=5)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=5)
text(0, sd(y)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=3)
text(sd(x)/2, 20, TeX("$\\hat{\\sigma}_x$"), pos=3, col=3)
```

The correlation in Figure \@ref(fig:correlationVisual2), corresponds to the ratio of the two areas: blue one (covariance) to the red one (the product of standard deviations). If the areas coincide, the correlation is equal to one. This would only happen if all the observations lie on the straight line, the case for which $\hat{\sigma}_y = b_1 \hat{\sigma}_x$. Mathematically, this can be seen if we take the variance of the response variable conditional on the slope parameter:
\begin{equation}
    \mathrm{V}(y | b_1) = b_1^2 \mathrm{V}(x) + V(e),
    (\#eq:varianceForCorrelation)
\end{equation}
which leads to the following equality for the conditional standard deviation of $y$:
\begin{equation}
    \hat{\sigma}_y = \sqrt{b_1^2 \hat{\sigma}_x^2 + \hat{\sigma}_e^2} ,
    (\#eq:sdForCorrelation)
\end{equation}
where $\hat{\sigma}_e$ is the standard deviation of the residuals. In this case, it becomes clear that the correlation is impacted by the variance of the error term $\hat{\sigma}_e^2$. If it is equal to zero, we get the equality: $\hat{\sigma}_y = b_1 \hat{\sigma}_x$, for which the areas in Figure \@ref(fig:correlationVisual2) will coincide and correlation becomes equal to one. The bigger the variance of residuals is, the lower the correlation coefficient becomes. Note that the value of $b_1$ does not impact the strength of correlation, it only regulates, whether the correlation is positive, negative or zero. Several correlation coefficients and respective rectangular areas are shown in Figure \@ref(fig:correlationVisual3).

```{r correlationVisual3, fig.cap="Visualisation of several correlation coefficients.", fig.width=8, fig.height=8, echo=FALSE}
par(mfrow=c(2,2), mar=c(4,4,4,1))

y <- 1.5*x + error*2
# b1 <- cov(x,y) / var(x)
plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(x,y),3)))
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2, col=5)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(sd(y),2)),
        density=10, lwd=2, col=3, angle=-45)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=5)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=5)
text(0, sd(y)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=3)
text(sd(x)/2, sd(y), TeX("$\\hat{\\sigma}_x$"), pos=3, col=3)

y <- 1.5*x + error
# b1 <- cov(x,y) / var(x)
plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(x,y),3)))
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2, col=5)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(sd(y),2)),
        density=10, lwd=2, col=3, angle=-45)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=5)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=5)
text(0, sd(y)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=3)
text(sd(x)/2, sd(y), TeX("$\\hat{\\sigma}_x$"), pos=3, col=3)

y <- 1.5*x
# b1 <- cov(x,y) / var(x)
plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(x,y),3)))
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2, col=5)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(sd(y),2)),
        density=10, lwd=2, col=3, angle=-45)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=5)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=5)
text(0, sd(y)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=3)
text(sd(x)/2, sd(y), TeX("$\\hat{\\sigma}_x$"), pos=3, col=3)

y <- error
b1 <- cov(x,y) / var(x)
plot(0, 0, col=1,
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(x,y),3)))
abline(h=0, col=15, lwd=2, lty=2)
abline(v=0, col=15, lwd=2, lty=2)

points(x,y, col=11)
abline(a=b0, b=b1, col=3, lwd=2, lty=2)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(b1*sd(x),2)),
        density=10, lwd=2, col=5)
polygon(c(0,sd(x),sd(x),0), c(0,0,rep(sd(y),2)),
        density=10, lwd=2, col=3, angle=-45)
text(10, b1*sd(x)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=5)
text(sd(x)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=5)
text(0, sd(y)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=3)
text(sd(x)/2, sd(y), TeX("$\\hat{\\sigma}_x$"), pos=3, col=3)
```


## Residuals of model estimated via OLS {#OLSResiduals}
OLS applied to any model guarantees two important properties about its residuals:

1. $\mathrm{E}(e_j) = \frac{1}{n} \sum_{j=1}^n e_j = 0$,
2. $\mathrm{E}(e_j x_{i,j}) = \frac{1}{n} \sum_{j=1}^n e_j x_{i,j} = 0$ for any $i$.

The first property means that the in-sample mean of residuals is always equal to zero, while the second implies that the estimation is done in a way that the in-sample correlation between the residuals and any explanatory variable in the model is equal to zero. These two conditions happen automatically, and there is no point in testing them or trying to see whether they have been violated or not. On the other hand, if a model was estimated using some other method, these properties might not hold anymore, and it might be the case that the mean of the in-sample residuals and/or the correlation between the error and explanatory variables would not be equal to zero. We will come back to these properties in Chapter \@ref(assumptions), when we discuss the standard assumptions of statistical models.

It is possible to prove mathematically that these two conditions hold. Here is a proof of the first one:

::: proof
Consider the sum of residuals of a simple linear regression model estimated using OLS:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n (y_j - b_0 - b_1 x_j) = \sum_{j=1}^n y_j - n b_0 - b_1  \sum_{j=1}^n x_j
    (\#eq:sumOfResiduals01)
\end{equation}
Inserting the formula for $b_0$ from \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals01) we get:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n y_j - n \frac{1}{n}\sum_{j=1}^n y_j + n b_1 \frac{1}{n}\sum_{j=1}^n x_j - b_1  \sum_{j=1}^n x_j
    (\#eq:sumOfResiduals02)
\end{equation}
which after some cancelations leads to:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n y_j - \sum_{j=1}^n y_j + b_1 \sum_{j=1}^n x_j - b_1 \sum_{j=1}^n x_j = 0
    (\#eq:sumOfResiduals03)
\end{equation}
Given that the sum of errors is equal to zero, their mean will be equal to zero as well.
:::

The second property is less straightforward, but it can be proven as well, using similar logic:

::: proof
For the same simple linear regression, estimated using OLS, consider:
\begin{equation}
    \sum_{j=1}^n e_j x_j = \sum_{j=1}^n (y_j x_j -b_0 x_j -b_1 x_j^2) = \sum_{j=1}^n y_j x_j -b_0 \sum_{j=1}^n x_j -b_1 \sum_{j=1}^n x_j^2 .
    (\#eq:sumOfResiduals04)
\end{equation}
Inserting the formula for $b_0$ from \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals04) leads to:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n e_j x_j = & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + b_1 \frac{1}{n}\sum_{j=1}^n x_j \sum_{j=1}^n x_j - b_1 \sum_{j=1}^n x_j^2 = \\
                               & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + b_1 \left( \frac{1}{n} \left(\sum_{j=1}^n x_j \right)^2 - \sum_{j=1}^n x_j^2 \right) .
    \end{aligned}
    (\#eq:sumOfResiduals05)
\end{equation}
Now we insert the formula for $b_1$ from \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals05) to get:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n e_j x_j = & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + \\
         & \frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \sum_{j=1}^n x_j^2 - \left(\sum_{j=1}^n x_j \right)^2} \left( \frac{1}{n} \left(\sum_{j=1}^n x_j \right)^2 - \sum_{j=1}^n x_j^2 \right) .
    \end{aligned}
    (\#eq:sumOfResiduals06)
\end{equation}
The ratio in the right-hand side of \@ref(eq:sumOfResiduals06) can be regrouped and rewritten as:
\begin{equation}
    \begin{aligned}
        & -\frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \left( \sum_{j=1}^n x_j^2 - \frac{1}{n}\left(\sum_{j=1}^n x_j \right)^2 \right)} \left( \sum_{j=1}^n x_j^2 - \frac{1}{n} \left(\sum_{j=1}^n x_j\right)^2 \right) = \\
        & - \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j,
    \end{aligned}
    (\#eq:sumOfResiduals07)
\end{equation}
which after inserting it back in \@ref(eq:sumOfResiduals06) leads to:
\begin{equation}
    \sum_{j=1}^n e_j x_j = \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j - \sum_{j=1}^n y_j x_j + \frac{1}{n} \sum_{j=1}^n y_j \sum_{j=1}^n x_j = 0
    (\#eq:sumOfResiduals08)
\end{equation}
Given that the sum \@ref(eq:sumOfResiduals08) is equal to zero, the mean of $e_j x_j$ will be equal to zero as well.
:::

In order to see that the second property implies that the correlation between the residuals and regressors is equal to zero, we need to take a step back and consider the covariance between $e_j$ and $x_j$ (because it is used in correlation coefficient as discussed in Section \@ref(correlationCoefficient)):
\begin{equation}
    \mathrm{cov}(e_j,x_j) = \sum_{j=1}^n (e_j - \bar{e})(x_j - \bar{x})
    (\#eq:sumOfResiduals09)
\end{equation}
The first thing to notice in \@ref(eq:sumOfResiduals09) is that $\bar{e}=0$ because of the property (1) discussed in the beginning of this section. This simplifies the formula and leads to:
\begin{equation}
    \mathrm{cov}(e_j,x_j) = \sum_{j=1}^n e_j (x_j - \bar{x}) = \sum_{j=1}^n e_j x_j - \bar{x} \sum_{j=1}^n e_j = \sum_{j=1}^n e_j x_j ,
    (\#eq:sumOfResiduals10)
\end{equation}
because the second sum in \@ref(eq:sumOfResiduals10) is equal to zero due to the same property (1).

These two basic properties on one hand are useful for further derivations and on the other one show what to expect from the residuals of a regression model estimated via the OLS. The latter means, for example, that there is no point in testing whether the two properties hold, they will be satisfied automatically in case of OLS.


## Quality of a fit {#linearRegressionSimpleQualityOfFit}
The term "Quality of a fit" is used often in statistics to outline approaches that provide some information about how the applied models fit the data. We find it misleading, because the word "quality" is not appropriate here. The measures discussed in this section only show how well the actual values are approximated by the model, but their values do not tell us whether a model is good or bad. Still, to get a general impression about the performance of the estimated model, we can calculate several in-sample measures, which could provide us insights about the approximating properties of the model.

### Sums of squares
The fundamental measure that lies in the basis of many other ones is SSE, which is the value of the OLS criterion \@ref(eq:OLSCriterion). It cannot be interpreted on its own and cannot be used for model comparison, but it shows the overall variability of the data around the regression line. In a more general case, it is written as:
\begin{equation}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - \hat{y}_j)^2 .
    (\#eq:SSE)
\end{equation}
This sum of squares is related to another two, the first being the Sum of Squares Total:
\begin{equation}
    \mathrm{SST}=\sum_{j=1}^n (y_j - \bar{y})^2,
    (\#eq:SST)
\end{equation}
where $\bar{y}$ is the in-sample mean. If we divide the value \@ref(eq:SST) by $n-1$, we get the in-sample variance (introduced in Section \@ref(dataAnalysisNumerical)):
\begin{equation*}
    \mathrm{V}(y)=\frac{\mathrm{SST}}{n-1}=\frac{1}{n-1} \sum_{j=1}^n (y_j - \bar{y})^2 .
\end{equation*}
The last sum of squares is Sum of Squares of Regression:
\begin{equation}
    \mathrm{SSR} = \sum_{j=1}^n (\bar{y} - \hat{y}_j)^2 ,
    (\#eq:SSR)
\end{equation}
which shows the variability of the regression line. It is possible to show that in *the linear regression* (**this is important**, this property might be violated in other models), the three sums are related to each other via the following equation:
\begin{equation}
    \mathrm{SST} = \mathrm{SSE} + \mathrm{SSR} .
    (\#eq:SSTSum)
\end{equation}

::: proof
This involves manipulations, some of which are not straightforward. First, we assume that SST equals to SSE + SSR, and see whether we reach the original formula of SST:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \mathrm{SSR} + \mathrm{SSE} = \sum_{j=1}^n (\hat{y}_j - \bar{y})^2 + \sum_{j=1}^n (y_j - \hat{y}_j)^2 \\
        &= \sum_{j=1}^n \left( \hat{y}_j^2 - 2 \hat{y}_j \bar{y} + \bar{y}^2 \right) + \sum_{j=1}^n \left( y_j^2 - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left( \hat{y}_j^2 - 2 \hat{y}_j \bar{y} + \bar{y}^2 + y_j^2 - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left(\bar{y}^2 -2 \bar{y} y_j + y_j^2 + 2 \bar{y} y_j + \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} y_j + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 y_j \hat{y}_j \right)
    \end{aligned} .
    (\#eq:SSTProof01)
\end{equation}
We can then substitute $y_j=\hat{y}_j+e_j$ in the right hand side of \@ref(eq:SSTProof01) to get:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} (\hat{y}_j+e_j) + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 (\hat{y}_j+e_j) \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} \hat{y}_j + 2 \bar{y} e_j + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 \hat{y}_j\hat{y}_j -2 e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} e_j + 2 \hat{y}_j^2 - 2 \hat{y}_j^2 -2 e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} e_j - 2 e_j \hat{y}_j \right) 
    \end{aligned} .
    (\#eq:SSTProof02)
\end{equation}
Now if we split the sum into three elements, we will get:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \sum_{j=1}^n (\bar{y} - y_j)^2 + 2 \sum_{j=1}^n \left(\bar{y} e_j\right) - 2 \sum_{j=1}^n \left(e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n (\bar{y} - y_j)^2 + 2 \bar{y} \sum_{j=1}^n e_j - 2 \sum_{j=1}^n \left(e_j \hat{y}_j \right)
    \end{aligned} .
    (\#eq:SSTProof03)
\end{equation}
The second sum in \@ref(eq:SSTProof03) is equal to zero, because OLS guarantees that the in-sample mean of error term is equal to zero (see proof in Subsection \@ref(OLSResiduals)). The third one can be expanded to:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n \left(e_j \hat{y}_j \right) = \sum_{j=1}^n \left(e_j b_0 + b_1 e_j x_j \right)
    \end{aligned} .
    (\#eq:SSTProof04)
\end{equation}
We see the sum of errors in the first sum of \@ref(eq:SSTProof04), so the first elements is equal to zero again. The second term is equal to zero as well due to OLS estimation (this was also proven in Subsection \@ref(OLSResiduals)). This means that:
\begin{equation}
    \mathrm{SST} =  \sum_{j=1}^n (\bar{y} - y_j)^2 ,
    (\#eq:SSTProof05)
\end{equation}
which is the formula of SST \@ref(eq:SST).
:::

The relation between SSE, SSR and SST can be visualised and is shown in Figure \@ref(fig:sumsSquaredRelation). If we take any observation in that figure, we will see how the deviations from the regression line and from the mean are related.

```{r sumsSquaredRelation, fig.cap="Relation between different sums of squares.", echo=FALSE}
knitr::include_graphics("images/09-SLR-SSE-white.png")
```


### Coefficient of determination, R$^2$
While the sums of squares do not have a nice interpretation and are hard to use for diagnostics, they can be used in calculating the measure called "Coefficient of Determination". It is calculated in the following way:
\begin{equation}
    \mathrm{R}^2 = 1 - \frac{\mathrm{SSE}}{\mathrm{SST}} = \frac{\mathrm{SSR}}{\mathrm{SST}} .
    (\#eq:Determination)
\end{equation}
Given the fundamental property \@ref(eq:SSTSum), we can see that R$^2$ will always lie between zero and one. To better understand its meaning, imagine the following two extreme situations:

1. The model fits the data in the same way as the mean line (black line coincides with the grey line in Figure \@ref(fig:sumsSquaredRelation)). In this case SSE would be equal to SST and SSR would be equal to zero (because $\hat{y}_j=\bar{y}$) and as a result the R$^2$ would be equal to zero.
2. The model fits the data perfectly, without any errors (all points lie on the black line in Figure \@ref(fig:sumsSquaredRelation)). In this situation SSE would be equal to zero and SSR would be equal to SST, because the regression would go through all points (i.e. $\hat{y}_j=y_j$). This would make R$^2$ equal to one.

So, the zero value of the coefficient of determination means that the model does not explain the data at all and one means that it overfits the data. The value itself is usually interpreted as a percentage of variability in data explained by the model.

::: remark
The properties above provide us an important point about the coefficient of determination: *it should not be equal to one, and it is alarming if it is very close to one*. This is because in this situation we are implying that there is no randomness in the data, which contradicts our definition of the statistical model (see Section \@ref(modelsMethods)). The adequate statistical model should always have some randomness in it. The situation of $\mathrm{R}^2=1$ implies mathematically:
\begin{equation*}
    y_j = b_0 + b_1 x_j ,
\end{equation*}
which means that all $e_j=0$, being unrealistic and only possible if there is a functional relation between $y$ and $x$ (no need for statistical inference then). So, in practice we should not maximise R$^2$ and should be careful with models that have very high values of it. At the same time, too low values of R$^2$ are also alarming, as they tell us that the model becomes:
\begin{equation*}
    y_j = b_0 + e_j,
\end{equation*}
meaning that it is not different from the simple mean of the data, because in that case $b_0=\bar{y}$. So, coefficient of determination in general is not a very good measure for assessing performance of a model. It can be used for further inferences, and for a basic indication of whether the model overfits (R$^2$ close to 1) or underfits (R$^2$ close to 0) the data. But no serious conclusions should be solely based on it.
:::

Here how this measure can be calculated in R based on the model that we estimated in Section \@ref(OLS):
```{r}
n <- nobs(slmTrees)
R2 <- 1 - sum(resid(slmTrees)^2) / (var(actuals(slmTrees))*(n-1))
R2
```
Note that in this formula we used the relation between SST and V$(y)$, multiplying the value by $n-1$ to get rid of the denominator. The resulting value tells us that the model has explained `r round(1 - sum(resid(slmTrees)^2) / (var(actuals(slmTrees))*(n-1)),3)*100`% deviations in the data.

Finally, based on coefficient of determination, we can also calculate the coefficient of multiple correlation, which we have already discussed in Section \@ref(correlationsMixed):
\begin{equation}
    R = \sqrt{R^2} = \sqrt{\frac{\mathrm{SSR}}{\mathrm{SST}}} .
    (\#eq:multipleCorrelation)
\end{equation}
It shows the closeness of relation between the response variable $y_j$ and the explanatory variables to the linear one. The coefficient has a positive sign, no matter what the relation between the variables is. In case of the simple linear regression, it is equal to the correlation coefficient (from Section \@ref(correlationCoefficient)) with the sign equal to the sign of the coefficient of the slop $b_1$ (this was discussed in Subsection \@ref(SLRCovarianceCor)):
\begin{equation}
    r_{x,y} = \mathrm{sign} (b_1) R .
    (\#eq:correlationInRegression)
\end{equation}

Here is a demonstration of the formula above in R:
```{r}
sign(coef(slmTrees)[2]) * sqrt(R2)
cor(SBA_Chapter_10_Trees$height,SBA_Chapter_10_Trees$volume)
```



## What about the "Timber Lend" company?
Coming back to the example that motivated this chapter, there is a way we can improve the model for the company and help them in making better decisions. After all, the determination coefficient of the model of volume from height was just `r round(R2, 4)`.

First, we check how the relation between the diameter and the volume looks (Figure \@ref(fig:TreesLineDiameter))

```{r TreesLineDiameter, fig.cap="Scatterplot matrix of the trees volume and dimeter.", echo=FALSE}
slmTreesDiam <- lm(volume~diameter, SBA_Chapter_10_Trees)

plot(SBA_Chapter_10_Trees$diameter, SBA_Chapter_10_Trees$volume,
     xlab="Diameter", ylab="Volume",
     ylim=c(0,max(SBA_Chapter_10_Trees$volume)))
abline(slmTreesDiam, col=5, lwd=2, lty=2)
```

It might not be apparent for an inexperienced analyst, but the relation between the diameter and volume is non-linear, because the points for the lowest and highest diameters lie consistently above the straight line. Even if this relation is not apparent visually, there is a fundamental reason for its existence: trunks of trees have a shape close to cylinder. Some of you might remember from geometry that the volume of a cylinder is calculated as:

\begin{equation}
    V = h \pi r^2 ,
    (\#eq:cylinderFormula)
\end{equation}
where $V$ is the volume, $h$ is the height, $r$ is the radius and $\pi$ is the constant number. The diameters that we have in the data equal to $d=2 \times r$. Having this fundamental formula, implies that the relation between the diameter and volume should be indeed non-linear. Based on that, we can create a new variable, which could be called `cylinder`:

```{r}
SBA_Chapter_10_Trees$cylinder <- (SBA_Chapter_10_Trees$height *
                                      SBA_Chapter_10_Trees$diameter^2)
```

Furthermore, because trunks of trees are not exactly cylinders, our model can be represented as:
\begin{equation*}
    volume = \beta_0 + \beta_1 cylinder + \epsilon_j.
\end{equation*}

This model in R gives is much more reasonable than either the model of volume from height or volume from diameter. We can fit it and see how much variability it explains:

```{r}
# Fit the model
slmTreesCyl <- lm(volume~cylinder, SBA_Chapter_10_Trees)
# Get the number of observations
n <- nobs(slmTreesCyl)
# Calculate R^2
1 - sum(resid(slmTreesCyl)^2) / (var(actuals(slmTreesCyl))*(n-1))
```

::: remark
While in general, we should not compare models based on R$^2$, in this specific case we can because the number of explanatory variables in the two models is exactly the same.
:::

As we see the model of volume from cylinders explains the data much better than the previous one.

::: question
In the formula of the cylinder \@ref(eq:cylinderFormula), we do not have any error term. Why do we expect it to be in the model that we fit? Shouldn't R$^2$ be equal to one in our example?
:::

::: answer
We do not expect to have zero error in this case, because the trunks of trees are not perfect cylinders: the diameter at the top of the tree is smaller than the diameter at the bottom, and trees have some curvature, deviating in shape from the perfect cylinder. Due to these factors, the formula \@ref(eq:cylinderFormula) does not perfectly describe the volume of the tree, but instead is a good approximation of it.
:::


As for the coefficients of the model, based on our sample, they were:

```{r}
coef(slmTreesCyl)
```

Using some specific measurements of a tree, we can get its expected volume. For example, if a tree has the height of 80 and diameter of 10.7, our new variable cylinder would be $80 \times 11.1^2 = 9856.8$. Inserting this value in the equation, we get the expected volume:

\begin{equation*}
    volume = -2.44 + 0.02 \times 9856.8 \approx 207.00 ,
\end{equation*}
which is not too far from the real volume of 226.5 that we have in the data (observation number 9). While this is not a perfect estimate of volume, it allows improving the operational process for the "Timber Lend" company, hopefully reducing some costs.
