# Simple Linear Regression {#simpleLinearRegression}
When we want to analyse some relations between variables, we can do [graphical](#dataAnalysisGraphical) and [correlations](#correlations) analysis. But this will not provide us sufficient information about what happens with the response variable with the change of explanatory variable. So it makes sense to consider the possible relations between variables, and the basis for this is Simple Linear Regression, which can be represented in the form:
\begin{equation}
    y_j = \beta_0 + \beta_1 x_j + \epsilon_j ,
    (\#eq:SLRFormula)
\end{equation}
where $\beta_0$ is the intercept (constant term), $\beta_1$ is the coefficient for the slope parameter and $\epsilon_j$ is the error term. The regression model is a basic [statistical model](#modelsMethods) that captures the relation between an explanatory variable $x_j$ and the response variable $y_j$. The parameters of the models are typically denoted as $\beta_0$ and $\beta_1$ in econometrics literature, but we use $\beta_0$ and $\beta_1$ because we will use $\beta$ for other purposes later in this textbook.

In order to better understand what simple linear regression implies, consider the scatterplot (we discussed it earlier in Section \@ref(dataAnalysisGraphical)) shown in Figure \@ref(fig:scatterWeightMPG2).

```{r scatterWeightMPG2, fig.cap="Scatterplot diagram between weight and mileage."}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
text(4,35,paste0(c("mpg=",round(coef(slmMPGWt),2),"wt+et"),collapse=""))
```

The line drawn on the plot is the regression line, parameters of which were estimated based on the available data. In this case the intercept ${b}_0$=`r round(coef(slmMPGWt)[1],2)`, meaning that this is where the red line crosses the y-axis, while the parameter of slope ${b}_1$=`r round(coef(slmMPGWt)[2],2)` shows how fast the values change (how steep the line is). Note that we use $b_0$ and $b_1$ for the parameters that were estimated on a sample of data. If we had all the data in the universe (population) and estimated the correct model on it, we would use $\beta_0$ and $\beta_1$. In simple linear regression, the red line will always go through the cloud of points, showing the averaged out tendencies. The one that we observe above can be summarise as "with the increase of weight, on average the mileage of cars goes down". Note that we might find some specific points, where the increase of weight would not decrease mileage (e.g. the two furthest points to the left show this), but this can be considered as a random fluctuation, so overall, the average tendency is as described above.


## Ordinary Least Squares (OLS) {#OLS}
For obvious reasons, we do not have the values of parameters from the population. This means that we will never know what the true intercept and slope are. Luckily, we can estimate them based on the sample of data. There are different ways of doing that, and the most popular one is called "Ordinary Least Squares" method. This is the method that was used in the estimation of the model in Figure \@ref(fig:scatterWeightMPG2). So, how does it work?

```{r scatterWeightMPG3, fig.cap="Scatterplot diagram between weight and mileage.", echo=FALSE}
slmMPGWt <- lm(mpg~wt,mtcarsData)
plot(mtcarsData$wt, mtcarsData$mpg,
     xlab="Weight", ylab="Mileage",
     xlim=c(0,6), ylim=c(0,40))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmMPGWt,col="red")
lines(rep(mtcarsData$wt[20],2),c(fitted(slmMPGWt)[20],mtcarsData$mpg[20]), lty=2)
text(mtcarsData$wt[20]+0.15,mean(c(fitted(slmMPGWt)[20],mtcarsData$mpg[20])),TeX("$e_j$"))
points(mtcarsData$wt[20],mtcarsData$mpg[20], pch=16)
points(mtcarsData$wt[20],fitted(slmMPGWt)[20], pch=3)
```

When we estimate the simple linear regression model, the model \@ref(eq:SLRFormula) transforms into:
\begin{equation}
    y_j = {b}_0 + {b}_1 x_j + e_j ,
    (\#eq:SLRFormulaEstimated)
\end{equation}
where $b_0$ is the estimate of $\beta_0$, $b_1$ is the estimate of $\beta_1$ and $e_j$ is the estimate of $\epsilon_j$. This is because we do not know the true values of parameters and thus they are substituted by their estimates. This also applies to the error term for which in general $e_j \neq \epsilon_j$ because of the sample estimation. Now consider the same situation with weight vs mileage in Figure \@ref(fig:scatterWeightMPG3) but with some arbitrary line with unknown parameters. Each point on the plot will typically lie above or below the line, and we would be able to calculate the distances from those points to the line. They would correspond to $e_j = y_j - \hat{y}_j$, where $\hat{y}_j$ is the value of the regression line (aka "fitted" value) for each specific value of explanatory variable. For example, for the weight of car of `r mtcarsData$wt[20]` tones, the actual mileage is `r mtcarsData$mpg[20]`, while the fitted value is `r round(fitted(slmMPGWt)[20],3)`. The resulting error (or residual of model) is `r round(residuals(slmMPGWt)[20],3)`. We could collect all these errors of the model for all available cars based on their weights and this would result in a vector of positive and negative values like this:

```{r echo=FALSE}
residuals(slmMPGWt)
```

This corresponds to the formula:
\begin{equation}
    e_j = y_j - {b}_0 - {b}_1 x_j.
    (\#eq:SLRFormulaEstimatedError)
\end{equation}
If we needed to estimate parameters ${b}_0$ and ${b}_1$ of the model, we would need to minimise those distances by changing the parameters of the model. The problem is that some errors are positive, while the others are negative. If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error and calculating Sum of Squared Errors for the whole sample $T$:
\begin{equation}
    \mathrm{SSE} = \sum_{j=1}^n e_j^2 .
    (\#eq:OLSCriterion)
\end{equation}
If we now minimise SSE by changing values of parameters ${b}_0$ and ${b}_1$, we will find those parameters that would guarantee that the line goes somehow through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as there is an analytical solution to this:
\begin{equation}
    \begin{aligned}
        {b}_1 = & \frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \\
        {b}_0 = & \bar{y} - {b}_1 \bar{x}
    \end{aligned} ,
    (\#eq:OLSSLREstimates)
\end{equation}
where $\bar{x}$ is the mean of the explanatory variable $x_j$ and $\bar{y}$ is the mean of the response variables $y_j$.

::: proof
In order to get \@ref(eq:OLSSLREstimates), we should first insert \@ref(eq:SLRFormulaEstimatedError) in \@ref(eq:OLSCriterion) to get:
\begin{equation*}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - {b}_0 - {b}_1 x_j)^2 .
\end{equation*}
This can be expanded to:
\begin{equation*}
    \begin{aligned}
        \mathrm{SSE} = & \sum_{j=1}^n y_j^2 - 2 b_0 \sum_{j=1}^n y_j - 2 b_1 \sum_{j=1}^n y_j x_j + \\
                       & n b_0^2 + 2 b_0 b_1 \sum_{j=1}^n x_j + b_1^2 \sum_{j=1}^n x_j^2
    \end{aligned}
\end{equation*}
Given that we need to find the values of parameters $b_0$ and $b_1$ minimising SSE, we can take a derivative of SSE with respect to $b_0$ and $b_1$, equating them to zero to get the following system of equations:
\begin{equation*}
    \begin{aligned}
        & \frac{d \mathrm{SSE}}{d b_0} = -2 \sum_{j=1}^n y_j + 2 n b_0 + 2 b_1 \sum_{j=1}^n x_j = 0 \\
        & \frac{d \mathrm{SSE}}{d b_1} = -2 \sum_{j=1}^n y_j x_j  + 2 b_0 \sum_{j=1}^n x_j + 2 b_1 \sum_{j=1}^n x_j^2 = 0
    \end{aligned}
\end{equation*}

Solving this system of equations for $b_0$ and $b_1$ we get:
\begin{equation}
    \begin{aligned}
        & b_0 = \frac{1}{n}\sum_{j=1}^n y_j - b_1 \frac{1}{n}\sum_{j=1}^n x_j \\
        & b_1 = \frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \sum_{j=1}^n x_j^2 - \left(\sum_{j=1}^n x_j \right)^2}
    \end{aligned}
    (\#eq:OLSSLREstimatesProof)
\end{equation}
In the system of equations \@ref(eq:OLSSLREstimatesProof), we have the following elements:

1. $\bar{y}=\frac{1}{n}\sum_{j=1}^n y_j$,
2. $\bar{x}=\frac{1}{n}\sum_{j=1}^n x_j$,
3. $\mathrm{cov}(x,y) = \frac{1}{n}\sum_{j=1}^n y_j x_j - \frac{1}{n^2}\sum_{j=1}^n y_j \sum_{j=1}^n x_j$,
4. $\mathrm{V}(x) = \frac{1}{n}\sum_{j=1}^n x_j^2 - \left(\frac{1}{n} \sum_{j=1}^n x_j \right)^2$,

which after inserting in \@ref(eq:OLSSLREstimatesProof) lead to \@ref(eq:OLSSLREstimates).
:::

Note that if for some reason ${b}_1=0$ in \@ref(eq:OLSSLREstimates) (for example, because the covariance between $x$ and $y$ is zero, implying that they are not correlated), then the intercept ${b}_0 = \bar{y}$, meaning that the global average of the data would be the best predictor of the variable $y_j$. This method of estimation of parameters based on the minimisation of SSE, is called "Ordinary Least Squares". It is simple and does not require any specific assumptions: we just minimise the overall distance by changing the values of parameters.

While we can do some inference based on simple linear regression, we know that the bivariate relations are not often met in practice: typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we will discuss inference in the following sections.


### Gauss-Markov theorem {#GaussMarkov}
OLS is a very popular estimation method for linear regression for a variety of reasons. First, it is relatively simple (much simpler than other approaches) and conceptually easy to understand. Second, the estimates of OLS parameters can be found analytically (as in formula \@ref(eq:OLSSLREstimates)). Furthermore, there is a mathematical proof that the estimates of parameters are efficient (Subsection \@ref(estimatesPropertiesEfficiency)), consistent (Subsection \@ref(estimatesPropertiesConsistency)) and unbiased (Subsection \@ref(estimatesPropertiesBias)). It is called "Gauss-Markov theorem". It states that:

::: theorem
If regression model is correctly specified then OLS will produce Best Linear Unbiased Estimates (BLUE) of its parameters.
:::

The term "correctly specified" implies that all main statistical assumptions about the model are satisfied (such as no omitted important variables, no autocorrelation and heteroscedasticity in the residuals, see details in Chapter \@ref(assumptions)). The "BLUE" part means that OLS guarantees the most efficient and the least biased estimates of parameters amongst all possible estimators of a linear model. For example, if we used a criterion of minimisation of Mean Absolute Error (MAE), then the estimates of parameters would be less efficient than in the case of OLS (this is because OLS gives "mean" estimates, while the minimum of MAE corresponds to the median, see Subsection \@ref(estimatesPropertiesEfficiency)).

Practically speaking, the theorem implies that when you use OLS, the estimates of parameters will have good statistical properties (given that the model is correctly specified), in some cases better than the estimates obtained by other estimators.


## Covariance, correlation and SLR {#SLRCovariance}
Now that we have introduced a simple linear regression, we can take a step back to better understand some statistics related to it.

### Covariance
One of the most complicated things to explain is covariance. Coming back to the formula \@ref(eq:OLSSLREstimates), where $b_1$ is calculated as:
\begin{equation*}
    {b}_1 = \frac{\mathrm{cov}(x,y)}{\hat{\sigma}_x^2} ,
\end{equation*}
where $\hat{\sigma}_x$ is the in-sample estimate of standard deviation, we can express cov$(x,y)$ as:
\begin{equation*}
    \mathrm{cov}(x,y) = {b}_1 \hat{\sigma}_x \hat{\sigma}_x .
\end{equation*}
Visually, this can be represented as the areas of a rectangular shown on right pane in Figure \@ref(fig:covarianceVisual).

```{r covarianceVisual, fig.cap="Visualisation of covariance between two random variables, $x$ and $y$.", fig.width=10, fig.height=5, echo=FALSE}
xr <- rnorm(1000,100,10)
error <- rnorm(1000,0,10)
xi <- 1.5*xr + error
b1 <- cov(xr,xi) / var(xr)
b0 <- mean(xi) - b1 * mean(xr)

par(mfcol=c(1,2), mar=c(4,4,2,1))
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
abline(h=0, col="darkgrey", lwd=2, lty=1)
abline(v=0, col="darkgrey", lwd=2, lty=1)
draw.arc(0,0,8,0,atan(b1), lwd=2, col="darkred")
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4)

plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2)
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4)
```

In Figure \@ref(fig:covarianceVisual), the data is centred around the means of $x$ and $y$. We draw a regression line with the angle $\arctan (b_1)$ through the cloud of points and then draw a segment of the length $\hat{\sigma}_x$ parallel to the x-axis. The multiplication of $b_1$ by $\hat{\sigma}_x$ gives the side denoted as $b_1 \hat{\sigma}_x$ (because $b_1$ equals to tangent of the angle of the line to the x-axis). And finally, the $b_1 \hat{\sigma}_x \times \hat{\sigma}_x = \mathrm{cov}(x,y)$ is the area of the rectangular in Figure \@ref(fig:covarianceVisual). The higher the standard deviation of $x$ is, the bigger the area will be, implying that the covariance becomes larger. At the same time, for the same values of $\hat{\sigma}_x$, the higher $b_1$ is, the larger the area becomes, increasing the covariance. In this interpretation the covariance becomes equal to zero either in one of the two cases:
\begin{equation*}
    \begin{aligned}
        & {b}_1 = 0 \\
        & \hat{\sigma}_x =0 ,
    \end{aligned}
\end{equation*}
which implies that either there is no variability in the variable $x$ or that the angle of the regression line is zero, i.e. there is no linear relation between $x$ and $y$.

Similar visualisations can be done if the axes are swapped and the regression $x = a_0 + a_1 y + u$ is constructed. The logic would be similar, only changing the value of the slope parameter $b_1$ by $a_1$ and substituting $\hat{\sigma}_x$ with $\hat{\sigma}_y$. Finally, in case of negative relation between the variables, the rectangular area will be drawn below the zero line and thus could be considered as being negative (although the surface of the area itself cannot be negative). Still, the logic explained for the positive case above could be transferred on the negative case as well.


### Correlation
Another thing to note is the connection between the parameter ${b}_1$ and the correlation coefficient. We have already briefly discussed this in Section \@ref(correlationCoefficient), we could estimate two models given the pair of variable $x$ and $y$:

1. Model \@ref(eq:SLRFormulaEstimated) $y_j = a_0 + a_1 x_j + e_j$;
2. The inverse model $x_j = a_0 + a_1 y_j + u_j$.

We could then extract the slope parameters of the two models via \@ref(eq:OLSSLREstimates) and get the value of correlation coefficient as a geometric mean of the two:
\begin{equation}
    r_{x,y} = \mathrm{sign}(b_1) \sqrt{{b}_1 a_1} = \mathrm{sign}(\mathrm{cov}(x,y)) \sqrt{\frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} \frac{\mathrm{cov}(x,y)}{\mathrm{V}(y)}} = \frac{\mathrm{cov}(x,y)}{\sqrt{V(x)V(y)}} ,
    (\#eq:correlationDerivationPearson)
\end{equation}
which is the formula \@ref(eq:measuresAssociationPearson). This is how the correlation coefficient was originally derived. Visually this is shown in Figure \@ref(fig:correlationVisual), where the two lines are drawn for the simple linear regressions mentioned above. 
```{r correlationVisual, fig.cap="Visualisation of correlation between two random variables, $x$ and $y$.", fig.width=8, fig.height=8, echo=FALSE}
# xr <- rnorm(1000,100,10)
# xi <- 1.5*xr + rnorm(1000,0,10)
# b1 <- cov(xr,xi) / var(xr)
a1 <- cov(xr,xi) / var(xi)

par(mfrow=c(2,2), mar=c(4,4,4,1))

plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Positive correlation")
abline(h=0, col="darkgrey", lwd=2, lty=1)
abline(v=0, col="darkgrey", lwd=2, lty=1)
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
draw.arc(0,0,8,0,atan(b1), lwd=2, col="darkred")
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4, col="darkred")
abline(a=0, b=1/a1, col="darkblue", lwd=2, lty=2)
draw.arc(0,0,8,atan(1/a1),pi/2, lwd=2, col="darkblue")
text(6, 12, TeX("$\\arctan (a_1)$"), pos=2, col="darkblue")

xi <- -xi
b1 <- cov(xr,xi) / var(xr)
a1 <- cov(xr,xi) / var(xi)
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Negative correlation")
abline(h=0, col="darkgrey", lwd=2, lty=1)
abline(v=0, col="darkgrey", lwd=2, lty=1)
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
draw.arc(0,0,8,0,atan(b1), lwd=2, col="darkred")
text(8, -5, TeX("$\\arctan (b_1)$"), pos=4, col="darkred")
abline(a=0, b=1/a1, col="darkblue", lwd=2, lty=2)
draw.arc(0,0,8,atan(1/a1)+pi,pi/2, lwd=2, col="darkblue")
text(9, 14, TeX("$\\arctan (a_1)$"), pos=2, col="darkblue")

plot(xr-mean(xr),xr-mean(xr), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Perfect correlation")
abline(h=0, col="darkgrey", lwd=2, lty=1)
abline(v=0, col="darkgrey", lwd=2, lty=1)
abline(a=0, b=1, col="darkred", lwd=2, lty=2)
draw.arc(0,0,8,0,pi/4, lwd=2, col="darkred")
text(8, 5, TeX("$\\arctan (b_1)$"), pos=4, col="darkred")
abline(a=0, b=1, col="darkblue", lwd=2, lty=2)
draw.arc(0,0,8,pi/4,pi/2, lwd=2, col="darkblue")
text(6, 12, TeX("$\\arctan (a_1)$"), pos=2, col="darkblue")

plot(xr-mean(xr),rnorm(1000,0,10), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main="Zero correlation")
abline(h=0, col="darkgrey", lwd=2, lty=1)
abline(v=0, col="darkgrey", lwd=2, lty=1)
abline(a=0, b=0, col="darkred", lwd=2, lty=2)
abline(v=0, col="darkblue", lwd=2, lty=2)
xi <- -xi
```

If the lines have positive slope (as shown in the left top pane in Figure \@ref(fig:correlationVisual)) then the resulting coefficient of correlation will be positive. If they are both negative, the correlation will be negative as well (right top pane in Figure \@ref(fig:correlationVisual)). If the lines coincide then the product of tangents of their angles will be equal to 1 (thus we would have a perfect correlation of 1, left bottom pane in Figure \@ref(fig:correlationVisual)). Finally, in the case, when there is no linear relation between variables, the lines will coincide with the x- and y- axes respectively, producing $a_1=b_1=0$ and thus leading to the zero correlation coefficient (right bottom pane in Figure \@ref(fig:correlationVisual)).

Finally, another way to look at the correlation is to consider the visualisation of covariance from Figure \@ref(fig:covarianceVisual) and to expand it to the correlation coefficient, for which the area $\hat{\sigma}_x \times \hat{\sigma}_y$, corresponding to the denominator of the formula \@ref(eq:correlationDerivationPearson) is shown in Figure \@ref(fig:correlationVisual2) as a red area.

```{r correlationVisual2, fig.cap="Visualisation of correlation between two random variables, $x$ and $y$.", fig.width=5, fig.height=5, echo=FALSE}
b1 <- cov(xr,xi) / var(xr)

plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y")
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2, col=rgb(0.2,0.2,0.8,1))
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(sd(xi),2)),
        density=10, lwd=2, col=rgb(0.8,0.2,0.2,1), angle=-45)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=rgb(0.2,0.2,0.8,1))
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=rgb(0.2,0.2,0.8,1))
text(0, sd(xi)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=rgb(0.8,0.2,0.2,1))
text(sd(xr)/2, 20, TeX("$\\hat{\\sigma}_x$"), pos=3, col=rgb(0.8,0.2,0.2,1))
```

The correlation in Figure \@ref(fig:correlationVisual2), corresponds to the ratio of the two areas blue one (covariance) to the red one (the product of standard deviations). If the areas coincide, the correlation is equal to one. This would only happen if all the observations lie on the straight line, the case for which $\hat{\sigma}_y = b_1 \hat{\sigma}_x$. Mathematically, this can be seen if we take the variance of the response variable conditional on the slope parameter:
\begin{equation}
    \mathrm{V}(y | b_1) = b_1^2 \mathrm{V}(x) + V(e),
    (\#eq:varianceForCorrelation)
\end{equation}
which leads to the following equality for the standard deviation of $y$:
\begin{equation}
    \hat{\sigma}_y = \sqrt{b_1^2 \hat{\sigma}_x^2 + \hat{\sigma}_e^2} ,
    (\#eq:sdForCorrelation)
\end{equation}
where $\hat{\sigma}_e$ is the standard deviation of the residuals. In this case, it becomes clear that the correlation is impacted by the variance of the error term $\hat{\sigma}_e^2$. If it is equal to zero, we get the equality: $\hat{\sigma}_y = b_1 \hat{\sigma}_x$, for which the areas in Figure \@ref(fig:correlationVisual2) will coincide and correlation becomes equal to one. The bigger the variance of residuals is, the lower the correlation coefficient becomes. Note that the value of $b_1$ does not impact the strength of correlation, it only regulates, whether the correlation is positive, negative or zero. Several correlation coefficients and respective rectangular areas are shown in Figure \@ref(fig:correlationVisual3)

```{r correlationVisual3, fig.cap="Visualisation of several correlation coefficients.", fig.width=10, fig.height=10, echo=FALSE}
par(mfrow=c(2,2), mar=c(4,4,4,1))

xi <- 1.5*xr + error*2
b1 <- cov(xr,xi) / var(xr)
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(xr,xi),3)))
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2, col=rgb(0.2,0.2,0.8,1))
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(sd(xi),2)),
        density=10, lwd=2, col=rgb(0.8,0.2,0.2,1), angle=-45)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=rgb(0.2,0.2,0.8,1))
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=rgb(0.2,0.2,0.8,1))
text(0, sd(xi)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=rgb(0.8,0.2,0.2,1))
text(sd(xr)/2, sd(xi), TeX("$\\hat{\\sigma}_x$"), pos=3, col=rgb(0.8,0.2,0.2,1))

xi <- 1.5*xr + error
b1 <- cov(xr,xi) / var(xr)
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(xr,xi),3)))
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2, col=rgb(0.2,0.2,0.8,1))
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(sd(xi),2)),
        density=10, lwd=2, col=rgb(0.8,0.2,0.2,1), angle=-45)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=rgb(0.2,0.2,0.8,1))
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=rgb(0.2,0.2,0.8,1))
text(0, sd(xi)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=rgb(0.8,0.2,0.2,1))
text(sd(xr)/2, sd(xi), TeX("$\\hat{\\sigma}_x$"), pos=3, col=rgb(0.8,0.2,0.2,1))

xi <- 1.5*xr
b1 <- cov(xr,xi) / var(xr)
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(xr,xi),3)))
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2, col=rgb(0.2,0.2,0.8,1))
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(sd(xi),2)),
        density=10, lwd=2, col=rgb(0.8,0.2,0.2,1), angle=-45)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=rgb(0.2,0.2,0.8,1))
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=rgb(0.2,0.2,0.8,1))
text(0, sd(xi)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=rgb(0.8,0.2,0.2,1))
text(sd(xr)/2, sd(xi), TeX("$\\hat{\\sigma}_x$"), pos=3, col=rgb(0.8,0.2,0.2,1))

xi <- error
b1 <- cov(xr,xi) / var(xr)
plot(xr-mean(xr),xi-mean(xi), col="lightblue",
     xlim=c(-30,30), ylim=c(-30,30),
     xlab="x", ylab="y", main=paste0("Correlation of ", round(cor(xr,xi),3)))
abline(a=0, b=b1, col="darkred", lwd=2, lty=2)
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(b1*sd(xr),2)),
        density=10, lwd=2, col=rgb(0.2,0.2,0.8,1))
polygon(c(0,sd(xr),sd(xr),0), c(0,0,rep(sd(xi),2)),
        density=10, lwd=2, col=rgb(0.8,0.2,0.2,1), angle=-45)
text(10, b1*sd(xr)/2, TeX("$b_1 \\hat{\\sigma}_x$"), pos=4, col=rgb(0.2,0.2,0.8,1))
text(sd(xr)/2, 0, TeX("$\\hat{\\sigma}_x$"), pos=1, col=rgb(0.2,0.2,0.8,1))
text(0, sd(xi)/2, TeX("$\\hat{\\sigma}_y$"), pos=2, col=rgb(0.8,0.2,0.2,1))
text(sd(xr)/2, sd(xi), TeX("$\\hat{\\sigma}_x$"), pos=3, col=rgb(0.8,0.2,0.2,1))
```


## Residuals of model estimated via OLS {#OLSResiduals}
OLS applied to any model guarantees two important properties about its residuals:

1. $\mathrm{E}(e_j) = \frac{1}{n} \sum_{j=1}^n e_j = 0$,
2. $\mathrm{E}(e_j x_{i,j}) = \frac{1}{n} \sum_{j=1}^n e_j x_{i,j} = 0$ for any $i$.

The first property means that the in-sample mean of residuals is always equal to zero, while the second implies that the estimation is done in a way that the in-sample correlation between the residuals and any explanatory variable in the model is equal to zero. We start by proving the first property.

::: proof
Consider the sum of residuals of a simple linear regression model estimated using OLS:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n (y_j - b_0 - b_1 x_j) = \sum_{j=1}^n y_j - n b_0 - b_1  \sum_{j=1}^n x_j
    (\#eq:sumOfResiduals01)
\end{equation}
Inserting the formula for $b_0$ from \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals01) we get:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n y_j - n \frac{1}{n}\sum_{j=1}^n y_j + n b_1 \frac{1}{n}\sum_{j=1}^n x_j - b_1  \sum_{j=1}^n x_j
    (\#eq:sumOfResiduals02)
\end{equation}
which after some cancelations leads to:
\begin{equation}
    \sum_{j=1}^n e_j = \sum_{j=1}^n y_j - \sum_{j=1}^n y_j + b_1 \sum_{j=1}^n x_j - b_1 \sum_{j=1}^n x_j = 0
    (\#eq:sumOfResiduals03)
\end{equation}
Given that the sum of errors is equal to zero, its mean will be equal to zero as well.
:::

The second property is less straightforward, but it can be proven as well, using similar logic:

::: proof
For the same simple linear regression, estimated using OLS, consider:
\begin{equation}
    \sum_{j=1}^n e_j x_j = \sum_{j=1}^n (y_j x_j -b_0 x_j -b_1 x_j^2) = \sum_{j=1}^n y_j x_j -b_0 \sum_{j=1}^n x_j -b_1 \sum_{j=1}^n x_j^2 .
    (\#eq:sumOfResiduals04)
\end{equation}
Inserting \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals04) leads to:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n e_j x_j = & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + b_1 \frac{1}{n}\sum_{j=1}^n x_j \sum_{j=1}^n x_j - b_1 \sum_{j=1}^n x_j^2 = \\
                               & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + b_1 \left( \frac{1}{n} \left(\sum_{j=1}^n x_j \right)^2 - \sum_{j=1}^n x_j^2 \right) .
    \end{aligned}
    (\#eq:sumOfResiduals05)
\end{equation}
Now we insert the formula for $b_1$ from \@ref(eq:OLSSLREstimatesProof) in \@ref(eq:sumOfResiduals05) to get:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n e_j x_j = & \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j + \\
         & \frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \sum_{j=1}^n x_j^2 - \left(\sum_{j=1}^n x_j \right)^2} \left( \frac{1}{n} \left(\sum_{j=1}^n x_j \right)^2 - \sum_{j=1}^n x_j^2 \right) .
    \end{aligned}
    (\#eq:sumOfResiduals06)
\end{equation}
The ratio in the right-hand side of \@ref(eq:sumOfResiduals06) can be regrouped and rewritten as:
\begin{equation}
        -\frac{n \sum_{j=1}^n y_j x_j - \sum_{j=1}^n y_j \sum_{j=1}^n x_j}{n \left( \sum_{j=1}^n x_j^2 - \frac{1}{n}\left(\sum_{j=1}^n x_j \right)^2 \right)} \left( \sum_{j=1}^n x_j^2 - \frac{1}{n} \left(\sum_{j=1}^n x_j\right)^2 \right) ,
    (\#eq:sumOfResiduals07)
\end{equation}
which after inserting in \@ref(eq:sumOfResiduals06) and the cancelation of elements leads to:
\begin{equation}
    \sum_{j=1}^n e_j x_j = \sum_{j=1}^n y_j x_j - \frac{1}{n}\sum_{j=1}^n y_j \sum_{j=1}^n x_j - \sum_{j=1}^n y_j x_j + \frac{1}{n} \sum_{j=1}^n y_j \sum_{j=1}^n x_j = 0
    (\#eq:sumOfResiduals08)
\end{equation}
Given that the sum \@ref(eq:sumOfResiduals08) is equal to zero, the mean of $e_j x_j$ will be equal to zero as well.
:::

In order to see that the second property implies that the correlation between the residuals and regressors is equal to zero, we need to take a step back and consider the covariance between $e_j$ and $x_j$ (because it is used in correlation coefficient as discussed in Section \@ref(correlationCoefficient)):
\begin{equation}
    \mathrm{cov}(e_j,x_j) = \sum_{j=1}^n (e_j - \bar{e})(x_j - \bar{x})
    (\#eq:sumOfResiduals09)
\end{equation}
The first thing to notice in \@ref(eq:sumOfResiduals09) is that $\bar{e}=0$ because of the property (1) discussed in the beginning of this subsection. This simplifies the formula and leads to:
\begin{equation}
    \mathrm{cov}(e_j,x_j) = \sum_{j=1}^n e_j (x_j - \bar{x}) = \sum_{j=1}^n e_j x_j - \bar{x} \sum_{j=1}^n e_j = \sum_{j=1}^n e_j x_j ,
    (\#eq:sumOfResiduals10)
\end{equation}
because the second sum in \@ref(eq:sumOfResiduals10) is equal to zero due to the same property (1).

These two basic properties on one hand are useful for further derivations and on the other one show what to expect from the residuals of a regression model estimated via the OLS. The latter means, for example, that there is no point in testing whether the two properties hold, they will be satisfied automatically in case of OLS.


## Quality of a fit {#linearRegressionSimpleQualityOfFit}
In order to get a general impression about the performance of the estimated model, we can calculate several in-sample measures, which could provide us insights about the fit of the model.

The fundamental measure that lies in the basis of many other ones is SSE, which is the value of the OLS criterion \@ref(eq:OLSCriterion). It cannot be interpreted on its own and cannot be used for model comparison, but it shows the overall variability of the data around the regression line. In a more general case, it is written as:
\begin{equation}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - \hat{y}_j)^2 .
    (\#eq:SSE)
\end{equation}
This sum of squares is related to another two, the first being the Sum of Squares Total:
\begin{equation}
    \mathrm{SST}=\sum_{j=1}^n (y_j - \bar{y})^2,
    (\#eq:SST)
\end{equation}
where $\bar{y}$ is the in-sample mean. If we divide the value \@ref(eq:SST) by $n-1$, then we will get the in-sample variance (introduced in Section \@ref(dataAnalysisNumerical)):
\begin{equation*}
    \mathrm{V}(y)=\frac{\mathrm{SST}}{n-1}=\frac{1}{n-1} \sum_{j=1}^n (y_j - \bar{y})^2 .
\end{equation*}
The last sum of squares is Sum of Squared of Regression:
\begin{equation}
    \mathrm{SSR} = \sum_{j=1}^n (\bar{y} - \hat{y}_j)^2 ,
    (\#eq:SSR)
\end{equation}
which shows the variability of the regression line. It is possible to show that in *the linear regression* (this is important! This property might be violated in other models), the three sums are related to each other via the following equation:
\begin{equation}
    \mathrm{SST} = \mathrm{SSE} + \mathrm{SSR} .
    (\#eq:SSTSum)
\end{equation}

::: proof
This involves manipulations, some of which are not straightforward:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \mathrm{SSR} + \mathrm{SSE} = \sum_{j=1}^n (\hat{y}_j - \bar{y})^2 + \sum_{j=1}^n (y_j - \hat{y}_j)^2 \\
        &= \sum_{j=1}^n \left( \hat{y}_j^2 - 2 \hat{y}_j \bar{y} + \bar{y}^2 \right) + \sum_{j=1}^n \left( y_j^2 - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left( \hat{y}_j^2 - 2 \hat{y}_j \bar{y} + \bar{y}^2 + y_j^2 - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left(\bar{y}^2 -2 \bar{y} y_j + y_j^2 + 2 \bar{y} y_j + \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 y_j \hat{y}_j + \hat{y}_j^2 \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} y_j + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 y_j \hat{y}_j \right)
    \end{aligned} .
    (\#eq:SSTProof01)
\end{equation}
We can then substitute $y_j=\hat{y}_j+e_j$ in the right hand side of \@ref(eq:SSTProof01) to get:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} (\hat{y}_j+e_j) + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 (\hat{y}_j+e_j) \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} \hat{y}_j + 2 \bar{y} e_j + 2 \hat{y}_j^2 - 2 \hat{y}_j \bar{y} - 2 \hat{y}_j\hat{y}_j -2 e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} e_j + 2 \hat{y}_j^2 - 2 \hat{y}_j^2 -2 e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n \left((\bar{y} - y_j)^2 + 2 \bar{y} e_j - 2 e_j \hat{y}_j \right) 
    \end{aligned} .
    (\#eq:SSTProof02)
\end{equation}
Now if we split the sum into three elements, we will get:
\begin{equation}
    \begin{aligned}
        \mathrm{SST} &= \sum_{j=1}^n (\bar{y} - y_j)^2 + 2 \sum_{j=1}^n \left(\bar{y} e_j\right) - 2 \sum_{j=1}^n \left(e_j \hat{y}_j \right) \\
        &= \sum_{j=1}^n (\bar{y} - y_j)^2 + 2 \bar{y} \sum_{j=1}^n e_j - 2 \sum_{j=1}^n \left(e_j \hat{y}_j \right)
    \end{aligned} .
    (\#eq:SSTProof03)
\end{equation}
The second sum in \@ref(eq:SSTProof03) is equal to zero, because OLS guarantees that the in-sample mean of error term is equal to zero (see proof in Subsection \@ref(OLSResiduals)). The second one can be expanded to:
\begin{equation}
    \begin{aligned}
        \sum_{j=1}^n \left(e_j \hat{y}_j \right) = \sum_{j=1}^n \left(e_j b_0 + b_1 e_j x_j \right)
    \end{aligned} .
    (\#eq:SSTProof04)
\end{equation}
We see the sum of errors in the first sum of \@ref(eq:SSTProof04), so the first elements is equal to zero again. The second term is equal to zero as well due to OLS estimation (this was also proven in Subsection \@ref(OLSResiduals)). This means that:
\begin{equation}
    \mathrm{SST} =  \sum_{j=1}^n (\bar{y} - y_j)^2 ,
    (\#eq:SSTProof05)
\end{equation}
which is the formula of SST \@ref(eq:SST).
:::

The relation between SSE, SSR and SST is shown in Figure \@ref(fig:sumsSquaredRelation). If we take any observation in that Figure, we will see how the deviations from the regression line and from the mean are related.

```{r sumsSquaredRelation, fig.cap="Relation between different sums of squares.", echo=FALSE}
knitr::include_graphics("images/09-SLR-SSE-white.png")
```

Building upon that, there is a measure called "Coefficient of Determination", which is calculated based on the sums of squares discussed above:
\begin{equation}
    \mathrm{R}^2 = 1 - \frac{\mathrm{SSE}}{\mathrm{SST}} = \frac{\mathrm{SSR}}{\mathrm{SST}} .
    (\#eq:Determination)
\end{equation}
Given the meaning of the sums of squares, we can imagine the following situations to interpret the values of $\mathrm{R}^2$:

1. The model fits the data in the same way as the mean line (grey line in Figure \@ref(fig:sumsSquaredRelation)). In this case SSE would be equal to SST and SSR would be equal to zero (because $\hat{y}_j=\bar{y}$) and as a result the R$^2$ would be equal to zero.
2. The model fits the data perfectly, without any errors (all points lie on the black line in Figure \@ref(fig:sumsSquaredRelation)). In this situation SSE would be equal to zero and SSR would be equal to SST, because the regression would go through all points (i.e. $\hat{y}_j=y_j$). This would make R$^2$ equal to one.

In the linear regression model due to \@ref(eq:SSTSum), the coefficient of determination would always lie between zero and one, where zero means that the model does not explain the data at all and one means that it overfits the data. The value itself is usually interpreted as a percentage of variability in data explained by the model. This definition above provides us an important point about the coefficient of determination: it should not be equal to one, and it is alarming if it is very close to one - because in this situation we are implying that there is no randomness in the data, but this contradicts our definition of the statistical model (see Section \@ref(modelsMethods)). The adequate statistical model should always have some randomness in it. The situation of $\mathrm{R}^2=1$ corresponds to:
\begin{equation*}
    y_j = b_0 + b_1 x_j ,
\end{equation*}
implying that all $e_j=0$, which is unrealistic and is only possible if there is a functional relation between $y$ and $x$ (no need for statistical inference then). So, in practice we should not maximise R$^2$ and should be careful with models that have very high values of it. At the same time, too low values of R$^2$ are also alarming, as they tell us that the model becomes:
\begin{equation*}
    y_j = b_0 + e_j,
\end{equation*}
meaning that it is not different from the global mean. So, coefficient of determination in general is not a very good measure for assessing performance of a model. It can be used for further inferences, and for a basic indication of whether the model overfits (R$^2$ close to 1) or underfits (R$^2$ close to 0) the data. But no serious conclusions should be made based on it.

Here how this measure can be calculated in R based on the model earlier:
```{r}
n <- nobs(slmMPGWt)
R2 <- 1 - sum(resid(slmMPGWt)^2) /
    (var(actuals(slmMPGWt))*(n-1))
R2
```
Note that in this formula we used the relation between SST and V$(y)$, multiplying the value by $n-1$ to get rid of the denominator. The resulting value tells us that the model has explained `r round(1 - sum(resid(slmMPGWt)^2) / (var(actuals(slmMPGWt))*(n-1)),3)*100`% deviations in the data.

Finally, based on coefficient of determination, we can also calculate the coefficient of multiple correlation, which we have already discussed in Section \@ref(correlationsMixed):
\begin{equation}
    R = \sqrt{R^2} = \sqrt{\frac{\mathrm{SSR}}{\mathrm{SST}}} .
    (\#eq:multipleCorrelation)
\end{equation}
It shows the closeness of relation between the response variable $y_j$ and the explanatory variables to the linear one. The coefficient has a positive sign, no matter what the relation between the variables is. In case of the simple linear regression, it is equal to the correlation coefficient (from Section \@ref(correlationCoefficient)) with the sign equal to the sign of the coefficient of the slop $b_1$:
\begin{equation}
    r_{x,y} = \mathrm{sign} (b_1) R .
    (\#eq:correlationInRegression)
\end{equation}

Here is a demonstration of the formula above in R:
```{r}
sign(coef(slmMPGWt)[2]) * sqrt(R2)
cor(mtcars$mpg,mtcars$wt)
```

<!-- Proof -->
