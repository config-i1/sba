# Variables transformations {#variablesTransformations}
So far we have discussed linear regression models, where the response variable linearly depends on a set of explanatory variables. These models work well in many contexts, especially when the response variable is measured in high volumes (e.g. sales in thousands of units). However, in reality the relations between variables can be non-linear. In this chapter we consider an example of application to see how transformations can be motivated by a real life example and then discuss different types of transformations and what they imply for analytics and forecasting


## Example of application
Consider, for example, the stopping distance vs speed of the car, the case we have discussed in the previous sections. This sort of relation in reality is non-linear. We know from physics that the distance travelled by car is proportional to the mass of car, the squared speed and inversely proportional to the breaking force:
\begin{equation}
    distance \propto \frac{mass}{2 breaking} \times speed^2.
    (\#eq:speedDistanceFormula)
\end{equation}
If we use the linear function instead, then we might fail in capturing the relation correctly. Here is how the linear regression looks like, when applied to the data (Figure \@ref(fig:speedDistanceExtrapolation)).

```{r speedDistanceExtrapolation, fig.cap="Speed vs stopping distance and a linear model", echo=FALSE}
plot(cars, xlab="Speed", ylab="Stopping distance", xlim=c(0,30), ylim=c(-20,120))
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(slmSpeedDistance, col="red")
```

The model on the plot in Figure \@ref(fig:speedDistanceExtrapolation) is misleading, because it predicts that the stopping distance of a car, travelling with speed less than 4mph will be negative. Furthermore, the modelunderestimates the real stopping distance for cars with higher speed. If a decision is made based on this model, then it will be inevitably wrong and might potentially lead to serious repercussions in terms of road safety. Given the relation \@ref(eq:speedDistanceFormula), we should consider a non-linear model. In this specific case, we should consider the model of the type:
\begin{equation}
    distance = \beta_0 speed^{\beta_1} \times (1+\epsilon).
    (\#eq:speedDistanceModel)
\end{equation}
The multiplication of speed by the error term is necessary, because the effect of randomness will have an increasing variability with the increase of speed: if the speed is low, then the random factors (such as road conditions, breaks condition etc) will not have a strong effect on distance, while in case of the high speed these random factors might lead either to the serious decrease or increase of distance (a car on a slippery road, stopping from 50mph will have much longer distance than the same car on a dry road). Note that I have left the parameter $\beta_1$ in \ref(eq:speedDistanceModel) and did not set it equal to two. This is done for the case we want to estimate the parameter based on the data. The problem with the model \@ref(eq:speedDistanceModel) is that it is difficult to estimate due to the non-linearity. In order to resolve this problem, we can linearise it by taking logarithms of both sides, which will lead to:
\begin{equation}
    \log (distance) = \log \beta_0 + \beta_1 \log (speed) + \log(1+\epsilon).
    (\#eq:speedDistanceModelLogs)
\end{equation}
If we substituted every element with $\log$ in \@ref(eq:speedDistanceModelLogs) by other names (e.g. $\log(\beta_0)=\beta^\prime_0$ and $\log(speed)=x$), it would be easier to see that this is a linear model, which can be estimated via [OLS](#OLS). This type of model is called "log-log", reflecting that it has logarithms on both sides. Even the data will be much better behaved if we use logarithms in this situation (see Figure \@ref(fig:speedDistanceLogs)).

```{r speedDistanceLogs, fig.cap="Speed vs stopping distance in logarithms", echo=FALSE}
slmSpeedDistanceModel01 <- alm(log(dist)~log(speed), cars, loss="MSE")
plot(log(cars), xlab="log(Speed)", ylab="log(Stopping distance)")
abline(slmSpeedDistanceModel01,col="red")
```

What we want to see on Figure \@ref(fig:speedDistanceLogs) is the linear relation between the variables with points having fixed variance. However, in our case we can notice that the variance of the stopping distances does not seem to be stable: the variability around 2.0 is higher than the variability around 3.0. This might cause issues in the model due to violation of assumptions (see Section \@ref(assumptions)). For now, we acknowledge the issue but do not aim to fix it. And here how the model \@ref(eq:speedDistanceModelLogs) can be estimated using R:
```{r eval=FALSE}
slmSpeedDistanceModel01 <- alm(log(dist)~log(speed), cars, loss="MSE")
```

The values of parameters of this model will have a different meaning than the parameters of the linear model. Consider the example with the model above:
```{r}
summary(slmSpeedDistanceModel01)
```

The value of parameter for the variable `log(speed)` now does not represent the marginal effect of speed on distance, but rather shows the elasticity, i.e. if the speed of a car increases by 1%, the travel distance will increase on average by `r round(coef(slmSpeedDistanceModel01)[2],2)`%.

In order to analyse the fit of the model on the original data, we would need to produce fitted values and exponentiate them. Note that in this case they would correspond to geometric rather than arithmetic means:

```{r speedDistanceLogExp, fig.cap="Speed vs stopping distance and the log-log model fit."}
plot(cars, xlab="Speed", ylab="Stopping distance")
lines(cars$speed,exp(fitted(slmSpeedDistanceModel01)),col="red")
```

The resulting model in Figure \@ref(fig:speedDistanceLogExp) is the power function, which exhibits the increase in speed of change of one parameter with a linear change of another one. Note that technically speaking, the log-log model only makes sense, when the data is strictly positive. If it also contains zeroes (the speed is zero, thus the stopping distance is zero), then some other transformations might be in order. For example, we could square the speed in the model and try constructing the linear model, aligning it better with the physical model \@ref(eq:speedDistanceFormula):
\begin{equation}
    distance = \beta_0 + \beta_1 speed^2 + \epsilon .
    (\#eq:speedDistanceModelSquare)
\end{equation}
The issue of this model would be that the error term is additive and thus the model would assume that the variability of the error does not change with the speed, which is not realistic.

```{r speedDistanceSquare, fig.cap="Speed squared vs stopping distance.", echo=FALSE}
slmSpeedDistanceModel02 <- alm(dist~I(speed^2), cars, loss="MSE")
plot(cars$speed^2, cars$dist, xlab="Speed^2", ylab="Stopping distance")
lines(c(-10:30)^2,coef(slmSpeedDistanceModel02)[1]+coef(slmSpeedDistanceModel02)[2]*c(-10:30)^2,col="red")
```
 
Figure \@ref(fig:speedDistanceSquare) demonstrates the scatterplot for squared speed vs stopping distances. While we see that the relation between variables is closer to linear, the problem with variance is not resolved. If we want to estimate this model, we can use the following command in R:
```{r eval=FALSE}
slmSpeedDistanceModel02 <- alm(dist~I(speed^2), cars, loss="MSE")
```

Note that we use `I()` in the formula to tell R to square the variable - it will not do the necessary transformation otherwise. Also note that in our specific case we did not include the non-transformed speed variable, because we know that the lowest distance should be, when speed is zero. But this might not be the case in other cases, so in general instead of the formula used above we should use: `y~x+I(x^2)`. Furthermore, if we know for sure that the intercept is not needed (i.e. we know that the distance will be zero, when speed is zero), then we can remove it and estimate the model:
```{r}
slmSpeedDistanceModel03 <- alm(dist~I(speed^2)-1, cars, loss="MSE")
```
`alm()` function will complain about the exclusion of the intercept, but it should estimate the model nonetheless. The fit of the model to the data would be similar in its shape to the one from the log-log model (see Figure \@ref(fig:speedDistanceSquare02)).

```{r speedDistanceSquare02, fig.cap="Speed squared vs stopping distance with models with speed^2.", echo=FALSE}
plot(cars$speed, cars$dist, xlab="Speed", ylab="Stopping distance",
     xlim=c(0,25))
lines(c(-10:30), coef(slmSpeedDistanceModel02)[1] +
        coef(slmSpeedDistanceModel02)[2]*c(-10:30)^2, col="red")
lines(c(-10:30), coef(slmSpeedDistanceModel03)[1]*c(-10:30)^2, col="blue")
legend("topleft",legend=c("Model 2","Model 3"), lwd=1, col=c("red","blue"))
```

The plot in Figure \@ref(fig:speedDistanceSquare02) demonstrates how the two models fit the data. The Model 2, as we see goes through the origin, which makes sense from the physical point of view. However, because of that it might fit the data worse than the Model 1 does. Still, it it better to have a more meaningful model than the one that potentially overfits the data.

Another way to introduce the squares in the model is to take square root of distance. This would potentially align better with the physical model of stopping distance \@ref(eq:speedDistanceFormula):
\begin{equation}
    \sqrt{distance} = \beta_0 + \beta_1 speed + \epsilon ,
    (\#eq:speedDistanceModelSqrt)
\end{equation}
which will be equivalent to:
\begin{equation}
    distance = (\beta_0 + \beta_1 speed + \epsilon)^2 .
    (\#eq:speedDistanceModelSqrt2)
\end{equation}
The good news is, the error term in this model will change with the change of speed due to the interaction effect, cause by the square of the sum in \@ref(eq:speedDistanceModelSqrt2). And, similar to the previous models, the parameter $\beta_0$ might not be needed. Graphically, this transformation is present on Figure \@ref(fig:speedDistanceSqrt).

```{r speedDistanceSqrt, fig.cap="Speed vs square root of stopping distance.", echo=FALSE}
slmSpeedDistanceModel04 <- alm(sqrt(dist)~speed, cars, loss="MSE")
plot(cars$speed, sqrt(cars$dist), xlab="Speed", ylab="sqrt(Stopping distance)")
abline(slmSpeedDistanceModel04, col="red")
```

As the plot in Figure \@ref(fig:speedDistanceSqrt) demonstrates, the relation has become linear and the variance seems to be constant, no matter what the speed is. This means that the proposed model might be more appropriate to the data than the previous ones. This is how we can estimate this model:
```{r eval=FALSE}
slmSpeedDistanceModel04 <- alm(sqrt(dist)~speed, cars, loss="MSE")
```
Similar to the Model 2 with squares, we will also consider the model without intercept on the grounds that if we capture the relation correctly, the zero speed should result in zero distance.
```{r}
slmSpeedDistanceModel05 <- alm(sqrt(dist)~speed-1, cars, loss="MSE")
```
Finally, we can see how both models will fit the original data (squaring the fitted values to get to the original scale):
```{r speedDistanceSqrt02, fig.cap="Speed squared vs stopping distance with Square Root models.", echo=FALSE}
plot(cars$speed, cars$dist, xlab="Speed", ylab="Stopping distance",
     xlim=c(0,25))
lines(c(-10:30), predict(slmSpeedDistanceModel04,
                         newdata=data.frame(speed=c(-10:30)))$mean^2, col="red")
lines(c(-10:30), predict(slmSpeedDistanceModel05,
                         newdata=data.frame(speed=c(-10:30)))$mean^2, col="blue")
legend("topleft",legend=c("Model 4","Model 5"), lwd=1, col=c("red","blue"))
```

Subjectively, I would say that Model 5 is the most appropriate from all the models under consideration: it corresponds to the physical model on one hand, and has constant variance on the other one. Here is its summary:
```{r}
summary(slmSpeedDistanceModel05)
```
Its parameter contains some average information about the mass of cars and their breaking forces (this is based on the formula \@ref(eq:speedDistanceFormula)). The interpretation of the parameter in this model, however, is challenging. In order to get to some crude interpretation, we need to revert to maths. Model 5 can be written as:
\begin{equation}
    distance = (\beta_1 speed + \epsilon)^2 .
    (\#eq:speedDistanceModelSqrt3)
\end{equation}
If we take the first derivative of distance with respect to speed, we will get:
\begin{equation}
    \frac{\mathrm{d}distance}{\mathrm{d}speed} = 2 (\beta_1 speed + \epsilon) ,
    (\#eq:speedDistanceModelSqrt4)
\end{equation}
which is now closer to what we need. We can say that if speed increases by 1mph, the distance will change on average by $2 \beta_1 speed$. But this does not explain what the meaning of $\beta_1$ in the model is. So we take the second derivative with respect to speed:
\begin{equation}
    \frac{\mathrm{d}^2 distance}{\mathrm{d}^2 speed} = 2 \beta_1 .
    (\#eq:speedDistanceModelSqrt5)
\end{equation}
The meaning of the second derivative is that it shows the change of change of distance with a change of change of speed by 1. This implies a tricky interpretation of the parameter. Based on the summary above, the only thing we can conclude is that when the change of speed increases by 1mph, the change of distance will increase by `r round(coef(slmSpeedDistanceModel05),4)*2` feet. An alternative interpretation would be based on the model \@ref(eq:speedDistanceModelSqrt): with the increase of speed of car by 1mph, the square roo tof stopping distance would increase by `r round(coef(slmSpeedDistanceModel05),4)` square root feet. Neither of these two interpretations are very helpful, but this is the best we have for the parameter $\beta_1$ in the Model 5.
<!-- The more useful interpretation would be related to equation \@ref(eq:speedDistanceModelSqrt4), if we know a specific speed. For example, if a car drives with the speed of 30mph, then an increase in speed by 1mph would on average lead to `r round(coef(slmSpeedDistanceModel05)*30,4)*2` 23 feet increase in stopping distance. -->

## Types of variables transformations
Having considered this case study, we can summarise the possible types of transformations of variables in regression models and what they would mean. Here, we only discuss monotonic transformations, i.e. those that guarantee that if $x$ was increasing before transformations, it would be increasing after transformations as well.

### Linear model
\begin{equation}
    y = \beta_0 + \beta_1 x + \epsilon .
    (\#eq:transformLinear)
\end{equation}

As discussed earlier, in this model, $\beta_1$ can be interpreted as a marginal effect of x on y. The typical interpretation is that with the increase of $x$ by 1 unit, $y$ will change on average by $\beta_1$ units. In case of dummy variables, their interpretation is that the specific category of product will have a different (higher or lower) impact on $y$ by $\beta_1$ units. e.g. "sales of red mobile phones are on average higher than the sales of the blue ones by 100 units".

### Log-Log model
Or power model or a multiplicative model:
\begin{equation}
    \log y = \beta_0 + \beta_1 \log x + \log (1+\epsilon) .
    (\#eq:transformLogLog)
\end{equation}
It is equivalent to
\begin{equation}
    y = \beta_0 x^{\beta_1} (1+\epsilon) .
    (\#eq:transformMultiplicative)
\end{equation}
The parameter $\beta_1$ is interpreted as elasticity: If $x$ increases by 1%, the response variable $y$ changes on average by $\beta_1$%. Depending on the value of $\beta_1$, this model can capture non-linear relations with slowing down or accelerating changes. Figure \@ref(fig:transformationsExamples01) demonstrates several examples of artificial data with different values of $\beta_1$.

```{r transformationsExamples01, fig.cap="Examples of log-log relations with different values of elasticity parameter.", echo=FALSE}
par(mfcol=c(2,2))
plot(c(1:100), 1.5*c(1:100)^0.5 * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=0.5$"))
lines(1.5*c(1:100)^0.5, col="red")
plot(c(1:100), 1.5*c(1:100)^{-0.5} * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-0.5$"))
lines(1.5*c(1:100)^{-0.5}, col="red")
plot(c(1:100), 1.5*c(1:100)^2 * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=2$"))
lines(1.5*c(1:100)^2, col="red")
plot(c(1:100), 1.5*c(1:100)^{-2} * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-2$"))
lines(1.5*c(1:100)^{-2}, col="red")
```

As discussed earlier, this model can only be applied to positive data. If there are zeroes in the data, then logarithm will be equal to $-\infty$ and it would not be possible to estimate the model correctly.

### Log-linear model
Or exponential model:
\begin{equation}
    \log y = \beta_0 + \beta_1 x + \log (1+\epsilon) .
    (\#eq:transformLogLinear)
\end{equation}
is equivalent to
\begin{equation}
    y = \beta_0 \exp(\beta_1 x) (1+\epsilon) .
    (\#eq:transformExponential)
\end{equation}

The parameter $\beta_1$ will control the change of speed of growth / decline in the model. If variable $x$ increases by 1 unit, then the variable $y$ will change on average by $(\exp(\beta_1)-1)\times 100$%. If the value of $\beta_1$ is small (roughly $\beta_1 \in (-0.2, 0.2)$), then due to one of the limits the interpretation can be simplified to: when $x$ increases by 1 unit, the variable $y$ will change on average by $\beta_1\times 100$%. The exponent is in general a dangerous function as it exhibits either explosive (when $\beta_1 > 0$) or implosive (when $\beta_1 < 0$) behaviour. This is shown in Figure \@ref(fig:transformationsExamples02), where the values of $\beta_1$ are -0.05 and 0.05, and we can see how fast the value of $y$ changes with the increase of $x$.

```{r transformationsExamples02, fig.cap="Examples of log-linear relations with two values of slope parameter.", echo=FALSE}
par(mfcol=c(1,2))
plot(c(1:100), 1.5*exp(0.05*c(1:100)) * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=0.05$"))
lines(1.5*exp(0.05*c(1:100)), col="red")
plot(c(1:100), 1.5*exp(-0.05*c(1:100)) * rlnorm(100,0,0.1),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-0.05$"))
lines(1.5*exp(-0.05*c(1:100)), col="red")
```
If $x$ is a [dummy variable](#dummyVariables), then its interpretation is slightly different: the presence of the effect $x$ leads on average to the change of variable $y$ by $\beta_1 \times 100$%. e.g. "sales of red laptops are on average 15% higher than sales of blue laptops".

### Linear-Log model
Or logarithmic model.
\begin{equation}
    y = \beta_0 + \beta_1 \log x + \epsilon .
    (\#eq:transformLinearLog)
\end{equation}
This is just a logarithmic transform of explanatory variable. The parameter $\beta_1$ in this case regulates the direction and speed of change. If $x$ increases by 1%, then $y$ will change on average by $\frac{\beta_1}{100}$ units. Figure \@ref(fig:transformationsExamples03) shows two cases of relations with positive and negative slope parameters.

```{r transformationsExamples03, fig.cap="Examples of linear-log relations with two values of slope parameter.", echo=FALSE}
par(mfcol=c(1,2))
plot(c(1:100), 5+1.5*log(c(1:100)) +rnorm(100,0,0.5),
     xlab="x", ylab="y", main=TeX("$\\beta_1=1.5$"))
lines(5 +1.5*log(c(1:100)), col="red")
plot(c(1:100), 5-1.5*log(c(1:100)) +rnorm(100,0,0.5),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-1.5$"))
lines(5 -1.5*log(c(1:100)), col="red")
```

The logarithmic model assumes that the increase in $x$ always leads on average to the slow down of the value of $y$.

### Square root model
\begin{equation}
    y = \beta_0 + \beta_1 \sqrt x + \epsilon .
    (\#eq:transformSqrt)
\end{equation}
The relation between $y$ and $x$ in this model looks similar to the on in linear-log model, but the with a lower speed of change: the square root represents the slow down in the change and might be suitable for cases of diminishing returns of scale in various real life problems. There is no specific interpretation for the parameter $\beta_1$ in this model - it will show how the response variable $y$ will change on average wih increase of square root of $x$ by one. Figure \@ref(fig:transformationsExamples04) demonstrates square root relations for two cases, with parameters $\beta_1=1.5$ and $\beta_1=-1.5$.

```{r transformationsExamples04, fig.cap="Examples of linear - square root relations with two values of slope parameter.", echo=FALSE}
par(mfcol=c(1,2))
plot(c(1:100), 5+1.5*sqrt(c(1:100)) +rnorm(100,0,0.5),
     xlab="x", ylab="y", main=TeX("$\\beta_1=1.5$"))
lines(5 +1.5*sqrt(c(1:100)), col="red")
plot(c(1:100), 5-1.5*sqrt(c(1:100)) +rnorm(100,0,0.5),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-1.5$"))
lines(5 -1.5*sqrt(c(1:100)), col="red")
```

### Quadratic model
\begin{equation}
    y = \beta_0 + \beta_1 x + \beta_2 x^2 + \epsilon .
    (\#eq:transformQuadratic)
\end{equation}
This relation demonstrates increase or decrease with an acceleration due to the present of squared $x$. This model has an extremum (either a minimum or a maximum), when $x=\frac{-\beta_1}{2 \beta_2}$. This means that the growth in the data will be changed by decline or vice versa with the increase of $x$. This makes the model potentially prone to overfitting, so it needs to be used with care. Note that in general the quadratic equation should include both $x$ and $x^2$, unless we know that the extremum should be at the point $x=0$ (see the example with Model 5 in the previous section). Furthrmore, this model is close to the one with square root of $y$: $\sqrt y = \beta_0 + \beta_1 x + \epsilon$, with the main difference being that the latter formulation assumes that the variability of the error term will change together with the change of $x$ (so called "heteroscedasticity" effect, see Section \@ref(assumptionsResidualsAreIID)). This model was used in the examples with stopping distance above. Figure \@ref(fig:transformationsExamples05) shows to classical examples: with branches of the function going down and going up.

```{r transformationsExamples05, fig.cap="Examples of linear-log relations with two values of slope parameter.", echo=FALSE}
par(mfcol=c(1,2))
plot(c(1:100), 400-20*c(1:100)+0.2*c(1:100)^2 +rnorm(100,0,10),
     xlab="x", ylab="y", main=TeX("$\\beta_1=-20$, $\\beta_2=0.2$"))
lines(400-20*c(1:100)+0.2*c(1:100)^2, col="red")
plot(c(1:100), 400+20*c(1:100)-0.2*c(1:100)^2 +rnorm(100,0,10),
     xlab="x", ylab="y", main=TeX("$\\beta_1=20$, $\\beta_2=-0.2$"))
lines(400+20*c(1:100)+-.2*c(1:100)^2, col="red")
```

### Polynomial model
\begin{equation}
    y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots \ \beta_k x^k + \epsilon .
    (\#eq:transformPolynomial)
\end{equation}
This is a more general model than the quadratic one, introducing $k$ polynomials. This is not used very often in analytics, because any data can be approximated by a high order polynomial, and because the branches of polynomial will inevitably lead to infinite increase / decrease, which is not a common tendency in practice.

### Box-Cox transform
Or power transform:
\begin{equation}
    \frac{y^\lambda -1}{\lambda} = \beta_0 + \beta_1 x + \epsilon .
    (\#eq:transformBoxCox)
\end{equation}
This type of transform can be applied to either response variable or any of explanatory variables and can be considered as something more general than linear, log-linear, quadratic and square root models. This is because with different values of $\lambda$, the transformation would revert to one of the above. For example, with $\lambda=1$, we end up with a linear model, just with a different intercept. If $\lambda=0.5$, then we end up with square root, and when $\lambda \rightarrow 0$, then the relation becomes equivalent to logarithmic. The choice of $\lambda$ might be a challenging task on its own, however it can be estimated via [likelihood](#likelihoodApproach). If estimated and close to either 0, 0.5, 1 or 2, then typically a respective transformation should be applied instead of Box-Cox. For example, if $\lambda=0.49$, then taking square root might be a preferred option.

### Logistic transform
In some cases, the variable of interest might lie in a specific region, for example between 0 and 100. In that case a non-linear transform is required to change the range to the conventional one $(-\infty, \infty)$ used in the classical regression. The logistic transform is supposed to do that. Assuming that $y \in (0,1)$ the model based on it can be written as:
\begin{equation}
    y = \frac{1}{1+\exp \left(-(\beta_0 + \beta_1 x + \epsilon)\right)}.
    (\#eq:transformLogit)
\end{equation}

::: remark
If $y$ lies in a different fixed range, then a scaling can be applied to it to make it lie between zero and one. For example, if it lies between 0 and 100, division by 100 will fix the scale.
:::

The inverse logistic transform might also be useful and allows estimating the model using the conventional methods after transforming the response variable:
\begin{equation}
    \log \left( \frac{y}{1-y}\right) = \beta_0 + \beta_1 x + \epsilon.
    (\#eq:transformLogitInverse)
\end{equation}


The logistic function is used in models with binary response variable and is also one of the conventional functions used in more advanced machine learning techniques (e.g. Artificial Neural Networks). Figure \@ref(fig:) demonstrates how the response variable might look in the case of the model \@ref(eq:transformLogit).

```{r transformationsExamples06, fig.cap="Examples of linear-log relations with two values of slope parameter.", echo=FALSE}
plot(1/(1+exp(8-0.03*c(1:500) +rnorm(100,0,1))),
     xlab="x", ylab="y", main="")
lines(1/(1+exp(8-0.03*c(1:500))), col="red")
```

Sometimes the value of $y$ in case of logistic model is interpreted as a probability of outcome. We will discuss models based on logistic function later in this textbook.


### Summary

In this subsection we discussed the basic types of variables transformations on examples with simple linear regression. The more complicated models with multiple explanatory variables and complex transformations can be considered as well. However, whatever transformation is considered, it needs to be meaningful and come from the theory, not from the data. Otherwise we may overfit the data, which will lead to a variety of issues, some of which are discussed in Section \@ref(assumptionsCorrectModel).
