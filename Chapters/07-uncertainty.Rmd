# Dealing with uncertainty in the data {#intervals}
As mentioned in the previous section, we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed [earlier](#estimatesProperties). And, if the [LLN](#LLN) and [CLT](#CLT) hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable $y$ itself. When dealing with the former, we typically work with the **confidence interval** - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the **prediction interval** - the interval constructed for the random variable $y$.

In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have:

1. A random variable $y$, which is assumed to follow some distribution with finite mean $\mu$ and variance $\sigma^2$;
2. A sample of size $T$ from the population of $y$;
3. Estimates of mean $\hat{\mu}=\bar{y}$ and variance $\hat{\sigma}^2 = s^2$, obtained based on the sample of size $T$.


## Confidence interval {#confidenceInterval}
What we want to get by doing this is an idea about the population mean $\mu$. The value $\bar{y}$ does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value $\mu$ can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this (Figure \@ref(fig:normalCurveBasic)).

```{r normalCurveBasic, fig.cap="Distribution of the sample mean.", echo=FALSE}
hist(rnorm(10000), xlab=TeX("$\\bar{y}$"), ylab="Density", xlim=c(-5,5), axes=FALSE, main="")
par(new=TRUE)
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab="", ylab="", type="l")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$")),
       lwd=c(2), col=c("darkblue"))
```

On its own, this distribution just tells us that the variable is random around the true mean $\mu$ and that its density function has a bell-like shape. In order to make this more useful, we can construct the **confidence interval** for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover $(1-\alpha)\times 100$% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this:

```{r normalCurveIntervals, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on the population data."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1)),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=1, lty=1, col="darkblue")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qnorm(0.025),0.01),rev(seq(-5,qnorm(0.025),0.01))),
        c(dnorm(seq(-5,qnorm(0.025),0.01)), rep(0,length(seq(-5,qnorm(0.025),0.01)))), col="grey")
polygon(c(seq(qnorm(0.975),5,0.01),rev(seq(qnorm(0.975),5,0.01))),
        c(dnorm(seq(qnorm(0.975),5,0.01)), rep(0,length(seq(qnorm(0.975),5,0.01)))), col="grey")
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c(TeX("$\\mu$"),"bounds",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkblue","darkred","grey95","grey"))
```

Figure \@ref(fig:normalCurveIntervals) shows the classical normal distribution curve around the population mean $\mu$, confidence interval of the level $1-\alpha$ and the cut off tails, the overall surface of which corresponds to $\alpha$. The value $1-\alpha$ is called **confidence level**, while $\alpha$ is the **significance level**. By constructing the interval this way, we expect that in the $(1-\alpha)\times 100$% of cases the value will be inside the bounds, and in $\alpha\times 100$% it will not.

In reality we do not know the true mean $\mu$, so we do a slightly different thing: we construct a confidence interval based on the sample mean $\bar{y}$ and sample variance $s^2$, hoping that due to [LLN](#LLNandCLT) they will converge to the true values. We use Normal distribution, because we expect [CLT](#LLNandCLT) to work. This process looks something like in Figure \@ref(fig:normalCurveIntervalsShades), with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample:

```{r normalCurveIntervalsShades, echo=FALSE, fig.cap="Distribution of the sample mean and the confidence interval based on a sample."}
plot(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), xlab=TeX("$\\bar{y}$"), ylab="Density", type="l", xlim=c(-5,5),
     lwd=1,lty=2, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),-1),rep(0,length(seq(-5,5,0.1)))),col="grey95",lty=0)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=2, lty=1, col="darkblue")
abline(v=-1, col="darkblue", lwd=2, lty=2)
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dnorm(seq(-5,5,0.1),0),rep(0,length(seq(-5,5,0.1)))),col="white",lty=0)
lines(c(-5,5), c(0,0), col="black", lwd=1)
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1),-1), lwd=1, lty=2, col="darkblue")
lines(seq(-5,5,0.1), dnorm(seq(-5,5,0.1)), lwd=2, lty=1, col="darkgreen")
lines(c(-1,-1), c(0,dnorm(-1)), col="darkblue", lwd=1, lty=2)
abline(v=qnorm(c(0.025,0.975)), col="darkred", lwd=2)
abline(v=0, col="darkgreen", lwd=2, lty=2)
legend("topright",legend=c(TeX("$\\mu$"),"Sample mean","Bounds","True PDF","Assumed PDF"),
       lwd=c(2,2,2,2,2), col=c("darkblue","darkgreen","darkred","darkblue","darkgreen"), lty=c(2,2,1,1,1))
```

So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of $\bar{y}$ and $s^2$. If we construct the confidence interval of the width $1-\alpha$ (e.g. 0.95) for thousands of random samples (thousands of trials), then in $(1-\alpha)\times 100$% of cases (e.g. 95%) the true mean will be covered by the interval, while in $\alpha \times 100$% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials.

Mathematically the red bounds in Figure \@ref(fig:normalCurveIntervalsShades) are represented using the following well-known formula for the confidence interval:
\begin{equation}
    \mu \in (\bar{y} + t_{\alpha/2}(df) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(df) s_{\bar{y}}),
    (\#eq:confidenceInterval)
\end{equation}
where $t_{\alpha/2}(df)$ is Student's t-statistics for $df=T-k$ degrees of freedom ($T$ is the sample size and $k$ is the number of estimated parameters, e.g. $k=1$ in our case) and level $\alpha/2$, and $s_{\bar{y}}=\frac{1}{\sqrt{T}}s$ is the estimate of the standard deviation of the sample mean. If we knew for some reason the true variance $\sigma^2$, then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics.

Note, that in order to construct *confidence interval*, we do not care what distribution $y$ follows, as long as LLN and CLT hold.

## Prediction interval {#confidenceIntervalsPrediction}
If we are interested in capturing the uncertainty about the random variable $y$, then we should refer to prediction interval. In this case, we typically rely on [LLN](#LLNandCLT) and the [assumed distribution](#distributions) for the random variable $y$. For example, if we know that $y \sim \mathcal{N}(\mu, \sigma^2)$, then based on our sample we can construct a prediction interval of the width $1-\alpha$:
\begin{equation}
    y \in (\bar{y} + z_{\alpha/2} s, \bar{y} + z_{1-\alpha/2} s),
    (\#eq:predictionInterval)
\end{equation}
where $z_{\alpha/2}$ is the z-statistics (quantile of standard normal distribution) for the level $\alpha/2$ and $\bar{y}$ is the sample estimate of $\mu$ and $s$ is the sample estimate of $\sigma$. If we assume some other distributions for the random variable, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the [assumptions](#assumptions) do not hold, we might switch to more advanced methods for prediction interval construction.


## Hypothesis testing {#hypothesisTesting}
While we will not need hypothesis testing for ADAM itself, we might face it in some parts of the textbook, so it is worth discussing it briefly.

Hypothesis testing arises naturally from the idea of confidence intervals: instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does.

Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null (H$_0$) and alternative (H$_1$) hypotheses about the process or a property in the population, and we want to find some evidence to reject the H$_0$. Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate H$_0$ that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject H$_0$ (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than £100k per year we would formulate the hypotheses in the following way:
\begin{equation*}
    \mathrm{H}_0: \mu \leq 100, \mathrm{H}_1: \mu > 100.
\end{equation*}
Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps:

1. Formulate null and alternative hypotheses (H$_0$ and H$_1$) based on your understanding of the problem;
2. Select the significance level $\alpha$ on which the hypothesis will be tested;
3. Select the test appropriate for the formulated hypotheses (1);
4. Conduct the test (3) and get the calculated value;
5. Compare the value in (4) with the threshold one;
6. Make a conclusion based on (5) on the selected level (2).

Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called "p-hacking", trying to make results look nicer than they really are.

Consider an example, where we want to check, whether the population mean $\mu$ is equal to zero, based on a sample of 36 observations, where $\bar{y}=-0.5$ and $s^2=1$. In this case, we formulate the null and alternative hypotheses:
\begin{equation*}
    \mathrm{H}_0: \mu=0, \mathrm{H}_1: \mu \neq 0.
\end{equation*}
We then select the significance level $\alpha=0.05$ (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula:
\begin{equation}
    t = \frac{\bar{y} - \mu}{s_{\bar{y}}} = \frac{-0.5 - 0}{\frac{1}{\sqrt{36}}} = -3 .
    (\#eq:ttestFormula)
\end{equation}
After that we get the critical value of t with $df=36-1=35$ degrees of freedom and significance level $\alpha/2=0.025$, which is approximately equal to `r round(qt(0.025, 36-1),3)`. We compare this value with the \@ref(eq:ttestFormula) by absolute and reject H$_0$ if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level.

Visually, the whole process of hypothesis testing explained above can be represented in the following way:

```{r hypothesisTesting01, echo=FALSE, fig.cap="The process of hypothesis testing with t value."}
plot(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), xlab="t", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-5,5,0.1),rev(seq(-5,5,0.1))),
        c(dt(seq(-5,5,0.1), 36-1),rep(0,length(seq(-5,5,0.1)))), col="grey95", lty=0)
lines(seq(-5,5,0.1), dt(seq(-5,5,0.1), 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-5,5), c(0,0), col="black", lwd=1)
polygon(c(seq(-5,qt(0.025, 36-1),0.01),rev(seq(-5,qt(0.025, 36-1),0.01))),
        c(dt(seq(-5,qt(0.025, 36-1),0.01),36-1), rep(0,length(seq(-5,qt(0.025, 36-1),0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1),5,0.01),rev(seq(qt(0.975, 36-1),5,0.01))),
        c(dt(seq(qt(0.975, 36-1),5,0.01),36-1), rep(0,length(seq(qt(0.975, 36-1),5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1), col="darkred", lwd=2)
abline(v=-3, col="darkblue", lwd=2)
legend("topright",legend=c("critical values","calculated t",TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

If the blue line on Figure \@ref(fig:hypothesisTesting01) would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject H$_0$. But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using \@ref(eq:confidenceInterval) and checking, whether the interval covers the $\mu=0$:
\begin{equation*}
    \begin{aligned}
        & \mu \in \left(-0.50 -2.03 \frac{1}{\sqrt{36}}, -0.50 + 2.03 \frac{1}{\sqrt{36}} \right), \\
        & \mu \in (-0.84, -0.16).
    \end{aligned}
\end{equation*}
In our case it does not, so we conclude that we reject H$_0$ on 5% significance level. This can be roughly represented by the graph on  Figure \@ref(fig:hypothesisTesting02):

```{r hypothesisTesting02, echo=FALSE, fig.cap="Confidence interval for for the population mean example."}
plot(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), xlab="Sample mean", ylab="Density", type="l",
     lwd=0, lty=0, col="darkblue")
polygon(c(seq(-1.5,0.5,0.01),rev(seq(-1.5,0.5,0.01))),
        c(dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1),rep(0,length(seq(-1.5,0.5,0.01)))), col="grey95", lty=0)
lines(seq(-1.5,0.5,0.01), dt((seq(-1.5,0.5,0.01)+0.5)*6, 36-1), lwd=1, lty=1, col="darkgreen")
lines(c(-1.5,0.5), c(0,0), col="black", lwd=1)
polygon(c(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01),rev(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01))),
        c(dt((seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(-1.5,qt(0.025, 36-1)/6-0.5,0.01)))), col="grey")
polygon(c(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01),rev(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01))),
        c(dt((seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)+0.5)*6,36-1), rep(0,length(seq(qt(0.975, 36-1)/6-0.5,0.5,0.01)))), col="grey")
abline(v=qt(c(0.025,0.975),36-1)/6-0.5, col="darkred", lwd=2)
abline(v=0, col="darkblue", lwd=2)
legend("topright",legend=c("bounds",TeX("$\\mu$=0"),TeX("$1-\\alpha$"),TeX("$\\alpha$")),
       lwd=c(2,2,6,6), col=c("darkred","darkblue","grey95","grey"))
```

Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from \@ref(eq:ttestFormula) to \@ref(eq:confidenceInterval). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject H$_0$ on the selected significance level.

Also **note** that we never say that we accept H$_0$, because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds.

Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure \@ref(fig:hypothesisTesting01) (calculated value). In R this can be done using the `pt()` function:
```{r}
pt(-3, 36-1)
```
Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately `r round(pt(-3, 36-1)*2, 4)`. This is the significance level, for which the switch from "reject" to "do not reject" happens. We could compare this value with the pre-selected significance level directly, rejecting H$_0$ if it is lower than $\alpha$. This value is called "p-value" and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: **p-value** is the smallest significance level at which a null hypothesis can be rejected.

Despite this simplification, we still need to follow the procedure and select $\alpha$ *before conducting the test*! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also *note* that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. *This is still a binary process*: we either reject, or fail to reject H$_0$, depending on whether p-value is smaller or greater than the selected significance level.

While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean:

1. We reject H$_0$ because t-value is -3, which is smaller than the critical value of `r round(qt(0.025, 36-1),3)` (or equivalently the absolute of t-value is 3, while the critical is `r round(qt(0.975, 36-1),3)`);
2. We reject H$_0$ because p-value is `r round(pt(-3, 36-1)*2, 4)`, which is smaller than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It does not include zero, so we reject $\mathrm{H}_0$.

In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject H$_0$, the approach (3) gives more appropriate information. Consider the case, when we test, whether $\mu=-0.6$ in the example above. We then have the following three approaches to the problem:

1. We fail to reject H$_0$ because t-value is 0.245, which is smaller than the critical value of `r round(qt(0.975, 36-1),3)`;
2. We fail to reject H$_0$ because p-value is 0.808, which is greater than the significance level $\alpha=0.05$;
3. The confidence interval for the mean is $\mu \in (-0.84, -0.16)$. It includes -0.6, so we fail to reject H$_0$. *This does not mean that the true mean is indeed equal to -0.6*, but it means that the region will cover this number in 95% of cases if we do resampling many times.

In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply.

Furthermore, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values [@Wasserstein2016]. Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below:

1. P-values can indicate how incompatible the data are with a specified statistical model;
2. P-values do not measure:
- the probability that the studied hypothesis is true,
- or the probability that the data were produced by random chance alone;
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold;
4. Proper inference requires full reporting and transparency;
5. A p-value, or statistical significance, does not measure:
- the size of an effect
- or the importance of a result;
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than $\alpha$) is not a good idea [@Wasserstein2016]. Follow the proper procedure if you want to test the hypothesis.

Finally, the hypothesis testing mechanism has been criticised by many scientists over the years. For example, @Cohen1994 discussed issues with the procedure, making several important points, some of which are outlined above. He also points out at the fundamental problem with hypothesis testing, which is typically neglected by proponents of the procedure: in practice, null hypothesis is always wrong. In reality, it is not possible for a value to be equal, for example, to zero. Even an unimportant effect of one variable on another would be close to zero, but not equal to it. This means that with the increase of the sample size, H$_0$ will inevitably be rejected. Furthermore, our mind operates with binary constructs: true / not true - while the hypothesis testing works in the dichotomy "**I know** / **I don't know**", with the latter appearing when there is not enough evidence to reject H$_0$. Summarising all of this, in my opinion, it makes sense to move away from hypothesis testing if possible and switch to other instruments for uncertainty measurement, such as confidence intervals.


### Common mistakes related to hypothesis testing {#hypothesisTestingMistakes}
Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently.

1. "Calculated value is lower than the critical one, so the null hypothesis is true".
- This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the H$_0$ being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the "Black swan in Australia". Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that "We fail to reject H$_0$ that all swans are white". Uneducated people would be tempted to say that " H$_0$: All swans are white" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting H$_0$ and showing that "not all swans are white", which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting H$_0$, collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly.
2. "Calculated value is lower than the critical one, so we accept the null hypothesis".
- We **never** accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not "accept H$_0$". This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white).
3. "The parameter in the model is significant, so we can conclude that..."
- You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level $\alpha$. The correct sentence would be "The parameter in the model is significant on 3%, so we can conclude that...", where 3% is the selected significance level $\alpha$.
3. "The parameter in the model is significant because p-value<0.0000"
- Indeed, some statistical software will tell you that `p-value<0.0000`, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, "why bother if it is that low?". Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you.
5. "The parameter is not significant, so we remove the variable from the model".
- This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level ([assumptions](#assumptions) do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject H$_0$. Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. *You always need to have some fundamental reason to include or remove variables in the model*. Hypothesis testing just gives you additional information that can be helpful for decision making.
6. "The parameter in the new model is more significant than in the old one".
- There is no such thing as "more significant" or "less significant". **Significance is binary** and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level $\alpha$ or not.
7. "The p-value of one variables is higher than the p-value of another, so...".
- p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake.

Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer [confidence intervals](#confidenceIntervals), when possible (as discussed above).

Finally, a related question to all of this, is *how to select the significance level*. Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many cases in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical test in variaous circumstances.
