# Introduction {#intro}
Whenever we talk about analytics or forecasting, we deal with models that are constructed based on available information. So, it is important to understand what a model is, what types of models exist and how to measure information in order to use it in models afterwards. This is what we will discuss in the introduction.


## What is model?
Reality is complex. Everything is connected with everything, and it is difficult to isolate an object or a phenomenon from the other objects or phenomena and their environment. Furthermore, due to this complexity of reality, we cannot work with itself, we need to simplify it, leave only the most important parts and analyse them. This process of simplification implies that we create a model of reality, work with it and make conclusions about the reality based on it.

@Pidd2010 defines the **model** as "*an external and explicit representation of part of reality as seen by the people who wish to use that model to understand, to change, to manage and to control that part of reality*". Let us analyse this definition.

- It is *external and explicit*, because if you only think about something, it is not a model. The unclear view on an object is not a model, it needs to be formulated.
- It is a *representation of part of reality*, because it is not possible to represent the reality at full - it is too complex, as discussed above.
- Technically speaking, the model without a purpose is still a model, but @Pidd2010 points out in his definition that without it, the model becomes useless (thus "*to understand, to change, to manage and to control*").
- Finally, *as seen by people* is an important element that shows that models are always subjective. One and the same question can be answered with different models based on preferences of analyst.

<!-- Some argue that there are models for prediction and the others for interpretation. This does not make sense from my perspective. -->

This definition is wide and covers different types of models, starting from simple graphical ones and ending with complex imitations. In fact, there are four fundamental types of models (they are ordered by the increase of complexity):

1. Textual,
2. Visual,
3. Mathematical,
4. Imitation.

*Textual* model is just a description of an object or a process. An instruction of how to assemble a chair is an example of a textual model. Any classification will be a textual model as well, so the list of four types of models is a textual model.

*Visual* model is a graphical or a schematic representation of an object or a process. An example of such a model is provided in Figure \@ref(fig:chairAssembly).

```{r chairAssembly, echo=FALSE, out.width='60%', fig.align='center', fig.cap="Chair assembly instruction. Found on Reddit."}
knitr::include_graphics("images/chairAssembly.jpg")
```

*Mathematical* model is a model that is represented using equations. It is more complex than the previous two, because it requires an understanding of mathematics. At the same time it can be more precise than the previous two models in terms of capturing the structure of reality and making predictions about it. The mass-energy equivalence equation is an example of such model:
\begin{equation*}
    E = m c^2 .
\end{equation*}

A mathematical model in turn can be either deterministic or stochastic. The former one assumes that there is no randomness in it, while the latter implies that the randomness exists and can be modelled in one way or another. The related classification of models based on the amount of randomness is:

a. White box - the deterministic model. An example of such model is a linear programming, which assumes that there is no randomness in the data;
b. Grey box - the model that assumes some randomness, but for which the structure is known (or assumed). Any statistical model can be considered as a grey box: typically, we have an understanding of how elements in it interact with each other and how the result is obtained;
c. Black box - the model with randomness, for which we do not know what is happening inside. An example of such model is an artificial neural network.

Finally, the *imitation* model is a simplified reproduction of a real object or a process. This can be, for example, a physical model of a building standing in a room of an architect, or a mental arrangement in psychology.

In this textbook, we will deal with only first three types of models, focusing on the third one.

When constructing mathematical models, we will inevitably deal with variables, with factors that are potentially related to each other and reflect some aspects of the real object or phenomenon. These variables can be split into two categories:

1. Input, or external, or exogenous, or explanatory variables - those that are provided to us and are assumed to impact the variable (or several variables) of interest;
2. Output, or internal, or endogenous, or response variable(s) - the variable of the main interest, which is assumed to be related by a set of explanatory variables.

The models that have only one response variable are called "*univariate*" models. But in some cases, we might have several response variables (for example, sales of several similar products). We would then deal with a *multivariate* model. In the literature, you might meet a different definition of univariate / multivariate models. For example, some consider a model with several variables multivariate, even if it has only one response and several explanatory ones. But throughout this textbook we use the definitions above, focused on response variable.


### Models, methods et al. {#modelsMethods}
There are several other definitions that will be useful throughout this textbook:

- **Statistical model** (or 'stochastic model', or just 'model' in this textbook) is a 'mathematical representation of a real phenomenon with a complete specification of distribution and parameters' [@Svetunkov2019a]. Very roughly, the statistical model is something that contains a structure (defined by its parameters) and a noise that follows some distribution.
- **True model** is the idealistic statistical model that is correctly specified (has all the necessary components in correct form), and applied to the data in population. By this definition, true model is never reachable in reality, but it is achievable in theory if for some reason we know what components and variables and in what form should be in the model, and have all the data in the world. The notion itself is important when discussing how far the model that we use is from the true one.
- **Estimated model** (aka 'applied model' or 'used model') is the statistical model that was constructed and estimated on the available sample of data. This typically differs from the true model, because the latter is not known. Even if the specification of the true model is known for some reason, the parameters of the estimated model will differ from the true parameters due to sampling randomness, but will hopefully [converge to the true ones](#statistics) if the sample size increases.
- **Data generating process** (DGP) is an artificial statistical model, showing how the data could be generated in theory. This notion is utopic and can be used in simulation experiments in order to check, how the selected model with the specific estimator behave in a specific setting. In real life, the data is not generated from any process, but is usually based on complex interactions between different agents in a dynamic environment. Note that I make a distinction between DGP and true model, because I do not think that the idea of something being generated using a mathematical formula is helpful. Many statisticians will not agree with me on this distinction.
- **Forecasting method** is a mathematical procedure that generates point and / or interval forecasts, with or without a statistical model [@Svetunkov2019a]. Very roughly, forecasting method is just a way of producing forecasts that does not explain how the components of time series interact with each other. It might be needed in order to filter out the noise and extrapolate the structure.

Mathematically in the simplest case the true model can be presented in the form:
\begin{equation}
    y_t = \mu_{y,t} + \epsilon_t,
    (\#eq:TrueModel)
\end{equation}
where $y_t$ is the actual observation, $\mu_{y,t}$ is the structure in the data and $\epsilon_t$ is the noise with zero mean, unpredictable element, which arises because of the effect of a lot of small factors and $t$ is the time index. An example would be the daily sales of beer in a pub, which has some seasonality (we see growth in sales every weekends), some other elements of structure plus the white noise (I might go to a different pub, reducing the sales of beer by one pint). So what we typically want to do in forecasting is to capture the structure and also represent the noise with a distribution with some parameters.

When it comes to applying the chosen model to the data, it can be presented as:
\begin{equation}
    y_t = \hat{\mu}_{y,t} + e_t,
    (\#eq:AppliedModel)
\end{equation}
where $\hat{\mu}_{y,t}$ is the estimate of the structure and $e_t$ is the estimate of the white noise (also known as "**residuals**"). As you see even if the structure is correctly captured, the main difference between \@ref(eq:TrueModel) and \@ref(eq:AppliedModel) is that the latter is estimated on a sample, so we can only approximate the true structure with some degree of precision.

If we generate the data from the model \@ref(eq:TrueModel), then we can talk about the DGP, keeping in mind that we are talking about an artificial experiment, for which we know the true model and the parameters. This can be useful if we want to see how different models and estimators behave in different conditions.

The simplest forecasting method can be represented with the equation:
\begin{equation}
    \hat{y}_t = \hat{\mu}_{y,t},
    (\#eq:Method)
\end{equation}
where $\hat{y}_t$ is the point forecast. This equation does not explain where the structure and the noise come from, it just shows the way of producing point forecasts.

In addition, we will discuss in this textbook two types of models:

1. Additive, where (most) components are added to one another;
2. Multiplicative, where the components are multiplied.

\@ref(eq:TrueModel) is an example of additive error model. A general example of multiplicative error model is:
\begin{equation}
    y_t = \mu_{y,t} \varepsilon_t,
    (\#eq:TrueModelMultiplicative)
\end{equation}
where $\varepsilon_t$ is some noise again, which in the reasonable cases should take only positive values and have mean of one. We will discuss this type of model later in the textbook. We will also see several examples of statistical models, forecasting methods, DGPs and other notions and discuss how they relate to each other.

**Note** that throughout this textbook we will use index $t$ to denote the time and index $i$ to denote the cross-sectional elements. So, for example, $y_t$ will mean the response variable changing over time, while $y_i$ will mean the response variable changing from one object to another (for instance, from one person to another).


<!-- Furthermore, we will be talking about forecasting in this textbook, so it is also important to introduce the notation $\hat{y}_{t+h}$, which corresponds to the $h$ steps ahead **point forecast** produced from the observation $t$. Typically, this corresponds to the conditional expectation of the model $\mu_{t+h|t}$, given all the information on observation $t$, although this does not hold all the time for the classical ETS models. When we use notation $\mu_{y,t}$, then this means one step ahead conditional expectation on observation $t$, given the information on the previous observation $t-1$. We will also discuss **prediction interval**, the term which means specific bounds that should include $(1-\alpha)\times100$% of observations in the holdout sample (for example, 95% of observations). Finally, we will also use sometimes the term **confidence interval** usually refers to similar bounds constructed either for parameters of models or for the specific statistics (e.g. conditional mean or variance). -->


<!-- What is system, information and model. How do we get to the data what are the data types. Response, endogenous and exogenous variables. -->


## Scales of information {#scales}
Whenever we work with information, we need to understand how to measure it properly. If we cannot do that, then we cannot construct any model and make proper decisions, supported by evidence. For example, if a person feels ill but we cannot say what the temperature of their body is, then we cannot decide, whether anything needs to be done to reduce it. If we can measure something then we can model it and produce forecasts. Continuing our example, if the temperature is 39°C, then we can conclude that the person is sick and needs to take Paracetamol or some other pills that would reduce the temperature. So, whenever we collect some sort of information about a system's behaviour or about a process, we will inevitably deal with scales of information and it is important to understand what we are dealing with in order to process that information correctly. There are four fundamental scales:

1. Nominal,
2. Ordinal,
3. Interval,
4. Ratio.

The first two form the so called "categorical" scale, while the latter two are typically united in the "numerical" one. Each one of these scales can have one of the following characteristics:

1. Description,
2. Order,
3. Distance,
4. Natural zero,
5. Natural unit.

The last characteristics is typically ignored analytics and forecasting as it does not provide any useful information. But as for the other four, they provide important properties to the scales of information, giving them more flexibility. Here we discuss the scales in detail.

### Nominal scale
This is the scale that only has "description" characteristics. It does not have an order, a distance or a natural zero. There is only one operation that can be done in this scale, and it is comparison, whether the value is "equal" or "not equal" to something. An example of data measured in such scale is the following question in a survey:

What is your nationality?

- Russian,
- English,
- Greek,
- Swiss,
- Belgian,
- Lebanese,
- Indonesian,
- Other.

In this case after collecting the data we can only say whether each respondent is Russian or not, English or not etc. So, the only thing that can be done with the data measured in this scale is to produce a basic summary, showing how many people selected one option or another. Among the statistical instruments, *only the mode* is useful, as it shows which of the options was selected the most. If there are several variables measured in nominal scale, we can calculate some measures of association to see if there are any patterns in respondents behaviour (e.g. those who select "Russian" would prefer Vodka, while those who selected "Belgian" will tend to drink "Beer").

When it comes to constructing models, the nominal scale is typically transformed in a set of dummy variables, which will be discussed later in [regression analysis](#regression) of this textbook.

If you are not sure, whether your data is measured in nominal or another scale, you can do a simple test: if changing the places of two values does not break the scale, then this is the nominal one. For example, in the question above, moving "Greek" to the first place will not change anything, so this is indeed the nominal scale. Another example of nominal scale is the number on the T-shirt of football players. They are only descriptive, and if two players change numbers, this will not change anything (although it might confuse football fans).


### Ordinal scale
In addition to description, the ordinal scale has the "order". It is possible to say that one value can be placed higher or lower than the other on a scale (thus, permitting operations "greater" and "smaller" in addition to the "equal" and "not equal") However, it is not possible to say how far the elements are placed from each other, so the number of operations in the scale is still limited. Here is an example of a survey question with such scale:

How old are you?

1. Too young,
2. Young,
3. Not too young,
4. Not too old
5. Old,
6. Too old.

In this scale above we have a natural order, and when collecting the data in this scale we can conclude, whether a respondent is older than another one or not. Sometimes ordinal scales look confusing and seem to be of a higher level than they are, here is an example:

How old are you?

1. Younger than 16,
2. 16 - 25,
3. 26 - 40,
4. 41 - 60
5. Older than 60.

This is still an ordinal scale, because it has the natural order, and because we cannot measure the distance between the value: if, for example we subtract "16 - 25" from "26 - 40", we will not get anything meaningful.

The ordinal scale, being more complex than the nominal one, allows using some additional statistical instruments (besides the mode), such as quantiles of distribution, including median. Unfortunately, the *arithmetic mean is not applicable to the data in ordinal scale*, because of the absence of distance. Even if you encode every answer in numbers, the resulting average will not be meaningful. Indeed, if in the question above with the five options, we use the numbers ("1" for the first option, "2" for the second one, etc.) and take average, the resulting number of, for example, 3.75 will not mean anything, as there is no element in the scale that would correspond to that number.

When it comes to measuring relations between two variables in ordinal scale, we can use Kendall's $\tau$ correlation coefficient, Goodman-Kruskal's $\gamma$ and Yule's Q. These are discussed in detail in Section \@ref(correlations). As for using the variables in ordinal scale in modelling, the typical thing to do would be to create a set of dummy variables, similarly to how it is done for variables in nominals scale.

As for the identification of scale, if in doubt, you can do any transformation of elements of scale without the loss of its meaning. For example, if we assign numbers from 1 to 5 to the responses above, we can square each one of them and get 1, 4, 9, 16 and 25, which would not change the original scale, but only encode the answers differently (select "16" for the option "41 - 60").


### Interval scale
This scale is even more complex than the previous two, as in addition to description and order it also has a distance. This permits doing addition and subtraction to the elements of scale, which are meaningful operations in this case. Arithmetic mean and standard deviation become available in this scale in addition to all those used in lower level scales discussed above. The classical example of a variable measured in this scale is the temperature. Indeed, we can not only say if the temperature of one person is higher than the temperature of the other one, but also by how much: 39°C - 37°C = 2°C, which is a meaningful number in the scale. The only limitation in this scale is that there is no natural zero. 0°C does not mean the absence of temperature, but rather means the point at which water starts freezing. If we switch to Fahrenheit (although why would anyone do that?!), then the 0°F would correspond to the point, where the mixture of ice, water, and ammonium chloride used to stabilise back in 1724, when Fahrenheit proposed the scale.

The relation between two variables in interval scale can be measured by Pearson's correlation coefficient. The scale can be used in the model as is, although some [error metrics](#errorMeasures) cannot be used for the measurement of accuracy of models for this scale (for example, MAPE cannot be used as it assumes meaningful zero).

Finally, when it comes to the identification of scale, only linear transformations are permitted for the variables without the loss of its properties. This means that if we measure temperature of two respondents and then do their linear transformations via $y=a+bx$, then the characteristics of scale will not be broken: it will still have description, order and distance with the same meaning as prior to the transformation. In the example of temperature, this is how you switch, for example, from Celsius to Fahrenheit ($y=a+bx$).


### Ratio scale
The most complex of the four, this ratio has a natural zero (in addition to all the other characteristics). It permits any operations to the values of scale, including product, division, and non-linear transformations. Coefficient of variation can be used together in addition to all the previous instruments. An example of the information measured in this scale is the height of respondents in meters. You can compare two respondents via their height and not only say that one is higher than the other, but also by how much and how many times. All these operations will be meaningful in this scale.

If you need to check, whether the variables is indeed in ratio scale, note that only the transformation via multiplication would maintain the meaning of the scale. For example, height measured in meters can be transformed into height in feet via the multiplication by approximately 3.28. If you add a constant to the values of scale, it will break it.

Being the most complex, this scale permits usage of all [correlation coefficients](#correlations) and all [error metrics](#errorMeasures).

Finally, the variables measured in this scale can be either integer or continuous. This might cause some confusions, because the integer numbers sometimes look suspiciously similar to the values of ordinal scale, but the tools of identification discussed above might help. If a company needs to buy 7 planes, then this is an integer variable measured in ratio scale: 7 planes is more than 6 planes by one plane, and zero planes means that there are no planes (all the characteristics of ratio scale). Furthermore, squaring the number of planes breaks the distance between them ($7^2 - 6^2 \neq 1^2$), while linear transformation breaks the scale ($7\times 2 + 3$ has a completely different meaning in the scale than just 7).


## Sources of uncertainty {#sourcesOfUncertainty}
When estimating any model on a sample of data, we will inevitably have to deal with uncertainty. Consider an example, when we want to estimate the average height of a person in the room. We could take heights of all the people in the room, then take average and we would get our answer. But what would happen with that average if another person comes in the room? We would need to do additional measures and re-estimate the average, and inevitably it will be different from the one we had before. This example demonstrates one of the classical sources of uncertainty - the one caused by estimation on a sample of data.

Furthermore, we might be interested in predicting the weight of a person based on their height. The two variables will be related, but would not have a functional relation: with the increase of height we expect that a person will weigh more, but this only holds on average. So, based on a sample of data, we could estimate the relation between the two variables and then having a height of a person, we could predict their expected weight. Their individual weight will inevitably vary from one person to another. This is the second source of uncertainty, appearing because of the individual discrepancies from one person to another.

Finally, the model of weight from height could be wrong for different reasons. For example, there might be plenty of other factors that would impact the weight of person that we have not taken into account. In fact, we never know the true model (see Section \@ref(modelsMethods)), so this is the third source of uncertainty, the one around the model form.

These three sources of uncertainty have been summarised for the first time in @Chatfield1996. Whenever we need to construct any type of model, we will deal with:

1. Uncertainty about the data, e.g. the error term $\epsilon_t$ (see Section \@ref(uncertaintyData));
2. Uncertainty about estimates of parameters (see Section \@ref(uncertaintyParameters));
3. Uncertainty about the model form (see Section \@ref(uncertaintyModel)).

In this textbook we will discuss all of them, slowly moving from (1) to (3), introducing more advanced techniques for model building.
