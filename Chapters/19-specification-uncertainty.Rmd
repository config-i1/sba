# Uncertainty about the model form {#uncertaintyModel}
In this Chapter, we discuss more advanced topics related to regression modelling. In a way, this part builds upon elements of Statistical Learning [see, for example, the textbook of @Hastie2009] and focuses on how to select variables for regression model. We start with a fundamental idea of bias-variance tradeoff, which lies in the core of many selection methods. We then move to the discussion of information criteria, explaining what they imply, after that - to several existing variable selection approaches, explaining their advantages and limitations. Furthermore, we discuss combination approaches and what they mean in terms of parameters of models. We finish this chapter with an introductory discussion of regularisation techniques (such as LASSO and RIDGE).

## Bias-variance tradeoff
In order to better understand, why we need to bother with model selection, combinations and advanced estimators, we need to understand the principle of bias-variance tradeoff. Consider a simple example of relation between fuel consumption of a car and the engine size based on the `mtcars` dataset in R (Figure \@ref(fig:biasVariance01)):

```{r biasVariance01, fig.cap="Fuel consumption vs engine size"}
plot(mtcars$disp, mtcars$mpg,
     xlab="Engine size", ylab="Miles per galon")
```

The plot in Figure \@ref(fig:biasVariance01) demonstrates clear non-linearity. Indeed, we would expect the relation between these variables to be non-linear in real life: it is difficult to imagine the situation, where a car with no engine will be able to drive at all. On the other hand, a car with a huge engine will still be able to drive some distance, although probably very small. The linear model would assume that the "no engine" case would correspond to the value of approximately 30 miles per gallon (the intersection with y-axis), while the case of "huge engine" would probably result in negative mileage. So, the theoretically suitable model should be multiplicative, which for example can be formulated in logarithms:
\begin{equation}
    \log mpg_j = \beta_0 + \beta_1 \log x_j + \epsilon_j .
    (\#eq:mpgLinear)
\end{equation}
We will assume for now that this is the "true model", which would fit the data in the following way if we knew all the data in the universe (Figure \@ref(fig:biasVariance02)):

```{r biasVariance02, fig.cap="Fuel consumption vs engine size and the true model", echo=FALSE}
load("data/biasVariance.Rdata")

plot(mtcars$disp, mtcars$mpg,
     xlab="Engine size", ylab="Miles per galon")
lines(seq(1,500,1),
      mpgLogsALMFitted,
      col="darkgreen",lwd=2)
```

While being wrong, we could still use the linear model to capture some relations in some parts of the data. It would not be a perfect fit (and would have some issues in the tails of our data), but it would be an acceptable approximation of the true model in some situations. If we vary the sample, we will see how the model would behave, which would help us in understanding of the uncertainty associated with it (Figure \@ref(fig:biasVariance03)).

```{r eval=FALSE, echo=FALSE}
mpgLogsALM <- alm(mpg~log(disp), mtcars, distribution="dlnorm")
mpgLogsALMFitted <- predict(mpgLogsALM,data.frame(disp=seq(1,500,1)))$mean

nIterations <- 1000
mpgLinALM <- vector("list",nrow(mtcars))
for(i in 1:nIterations){
    mpgLinALM[[i]] <- lm(mpg~disp, mtcars, subset=sample(c(1:nrow(mtcars)),20,TRUE))
}
mpgLinALMFitted <- sapply(mpgLinALM, predict, newdata=data.frame(disp=seq(1,500,1)))

mpgPolyALM <- vector("list",nrow(mtcars))
for(i in 1:nIterations){
    mpgPolyALM[[i]] <- lm(mpg~disp+I(disp^2)+I(disp^3)+I(disp^4), mtcars, subset=sample(c(1:nrow(mtcars)),20,TRUE))
}
mpgPolyALMFitted <- sapply(mpgPolyALM, predict, newdata=data.frame(disp=seq(1,500,1)))

mpgPolyALM2 <- vector("list",nrow(mtcars))
for(i in 1:nIterations){
    mpgPolyALM2[[i]] <- lm(mpg~disp+I(disp^2)+I(disp^3)+I(disp^4)+I(disp^5)+I(disp^6)+I(disp^7), mtcars, subset=sample(c(1:nrow(mtcars)),20,TRUE))
}
mpgPolyALM2Fitted <- sapply(mpgPolyALM2, predict, newdata=data.frame(disp=seq(1,500,1)))

save(mpgLogsALMFitted, mpgLinALMFitted, mpgPolyALMFitted, mpgPolyALM2Fitted, file="../data/biasVariance.Rdata")

```

```{r biasVariance03, fig.cap="Fuel consumption vs engine size, the true and the linear models", echo=FALSE}
plot(mtcars$disp, mtcars$mpg,
     xlab="Engine size", ylab="Miles per galon",
     col="white")
for(i in 1:nIterations){
    lines(seq(1,500,1),
          mpgLinALMFitted[,i],
          col=rgb(0.5,0.5,1,0.5),lwd=1)
}
lines(seq(1,500,1),
      mpgLogsALMFitted,
      col="darkgreen",lwd=1)
points(mtcars$disp, mtcars$mpg)
```

Figure \@ref(fig:biasVariance03) demonstrates the situation, where the linear model was fit to randomly peaked sub-samples of the original data. We can see that the linear model would exhibit some sort of bias in comparison with the true one: it is consistently above the true model in the region in the middle and is consistently below it in the tails of the sample.

Alternatively, we could fit a high order polynomial model to approximate the data and repeat the same proecedure with sub-samples as before (Figure \@ref(fig:biasVariance04)):

```{r biasVariance04, fig.cap="Fuel consumption vs engine size, the true, the linear and the polynomial models.", echo=FALSE}
plot(mtcars$disp, mtcars$mpg,
     xlab="Engine size", ylab="Miles per galon",
     col="white")
for(i in 1:nIterations){
    lines(seq(1,500,1),
          mpgPolyALMFitted[,i],
          col=rgb(0.9,0.5,0.2,0.5),lwd=1)
}
lines(seq(1,500,1),
      mpgLogsALMFitted,
      col="darkgreen",lwd=1)
points(mtcars$disp, mtcars$mpg)
```

The new polynomial model on the plot in Figure \@ref(fig:biasVariance04) has a lower bias than the linear one, because on average it is closer to the true model in sample, getting closer to the green line (true model) even in the tails. However, it is also apparent that it has higher variability. This is because it is a more complex model than the linear one: it includes more variables (polynomial terms), making it more sensitive to specific observations in the sample. If we were to introduce even more polynomial terms, the model would have even more variance around the true model than before (Figure \@ref(fig:biasVariance05)).

```{r biasVariance05, fig.cap="Fuel consumption vs engine size, the true and the polynomial (7th order) models", echo=FALSE}
plot(mtcars$disp, mtcars$mpg,
     xlab="Engine size", ylab="Miles per galon",
     col="white")
for(i in 1:nIterations){
    lines(seq(1,500,1),
          mpgPolyALM2Fitted[,i],
          col=rgb(1,0.5,0.5,0.5),lwd=1)
}
lines(seq(1,500,1),
      mpgLogsALMFitted,
      col="darkgreen",lwd=1)
points(mtcars$disp, mtcars$mpg)
```

The model on plot in Figure \@ref(fig:biasVariance05) exhibits even higher variance in comparison with the true model, but it is still less biased than the linear model. The pattern that we observe in this demonstration is that the variance of the model in comparison with the true one increases with the increase of complexity, while the bias either decreases or does not change substantially. This is bias-variance tradeoff in action. It is the principle that states that with the increase of complexity of model, its variance (with respect to the true one) increases, while the bias decreases. This implies that typically you cannot minimise both variance and bias at the same time - depending on how you formulate and estimate a model, it will either have bigger variance or a bigger bias. This principle can be applied not only to models, but also to estimates of parameters or to forecasts from the models. It is one of the fundamental basic modelling principles.
