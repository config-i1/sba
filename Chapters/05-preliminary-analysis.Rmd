# Data handling and analysis {#dataAnalysis}
Take any of the examples discussed in Chapters \@ref(countDistributions) and \@ref(distributions). How do we know the true parameter of a distribution? For example, how can we get the arrival rate of patients for the Poisson distribution in the example from Section \@ref(distributionPoisson)? There usually is only one way to get the value - collect the data and estimate it.

::: remark
While one can also use judgment to set specific values, in general such estimates contain biases and should only be used if no data is available for the analysis.
:::

Furthermore, in some cases we need to get an idea about the distribution of values for further analysis and to have a feeling about the potential relation between the variables.

How can we do all of that?

In this chapter, we discuss what the potential issues with the data can be, how we can clean it, and what sort of basic preliminary analysis we can do based on data to get some estimates of parameters, which we can then use for a more advanced analysis. The latter can be done either using numerical or graphical methods. The numerical one is useful when you want to have a summary information about the data, while the graphical is useful when you can spend more time, investigating relations and issues in the data. In many cases, they complement each other.

::: remark
We do not recommend to end your work with just preliminary data analysis. Usually, this is just a first step to get a better understanding of the dataset, which can then be followed by model construction (discussed in Chapters \@ref(simpleLinearRegression) and \@ref(linearRegression)).
:::


## Numerical analysis {#dataAnalysisNumerical}
In this section we will use the classical `mtcars` dataset from `datasets` package for R. It contains 32 observations with 11 variables. While all the variables are numerical, some of them are in fact categorical variables encoded as binary ones. We can check the description of the dataset in R:

```{r eval=FALSE}
?mtcars
```

Judging by the explanation in the R documentation, the following variables are categorical:

1. vs - Engine (0 = V-shaped, 1 = straight),
2. am - Transmission (0 = automatic, 1 = manual).

In addition, the following variables are integer numeric ones:

1. cyl - Number of cylinders,
2. hp - Gross horsepower,
3. gear - Number of forward gears,
4. carb - Number of carburettors.

All the other variables are continuous numeric.

Taking this into account, we will create a data frame, encoding the categorical variables as factors for further analysis:

```{r}
mtcarsData <- data.frame(mtcars)
mtcarsData$vs <- factor(mtcarsData$vs,levels=c(0,1),labels=c("V-shaped","Straight"))
mtcarsData$am <- factor(mtcarsData$am,levels=c(0,1),labels=c("automatic","manual"))
```

Given that we only have two options in those variables, it is not compulsory to do this encoding, but it will help us in the further analysis.

We can start with the basic summary statistics. We remember from the scales of information (Section \@ref(scales)) that the nominal variables can be analysed only via frequencies, so this is what we can produce for them:

```{r}
table(mtcarsData$vs)
table(mtcarsData$am)
```

These tables are called **contingency tables**, they show the frequency of appearance of values of variables. Based on this, we can conclude that the cars with V-shaped engine are met more often in the dataset than the cars with the Straight one. In addition, the automatic transmission prevails in the data. The related statistics which is useful for analysis of categorical variables is called **mode**. It shows which of the values happens most often in the data. Judging by the frequencies above, we can conclude that the mode for the first variable is the value "V-shaped".

All of this is purely descriptive information, which does not provide us much. We could probably get more information if we analysed the contingency table based on these two variables:

```{r}
table(mtcarsData$vs,mtcarsData$am)
```

For now, we can only conclude that the cars with V-shaped engine and automatic transmission are met more often than the other cars in the dataset.

Next, we can look at the numerical variables. As we recall from Section \@ref(scales), this scale supports all operations, so we can use quantiles, mean, standard deviation etc. Here how we can analyse, for example, the variable mpg:

```{r}
setNames(mean(mtcarsData$mpg),"mean")
quantile(mtcarsData$mpg)
setNames(median(mtcarsData$mpg),"median")
```

The output above produces:

1. **Mean** - the average value of mpg in the dataset, $\bar{y}=\frac{1}{n}\sum_{j=1}^n y_j$.
2. **Quantiles** - the values that show, below which values the respective proportions of the dataset lie. For example, 25% of observations have mpg less than 15.425. The 25%, 50% and 75% quantiles are also called 1st, 2nd and 3rd **quartiles** respectively.
3. **Median**, which splits the sample in two halves. It corresponds to the 50% quantile.

If median is greater than mean, then this typically means that the distribution of the variable is skewed (it has some rare observations that have large values).

Making a step back, we also need to mention the **variance**, which shows the overall variability of the variable around its mean:
\begin{equation}
    \mathrm{V}(y)= \frac{1}{n-1}\sum_{j=1}^n (y_j - \bar{y})^2 .
    (\#eq:Variance)
\end{equation}
Note that the division in \@ref(eq:Variance) is done by $n-1$, and not by $n$. This is done in order to correct the value for the in-sample bias (we will discuss this in Subsection \@ref(estimatesPropertiesBias)). The alternative formula for the variance can be derived by opening the brackets and regrouping the elements to obtain:
\begin{equation}
    \mathrm{V}(y)= \mathrm{E}(y^2) - \mathrm{E}(y)^2.
    (\#eq:VarianceAlt)
\end{equation}
This form sometimes becomes useful for some derivations. The value of the variance itself does not tell us much about the variability, but having it allows calculating other more advanced measures. Typically the square root of variance is used in inferences, because it is measured in the same scale as the original data. It is called **standard deviation**:
\begin{equation}
    \hat{\sigma} = \sqrt{\mathrm{V}(y)} ,
    (\#eq:StdDev)
\end{equation}
it has the same scale as the variable $y_j$. In our example, both can be obtained via:

```{r}
var(mtcarsData$mpg)
sd(mtcarsData$mpg)
```

Visually, standard deviation can be represented as a straight line, depicting the overall variability of the data (Figure \@ref(fig:varianceVisual)).

```{r varianceVisual, fig.cap="Visual presentation of standard deviation. The value of standard deviation corresponds to the segment between squares at the top of the histogram.", echo=FALSE}
hist(mtcarsData$mpg, xlab="Miles per galon", main="", ylim=c(0,15),
     col=17)
lines(rep(mean(mtcarsData$mpg),2), c(0,13), col=2, lwd=2, lty=2)
lines(mean(mtcarsData$mpg)+c(-0.5,0.5)*sd(mtcarsData$mpg), c(13,13),
      col=2, lwd=2)
points(mean(mtcarsData$mpg)+c(-0.5,0.5)*sd(mtcarsData$mpg), c(13,13),
       pch=15, col=2, lwd=2)
text(20, 14, TeX("$\\hat{\\sigma}$"))
```

In Figure \@ref(fig:varianceVisual), we depict the distribution of the variable `mpg` using histogram (see Section \@ref(dataAnalysisGraphical)) and show how the standard deviation relates to it -- it is equal to the length of the segment above the histogram.

Coming back to the point about the asymmetry of the distribution of a variable in our example, we can investigate it further using skewness and kurtosis from `timeDate` package:

```{r}
timeDate::skewness(mtcarsData$mpg)
timeDate::kurtosis(mtcarsData$mpg)
```

**Skewness** shows the asymmetry of distribution. If it is greater than zero, then the distribution has the long right tail. If it is equal to zero, then it is symmetric. It is calculated as:
\begin{equation}
    \mathrm{skew}(y)= \frac{1}{n}\sum_{j=1}^n \frac{(y_j - \bar{y})^3}{\hat{\sigma}^3} .
    (\#eq:Skewness)
\end{equation}

**Kurtosis** shows the excess of distribution (fatness of tails) in comparison with the normal distribution. If it is equal to zero, then it is the same as for the normal distribution. Here how it is calculated:
\begin{equation}
    \mathrm{kurt}(y)= \frac{1}{n}\sum_{j=1}^n \frac{(y_j - \bar{y})^4}{\hat{\sigma}^4} - 3 .
    (\#eq:Kurtosis)
\end{equation}
Note that there is $3$ in the formula. This is because the excess (which is the value without 3) of Normal distribution is equal to 3. Instead of dividing the value by 3 (which would make kurtosis easier to interpret), Karl Pearson has decided to use subtraction. The same formula \@ref(eq:Kurtosis) can be rewritten as:
\begin{equation}
    \mathrm{kurt}(y)= \frac{1}{n}\sum_{j=1}^n \left(\frac{y_j - \bar{y}}{\hat{\sigma}}\right)^4 - 3 .
    (\#eq:KurtosisRewritten)
\end{equation}
Analysing the formula \@ref(eq:KurtosisRewritten), we see that the impact of the deviations lying inside one $\hat{\sigma}$ bounds are reduced, while those that lie outside, are increased. e.g. if the value $y_j - \bar{y} > \hat{\sigma}$, then the value in the ratio will be greater than one (e.g. 2), thus increasing the final value (e.g. $2^4=16$), while in the opposite case of $y_j - \bar{y} < \hat{\sigma}$, the ratio will be diminished (e.g. $0.5^4 = 0.0625$). After summing up all $n$ values in \@ref(eq:KurtosisRewritten), the values outside one $\hat{\sigma}$ will have a bigger impact on the resulting kurtosis than those lying inside. So, kurtosis will be higher for the distributions with longer tails and in the cases when there are outliers in the data.


<!-- Another potentially useful measure is the half moment (`hm` from `greybox`), which returns the complex number: -->
<!-- ```{r} -->
<!-- hm(mtcarsData$mpg) -->
<!-- ``` -->

<!-- It is calculated as: $\mathrm{hm}=\frac{1}{n}\sum_{j=1}^n \sqrt{y_j}$. The real part of this number corresponds to the positive values in comparison with the centre of distribution, while the imaginary shows the negative ones. Given that the imaginary part is greater than the real, we can conclude that the values are more density to the left of the mean of distribution rather than to the right. This corresponds to the positive skewness. The related coefficient based on that is coefficient of asymmetry (`asymmetry` from `greybox`, starting from v1.0.0): -->

<!-- ```{r} -->
<!-- asymmetry(mtcarsData$mpg) -->
<!-- ``` -->

<!-- It lies between -1 and 1 and is equal to zero only if the distribution is symmetric. It shows, where the higher density of values is (the negative number means that it is to the left from the centre, while the positive means that it is to the right). The main advantage of hm and complex asymmetry coefficients is that they are robust to outliers and are not as sensitive as the classical skewness coefficient is. -->

Based on all of this, we can conclude that the distribution of `mpg` is skewed and has the longer right tail. This is expected for such variable, because the cars that have higher mileage are not common in this dataset.

All the conventional statistics discussed above can be produced using the following summary for all variables in the dataset:
```{r}
summary(mtcarsData)
```

Finally, one other moment that is often used in statistics is called "covariance". It measures the joint variability of two variables and is calculated as:
\begin{equation}
    \mathrm{cov}(x,y)= \frac{1}{n-1}\sum_{j=1}^n (x_j - \bar{x}) (y_j - \bar{y}) .
    (\#eq:Covariance)
\end{equation}
Covariance is one of the more complicated moments to explain. It is equal to zero if one of the variables does not have any variability and otherwise can take any real value. We will discuss its meaning later in Section \@ref(SLRCovariance). One thing that we can note at this stage is that it is symmetric, meaning that the covariance between $x$ and $y$ is the same as the covariance between $y$ and $x$. This becomes apparent from the formula \@ref(eq:Covariance), where switching $x_j$ with $y_j$ and $\bar{x}$ with $\bar{y}$ will give you exactly the same formula.

Furthermore, the same formula can also be rewritten the following way, showing its connection with the variance and the expectations of random variables:
\begin{equation}
    \mathrm{cov}(x,y)= \mathrm{E}(x \times y) - \mathrm{E}(x) \times \mathrm{E}(y).
    (\#eq:CovarianceAlt)
\end{equation}
Comparing the formula \@ref(eq:CovarianceAlt) with \@ref(eq:VarianceAlt), we can see that if $x=y$ then the covariance transforms into the variance of the variable. So, you can think of a variance as a measure of a joint variability of a variable with itself.

Finally, if we have several random variables, we can create an object called "matrix" that would have variances of variables and covariances between them. This is sometimes convenient to do for some further analysis or for presenting the results. This is called "covariance matrix" or "variance-covariance matrix". The idea is to group elements in a way that all variances lie on the diagonal, while the covariances would lie on the off-diagonals. In case of three variables ($x$, $y$ and $z$) it would look like this:
\begin{equation}
    \begin{pmatrix}
        \mathrm{V}(x) & \mathrm{cov}(x,y) & \mathrm{cov}(x,z) \\
        \mathrm{cov}(y,x) & \mathrm{V}(y) & \mathrm{cov}(y,z) \\
        \mathrm{cov}(z,x) & \mathrm{cov}(z,y) & \mathrm{V}(z)\\
    \end{pmatrix}
    (\#eq:CovarMat)
\end{equation}
Given the symmetry property of the covariance, the off-diagonal elements (such as $\mathrm{cov}(x,y)$ and $\mathrm{cov}(y,x)$) would be the same.

We will come back to the covariance matrix in the future topics.


## Graphical analysis {#dataAnalysisGraphical}
### One categorical/discrete variable
Continuing our example with `mtcars` dataset, we now investigate what plots can be used for different types of data. As discussed earlier, we have two categorical variables: vs and am - and they need to be treated differently than the numerical ones. We can start by producing their barplots:

```{r barplotVS, fig.cap="Barplot for the engine type."}
barplotVS <- barplot(table(mtcarsData$vs), xlab="Type of engine", col=17)
text(barplotVS,table(mtcarsData$vs)/2,table(mtcarsData$vs),cex=1.25)
```

This is just a graphical presentation of the contingency table we have already discussed earlier.

::: remark 
Histograms do not make sense in case of categorical variables, because they assume that variables are numerical and continuous (see Section \@ref(whatIsRandomVariable)) - they will split the values of a variable in the bins, based on the idea that the variable can take any of the values in each bin.
:::
Barplots are useful when you deal with either categorical variables or integer numerical ones. Here is what we can produce in case of the integer variable `cyl`:

```{r barplotCYL, fig.cap="Barplot for the number of cylinders."}
barplotCYL <- barplot(table(mtcarsData$cyl),
                      xlab="Number of cylinders", col=17)
text(barplotCYL, table(mtcarsData$cyl)/2, table(mtcarsData$cyl), cex=1.25)
```

Figure \@ref(fig:barplotCYL) shows that there are three types of cars in the data: with 4, 6 and 8 cylinders. The most frequently met is the car with 8 cylinders. Judging by the plot, half of cars have not more than 6 cylinders (median is equal to 6). All of this can be deducted from the barplot. And here how the histogram would look like for cylinders:

```{r histogramCYL, fig.cap="Histogram for the number of cylinders. Do not do this!"}
hist(mtcarsData$cyl, col=17)
```

Figure \@ref(fig:histogramCYL) is difficult to read, because on histogram, the bars show frequency at which continuous variable appears in pre-specified bins. In our case we would conclude that the most frequently cars in the dataset are those that have 7.5 - 8 cylinders, which is wrong and misleading. In addition, this basic plot does not have a readable label for x-axis and a meaningful title (in fact, we do not need one, given that we have caption). So, always label your axis and make sure that the text on plots is easy to understand for those people who do not work with the data.


### Two categorical/discrete variables
Coming back to categorical variables, we can construct two-dimensional plots to investigate potential relations between variables. We will first try the same barplot as above, but with `vs` and `am` variables:

```{r barplotVSAM, fig.cap="Barplot for the type of engine and transmission."}
barplot(table(mtcarsData$vs,mtcarsData$am),
        xlab="Type of transmission", legend.text=levels(mtcarsData$vs))
```

Figure \@ref(fig:barplotVSAM) provides some information about the distribution of type of engine and transmission. For example, we can say that the most often met car in the dataset is the one with automatic transmission and V-shaped engine. However, it is not possible to say much about the relation between the two variables based on this plot. So, there is an alternative presentation, which uses the heat map (`tableplot()` from `greybox`):

```{r tableplotVSAM, fig.cap="Heat map for the type of engine and transmission."}
tableplot(mtcarsData$vs,mtcarsData$am,
          xlab="Type of engine", ylab="Type of transmission")
```

The idea of this plot is that the darkness of areas shows the frequency of occurrence of each specific value. This message is duplicated by the number of dots in the plot (the more dots there are, the more observations there are in that specific area). The numbers inside the box show the proportions for each answer. So, we can conclude (again), that automatic transmission with V-shaped engine is met in 37.5% of cases. On the other hand, the least frequent type of car is the one with V-shaped engine and manual transmission. There might be some tendency in the dataset: the engine and transmission might be related (v-shaped with automatic vs Straigh with manual) - but it is not very well pronounced. The same table plot can be used for the analysis of relations between integer variables (and categorical). Here, for example, the plot between the number of cylinders and the type of engine:

```{r tableplotCYLVS, fig.cap="Heat map for the number of cylinders and the type of engine."}
tableplot(mtcarsData$cyl,mtcarsData$vs,
          xlab="Number of cylinders", ylab="Type of engine")
```

Figure \@ref(fig:tableplotCYLVS) allows making more solid conclusions about the relation between the two variables: we see that with the increase of the number of cylinders, the cars tend to switch from Straight to the V-shaped engines. This has an explanation: the engines with more cylinders need to have a different geometry to fit them all, and the V shape is more suitable for them. The table plot shows clearly this relation between the two variables.


### One numerical continuous variable
Next, we can analyse the numerical continuous variables. We can start with the basic histogram:

```{r histWeight, fig.cap="Distribution of the weights of cars."}
hist(mtcarsData$wt, xlab="Weight", main="", probability=TRUE,
     col=17)
lines(density(mtcarsData$wt), col=2, lwd=2)
```

The histogram \@ref(fig:histWeight) shows that there is a slight skewness in the data: the cars with weight from 3 to 4 thousands pounds are met more often than the cars with more than 5. The left tail of this distribution is slightly longer than the right one. Note that I have produced the probabilities on the y-axis of the plot in order to add the density curve, which smooths out the frequencies and shows how the distribution looks like.

An alternative presentation of the histogram is the boxplot, which graphically presents quantiles of distribution:

```{r boxWeight, fig.cap="Boxplot of the variable weight."}
boxplot(mtcarsData$wt, ylab="Weight")
points(mean(mtcarsData$wt),col=2, pch=16)
```

This plot has the box in the middle, the whiskers on the sides, points at the top and the red point at the centre. The box shows 1st, 2nd and 3rd quartiles of distribution, thus the black line in the middle is the median. The distance between the 1st and the 3rd quartiles is called "Interquartile range" (IQR) and is used for the calculation of the interval (1st / 3rd quartile $\pm 1.5 \times$IQR), which corresponds roughly to the 99.3% interval (read more about this in Section \@ref(confidenceIntervals)) from Normal distribution and is graphically drawn as the furthest observation in the interval. So, the lower whisker on our plot corresponds to the minimum value in the data, which is still in the interval, while the upper whisker corresponds to the bound of the interval. All the observations that lie beyond the interval are marked as potential outliers. Note that *this does not mean that the values are indeed outliers*, they just lie outside the 99.3% interval of Normal distribution. Finally, the red dot was added by me to show where the mean is. It is lower than median, this implies that there is a slight skewness in the distribution of weight.

There is also a way for producing the plots that would combine elements of histogram, density curve and boxplot. There is a plot called "violin plot". We will use `vioplot()` function from `vioplot` package in order to produce them:

```{r vioWeight, fig.cap="Violin plot together with boxplot of the variable weight."}
vioplot(mtcarsData$wt, ylab="Weight", col=17, rectCol=16)
points(mean(mtcarsData$wt),col=2, pch=16)
```

Figure \@ref(fig:vioWeight) unites the boxplot and the density curve from the plots above, providing not only information about the quantiles, but also about the shape of the distribution.

Finally, if we want to compare the distribution of a variable with a known theoretical distribution, we can produce the QQ-plot. Here how it looks for Normal distribution:

```{r QQWeight, fig.cap="QQ plot of Normal distribution for variable weight."}
qqnorm(mtcarsData$wt)
qqline(mtcarsData$wt)
```

The idea of the plot on Figure \@ref(fig:QQWeight) is to compare theoretical quantiles with the empirical ones. If the variable would follow the specific distribution, then all the points would lie on the solid line. In our case, they do not: there are points in the right tail that are very far from the line - so we would conclude that the distribution of weight does not look Normal.


### Two continuous numerical variables
So far, we have discussed the univariate analysis of numerical variables, but we can also produce plots showing potential relations between them. We start with the classical scatterplot:

```{r scatterWeightMPG, fig.cap="Scatterplot diagram between weight and mileage."}
plot(mtcarsData$wt, mtcarsData$mpg, xlab="Weight", ylab="Mileage")
lines(lowess(mtcarsData$wt, mtcarsData$mpg), col=2, lwd=2)
```

The plot on Figure \@ref(fig:scatterWeightMPG) shows the observations that have specific weight and mileage. Based on this, we can see if there is a relation between variables or not and what sort of relation this is. In order to simplify analysis, I have added the lowess line to the plot. It smooths the relation between variables, drawing the smooth line through the points and helps in understanding the existing relations in the data. Judging by Figure \@ref(fig:scatterWeightMPG), there is a negative, slightly non-linear relation between the variables: the mileage decreases with reduced speed, when weight of a car increases. This relation makes sense, because heavier cars will consume more fuel and thus drive less on a gallon of petrol.


### A mixture of variables
We could construct similar plots for all the other numerical variables, but not all plots would be helpful. For example, a plot of mileage versus number of forward gears would be very difficult to read (see Figure \@ref(fig:scatterGearMPG)).

```{r scatterGearMPG, fig.cap="Scatterplot diagram between weight and mileage."}
plot(mtcarsData$gear, mtcarsData$mpg, xlab="Number of gears", ylab="Mileage")
```

This is because one of the variables is integer and takes only a handful of values. In this case, a boxplot or a violin plot would be more useful:

```{r boxGearMPG, fig.cap="Boxplot of mileage vs number of gears."}
boxplot(mpg~gear, mtcarsData, xlab="Number of gears", ylab="Mileage")
points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=2, pch=16)
```

The plot on Figure \@ref(fig:boxGearMPG) is more informative than the one on Figure \@ref(fig:scatterGearMPG): it shows how the distribution of mileage changes with the increase of the numeric variable number of gears. We can also see that the mean value first increases and then goes down slightly. I do not have any good explanation of this phenomenon, but it might be related with how efficient the cars become with the increase fo the number of gears, or this could happen due to some latent, unobserved factors. So, the data tells us that there is a non-linear relation between number of gears and mileage.

Similarly, we can produce violin plots for the same data using the following code:

```{r vioGearMPG, fig.cap="Violin plot of mileage vs number of gears.", eval=FALSE}
vioplot(mpg~gear, mtcarsData, xlab="Number of gears", ylab="Mileage",
        col=17, rectCol=16)
points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=2, pch=16)
```

Finally, using exactly the same idea with boxplots / violin plots, we can analyse relations between categorical and numerical variables. Figure \@ref(fig:vioAMMPG) shows the relation between transmission type and mileage. We can conclude that the cars with manual transmission tend to have a higher mileage than the ones with the automatic one in our dataset.

```{r vioAMMPG, fig.cap="Violin plot of mileage vs transmission type."}
vioplot(mpg~am, mtcarsData, xlab="Transmission type", ylab="Mileage",
        col=17, rectCol=16)
points(tapply(mtcarsData$mpg, mtcarsData$am, mean), col=2, pch=16)
```


### Plot for several variables
Finally, producing plots one by one might be a tedious and challenging task, so it is good to have some instruments for producing several of them together. The `plot()` method will produce scatterplot matrix for numerical variables, but does not deal well with integer and categorical variables:

```{r scatterMatrix, fig.cap="Scatterplot matrix for the mtcars dataset."}
plot(mtcars)
```

Figure \@ref(fig:scatterMatrix) is informative for the variables `mpg`, `cyl`, `disp`, `hp`, `drat`, `qsec` and `carb`, but is difficult to read for the others. In order to address this issue, we can use the `spread()` function from `greybox`, which will detect types of variables and produce the necessary plots automatically:

```{r spreadPlot, fig.cap="Spread plot for the mtcars dataset."}
# Amend the palette. This is just to show how to use it
# This palette is in red colours
palette(c(rgb(0.5,0.2,0.2,0.5), rgb(0.5,0.2,0.2,1), rgb(0.9,0.6,0.6,0.5)))
# Call the function
spread(mtcarsData, lowess=TRUE)
```

```{r echo=FALSE}
palette(ourPalette)
```

The plot on Figure \@ref(fig:spreadPlot) is the collection of the plots discussed above, so I will not stop on explaining what it shows.

As a final word for this section, when analysing data, it is critically important not to just describe what we see, but also explain why a result or a relationship is meaningful, otherwise this becomes an exercise of stating the obvious which does not have any value. So, for example, concluding based on Figure \@ref(fig:spreadPlot) that the mileage has a negative relation with displacement is not enough. If you want to analyse the data properly, you need to explain that this relation is meaningful, because with the increase of the size of engine, the fuel consumption will increase as well, and as a result the mileage will go down. Furthermore, the relation is non-linear because the change in decrease will slow down with cars with bigger engines. Inevitably, the car with a gigantic engine will be able to travel a short distance on a gallon of fuel - the mileage will not become negative, so the non-linearity is not an artefact of the data, but an existing phenomenon.
