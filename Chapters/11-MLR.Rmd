# Multiple Linear Regression {#linearRegression}
::: example
One of the problems that construction companies face is getting a good estimate of the budget needed to build something. Many companies tend to underestimate the costs and time the project will take. To address this, a mid-size company called "Eden city", which specialises on construction of residential buildings, decided to take a more analytical approach to the problem. They have collected the data of their previous projects and needs help in building a model that would explain what forms the costs for different types of buildings. Their idea is to use this model during the business plan write-up phase to get an estimate of the future project, which they hope will be better than the ones they used before based on their pure judgment.
:::

In this example, we are interested in the overall costs of construction (in thousands of pounds), which can be impacted by:

- The size of a building in squared meters,
- The cost of materials (in thousands of pounds),
- Type of building (detached, semi-detached, bungalow etc),
- How many projects the specific crew did before,
- Year when the project was started.

::: question
What else do you think can impact such costs in theory?
:::

```{r echo=FALSE}
load("data/SBA_Chapter_11_Costs.Rdata")
```

This data is available online:

```{r eval=FALSE}
load(url("https://github.com/config-i1/sba/raw/refs/heads/master/data/SBA_Chapter_11_Costs.Rdata"))
```

Based on what we have discussed before, we can do analysis of measures of association and even build simple regression model (or several of them), but we acknowledge that in many real life situations, there are many factors that impact the variable of interest. In the example above, we have listed five explanatory variables that can be connected to the overall costs. This means that a basic bi-variate analysis (one variable vs the other) might not be sufficient. Furthermore, the relations between variables are typically complicated. So analysing, for example, only the relation between the cost of project and the size of building without considering the cost of materials might be misleading.

Here is how the bi-variate relations between the variables in our dataset look like (Figure \@ref(fig:CostsSpread)):

```{r CostsSpread, fig.cap="Spread plot between variables in the building costs dataset."}
spread(SBA_Chapter_11_Costs)
```

While the plot in Figure \@ref(fig:CostsSpread) gives a good idea about the relations, for example, between the size of property and the overall costs (it seems to be linear positive) or between material and the overall costs (linear positive again), it does not tell us much about the complex relation between one variable (overall costs) and several others.

All of this gives a motivation to having a so called "Multiple Linear Regression", the model that expresses the relation between one variable and several of others. Mathematically, this is a straight forward extension of the simple linear regression model from Chapter \@ref(simpleLinearRegression), where we just add variables to the right-hand side of the equation. For example, if we had two variables impacting one (e.g. $size$ of project and cost of $materials$ vs $overall$ cost of the project), we could write:
\begin{equation}
    overall_j = \beta_0 + \beta_1 size_{j} + \beta_2 material_{j} + \epsilon_j ,
    (\#eq:MLRFormulaExample)
\end{equation}
where $beta_0$ is the intercept, and $beta_1$, $beta_2$ are the coefficients for the respective variables. The predicted overall costs can be calculated based on this model by dropping the error term $\epsilon_j$:
\begin{equation*}
    \widehat{overall}_j = \beta_0 + \beta_1 size_{j} + \beta_2 material_{j}.
\end{equation*}
While in the example with the Simple Linear Regression the predicted (or fitted) values implied drawing a line through the cloud of dots on the plane of the two variables, now we are talking about drawing a plane through the point in the three-dimensional space. It can be visualised in the following way (Figure \@ref(fig:scatterplot3dProjectCosts)):

```{r scatterplot3dProjectCosts, fig.cap="3D scatterplot of Overall costs vs size of project and costs of materials.", echo=FALSE}
fit <- lm(overall ~ materials+size, data=SBA_Chapter_11_Costs)
s3d <- scatterplot3d::scatterplot3d(SBA_Chapter_11_Costs$materials, SBA_Chapter_11_Costs$size,
                                    SBA_Chapter_11_Costs$overall,
                                    pch=16, highlight.3d=TRUE,
                                    type="h", xlab="Materials costs", ylab="Size of the building",
                                    zlab="Overall costs", main="")
s3d$plane3d(fit, lty.box="solid", col=4, draw_polygon=TRUE,
            polygon_args=list(col=10))
```

The 3d image in Figure \@ref(fig:scatterplot3dProjectCosts) is already hard to analyse, but at least it gives an idea of how the overall costs change with the change of materials costs and size of buildings. However, it would be impossible to produce a meaningful plot of overall costs from more than two variables. What the figure above gives us is the connection between the simple linear regression (which is just a straight line in the two-dimensional plane) and the multiple one (which is a plane in a multi-dimensional space).

In a more general way, the multiple linear regression can be written as:
\begin{equation}
    y_j = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} + \epsilon_j ,
    (\#eq:MLRFormula)
\end{equation}
where $\beta_i$ is a $i$-th parameter for the respective $i$-th explanatory variable and there is $k-1$ of them in the model, meaning that when we want to estimate this model, we will have $k$ unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is:
\begin{equation}
    \mu_{y,j} = \mathrm{E}(y_j | \mathbf{x}_j) = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} .
    (\#eq:MLRExpectation)
\end{equation}
Furthermore, similar to how we discussed it in Chapter \@ref(simpleLinearRegression), when we want to estimate model \@ref(eq:MLRFormula), we should substitute all parameters $\beta_j$ with their estimates $b_j$:
\begin{equation}
    \hat{y}_j = b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} .
    (\#eq:MLRExpectationSample)
\end{equation}
Similar to the Simple Linear Regression, each parameter in equation \@ref(eq:MLRExpectationSample) represents the slope for the respective variable, showing how on average the value of the response variable (overall costs in our example) changes with the change of each variable.

### A couple of notes about the "true" model {#TrueModel}
One thing we should discuss briefly is the term "true" model, which was mentioned in Subsection \@ref(modelsMethods). The whole idea of the true model implies that this is a model that has the following important properties:

1. It is applied to the population of data (i.e. all the data in the universe);
2. It has all the important variables in it, i.e. there are no omitted variables that could impact the response variable;
3. All included important variables are in the correct format. i.e., all necessary transformations are done;
4. Because of (1) - (3), the error term of the model does not have any structure left, and represents small random fluctuations around the structure that cannot be captured by any other variables;
5. Because of (4), the error term has a zero expectation;
6. As a result of (4), the error term cannot be correlated with any of the variables, either included in the model or omitted by it. Having this the other way around would imply that there is some structure left in the error, which should be transferred away from it;
7. Similarly, the error term cannot be related to its values on other observations. This issue can potentially appear on time series, where an error of an applied model on Monday can be related to the error on Tuesday. But when we talk about the true model, the errors cannot be related, otherwise this would imply that we missed some structure, thus violating (1) - (3);
8. In the simple case of a linear model that we are discussing in this Chapter, the error term also has some fixed variance;
9. The parameters of the model should also be independent of the error term for the same reason (4).

In the simple case of a linear true model, it can be represented as:
\begin{equation}
    y_j = \mu_{y,j} + \epsilon_j .
    (\#eq:TrueModel)
\end{equation}
where $\mu_j$ is the structure that we can capture in theory with the correct set of explanatory variables, and $\epsilon_j$ is the true error term that mathematically has $\mathrm{E}(\epsilon_j)=0$ due to (5) and $\mathrm{V}(\epsilon_j)=\sigma^2$ due to (8). The property (6) can be written mathematically as $\mathrm{E}(x_{i,j}\epsilon_j)=0$ for any $i$ and $j$. This model can be called "linear" or "additive", because the structure and the error interact are added in one linear equation. But the true model can be more complicated and might contain non-linear structure or some complicated interactions between variables and the error term.

For now, we focus on the model \@ref(eq:TrueModel) and we assume that when we estimate a model, we know its form and have all the necessary variables. We will discuss what happens when this is violated in Chapter \@ref(assumptions).


## OLS estimation {#OLSMLR}
We have already discussed the idea of the OLS in Section \@ref(OLS) on the example of the Simple Linear Regression. The logic with the multiple one is exactly the same: we want to minimise the sum of squared errors by changing the values of parameters. The main difference now is that we can have more than two parameters and as a result, the formulae become more complicated.


### Application
Luckily, you do not need to know any formula to calculate the estimates of parameters and should not need to use it in real life, because it is used in all statistical software, including `lm()` function from `stats` package for R. If you want to know the maths behind this, it is explained in Subsection \@ref(OLSMLRMaths).

Here is an example of the model estimation via OLS with the same dataset:

```{r}
costsModel01 <- lm(overall~size+materials+projects+year, SBA_Chapter_11_Costs)
```

After running the above command, we will get a model with some estimates of parameters:

```{r}
coef(costsModel01)
```

To better understand what fitting such model to the data implies, we can produce a plot of fitted vs actuals values, with $\hat{y}_j$ on x-axis and $y_j$ on the y-axis:

```{r}
plot(fitted(costsModel01),actuals(costsModel01))
```

The same plot is produced via `plot()` method if we use `alm()` function from `greybox` instead:

```{r costsModel02Plot, fig.cap="Actuals vs fitted values for multiple linear regression model on mtcars data."}
costsModel02 <- alm(overall~size+materials+projects+year,
                    SBA_Chapter_11_Costs, loss="MSE")
plot(costsModel02,1)
```

We use `loss="MSE"` in this case, to make sure that the model is estimated via OLS. We will discuss the default estimation method in `alm()`, likelihood, in Section \@ref(likelihoodApproach).

The plot on Figure \@ref(fig:costsModel02Plot) can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Chapter \@ref(assumptions)). 


### A bit of maths {#OLSMLRMaths}
There are several ways how we can get the formula for the OLS of the Multiple Linear Regression. We could start with the same SSE value, expanding it based on the equation of the regression \@ref(eq:MLRExpectationSample):
\begin{equation*}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - \hat{y}_j)^2 = \sum_{j=1}^n (y_j - b_0 - b_1 x_{1,j} - b_2 x_{2,j} - \dots - b_{k-1} x_{k-1,j})^2 .
\end{equation*}
Taking derivatives of SSE with respect to $b_0$, $b_1$, $b_2$, $\dots$, $b_{k-1}$ and then equating them to zero, we would get a so-called **System of Normal Equations** (we have discussed it when providing a proof in Section \@ref(OLS)), which in general case has form: 
\begin{equation*}
    \begin{aligned}
        & \sum_{j=1}^n y_j - n b_0 - b_1 \sum_{j=1}^n x_{1,j} - b_2 \sum_{j=1}^n x_{2,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} = 0 \\
        & \sum_{j=1}^n y_j x_{1,j} - b_0 \sum_{j=1}^n x_{1,j} - b_1 \sum_{j=1}^n x^2_{1,j} - b_2 \sum_{j=1}^n x_{2,j} x_{1,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} x_{1,j} = 0 \\
        & \sum_{j=1}^n y_j x_{2,j} - b_0 \sum_{j=1}^n x_{2,j} - b_1 \sum_{j=1}^n x_{1,j} x_{2,j} - b_2 \sum_{j=1}^n x^2_{2,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} x_{2,j} = 0 \\
        \vdots
        & \sum_{j=1}^n y_j x_{k-1,j} - b_0 \sum_{j=1}^n x_{k-1,j} - b_1 \sum_{j=1}^n x_{1,j} x_{k-1,j} - b_2 \sum_{j=1}^n x_{2,j} x_{k-1,j} - \dots - b_{k-1} \sum_{j=1}^n x^2_{k-1,j} = 0
    \end{aligned}
\end{equation*}
Solving this system of equations gives us formulae for parameters $b_0$, $b_1$, $b_2$, $\dots$, $b_{k-1}$, that guarantee that the SSE is minimal.

However, there is a more compact and easier in logic way of getting the formulae, but it requires some basic knowledge of linear algebra. To explain it, we need to present the multiple linear regression in a more compact form. In order to do that we will introduce the following vectors:
\begin{equation}
    \mathbf{x}'_j = \begin{pmatrix}1 & x_{1,j} & \dots & x_{k-1,j} \end{pmatrix},
    \boldsymbol{\beta} = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{k-1} \end{pmatrix} ,
    (\#eq:MLRVectors)
\end{equation}
where $'$ symbol is the transposition. This can then be substituted in \@ref(eq:MLRFormula) to get:
\begin{equation}
    y_j = \mathbf{x}'_j \boldsymbol{\beta} + \epsilon_j .
    (\#eq:MLRFormulaCompacter)
\end{equation}
This form is just convenient, but it denotes exactly the same model as in equation \@ref(eq:MLRFormula). All we need to remember in case of the equation \@ref(eq:MLRFormulaCompacter) is that it represents the sum of products of variables by their coefficients, just in a compact way.

But this is not over yet, we can make it even more compact, if we pack all those values with index $j$ in vectors and matrices:
\begin{equation}
    \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_n \end{pmatrix} = 
    \begin{pmatrix} 1 & x_{1,1} & \dots & x_{k-1,1} \\ 1 & x_{1,2} & \dots & x_{k-1,2} \\ \vdots \\ 1 & x_{1,n} & \dots & x_{k-1,n} \end{pmatrix}, 
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}, 
    \boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix} ,
    (\#eq:MLRMatrices)
\end{equation}
where $n$ is the sample size. This leads to the following even more compact form of the multiple linear regression:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} .
    (\#eq:MLRFormulaCompactest)
\end{equation}
If you compare \@ref(eq:MLRFormulaCompactest) with the original one \@ref(eq:MLRFormula):
\begin{equation*}
    y_j = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} + \epsilon_j ,
\end{equation*}
you will probably see the connection. But the form \@ref(eq:MLRFormulaCompactest) is just more abstract. This abstraction, however, allows us getting an analytical formula for the calculation of the estimates of parameters of the model (remember, we substitute the true values $\beta_j$ by their sample estimates $b_j$):
\begin{equation}
    \mathbf{b} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLS)
\end{equation}

One way to get a better understanding of this formula is to connect it with the one for the slope parameter in the Simple Linear Regression, which we discussed in Section \@ref(OLS):
\begin{equation*}
        {b}_1 = \frac{\mathrm{cov}(x,y)}{\mathrm{V}(x)} .
\end{equation*}
In case of only one explanatory variable (simple linear regression), $\mathbf{X}' \mathbf{y}$ in \@ref(eq:MLROLS) would be equivalent to $\mathrm{cov}(x,y)$, while $\left(\mathbf{X}' \mathbf{X}\right)^{-1}$ would be $\frac{1}{\mathrm{V}(x)}$.


::: warning
Linear algebra! Tread lightly!
:::

::: proof
Sum of squared errors based on the model \@ref(eq:MLRFormulaCompactest) applied to the data can be expressed as:
\begin{equation*}
    \mathrm{SSE} = \mathbf{e}' \mathbf{e} = (\mathbf{y} - \mathbf{X} \mathbf{b})' (\mathbf{y} - \mathbf{X} \mathbf{b}) ,
\end{equation*}
where $\mathbf{e}$ is the estimate of $\boldsymbol{\epsilon}$ and $\mathbf{b}$ is the estimate of $\boldsymbol{\beta}$. This can be expanded by opening brackets to:
\begin{equation}
    \begin{aligned}
        \mathrm{SSE} = & \mathbf{y}'\mathbf{y} - \mathbf{y}' \mathbf{X} \mathbf{b} - \mathbf{b}' \mathbf{X}' \mathbf{y} + \mathbf{b}' \mathbf{X}' \mathbf{X} \mathbf{b} = \\
        & \mathbf{y}'\mathbf{y} - 2 \mathbf{b}' \mathbf{X}' \mathbf{y} + \mathbf{b}' \mathbf{X}' \mathbf{X} \mathbf{b}
    \end{aligned} ,
    (\#eq:MLROLS02)
\end{equation}
which can be done because $\mathbf{y}' \mathbf{X} \mathbf{b}$ is a scalar and $\mathbf{y}' \mathbf{X} \mathbf{b} = (\mathbf{y}' \mathbf{X} \mathbf{b})' = \mathbf{b}' \mathbf{X}' \mathbf{y}$. Now we need to minimise \@ref(eq:MLROLS02) with respect to parameters to find their estimates. This can be done by taking derivative of \@ref(eq:MLROLS02) with respect to $\mathbf{b}$ and equating it to zero:
\begin{equation}
    \frac{\partial \mathrm{SSE}}{\partial \mathbf{b}} = - 2 \mathbf{X}' \mathbf{y} + 2 \mathbf{X}' \mathbf{X} \mathbf{b} = 0.
    (\#eq:MLROLS03)
\end{equation}
After that, we can regroup the elements in \@ref(eq:MLROLS03) to get:
\begin{equation*}
    \mathbf{X}' \mathbf{X} \mathbf{b} = \mathbf{X}' \mathbf{y}.
\end{equation*}
And then, we can multiply both parts of the equation by the inverse of $\mathbf{X}' \mathbf{X}$ to get rid of that part in the left-hand side of the equation:
\begin{equation*}
    (\mathbf{X}' \mathbf{X})^{-1} \mathbf{X}' \mathbf{X} \mathbf{b} = (\mathbf{X}' \mathbf{X})^{-1} \mathbf{X}' \mathbf{y},
\end{equation*}
which then leads to the final formula:
\begin{equation*}
    \mathbf{b} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
\end{equation*}
:::

### Properties of the OLS Estimators
There are several important properties of the OLS estimated regression that are worth keeping in mind (we discussed some of them specifically for the Simple Linear Regression in Section \@ref(OLSResiduals)):

1. The mean of residuals of the model is always equal to zero as long as it contains intercept.
2. The explanatory variables in the model are not correlated with the residuals of the model.
3. The mean of the fitted values coincides with the mean of the actual values.

They all follow directly from the derivation of the OLS formula. 

::: warning
Linear algebra! Tread lightly!
:::

::: proof
Consider the system of normal equations
\begin{equation}
    \left(\mathbf{X}' \mathbf{X}\right) \mathbf{b} = \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLSProp01)
\end{equation}
Given that we estimated the model in sample, we can rewrite the multiple linear regression as:
\begin{equation*}
    \mathbf{y} = \mathbf{X} \boldsymbol{b} + \boldsymbol{e} 
\end{equation*}
and substitute it in \@ref(eq:MLROLSProp01) to get:
\begin{equation}
    \left(\mathbf{X}' \mathbf{X}\right) \mathbf{b} = \mathbf{X}' \mathbf{X} \boldsymbol{b} + \mathbf{X}' \boldsymbol{e} .
    (\#eq:MLROLSProp02)
\end{equation}
Equation \@ref(eq:MLROLSProp02) implies that $\mathbf{X}' \boldsymbol{e}=0$, which given how the matrix $\mathbf{X}$ is formed prooves the first two properties:

1. The first column of the matrix (which contains ones) corresponds to the intercept and the multiplication of it by the error implies that $\sum_{j=1}^n e_j =0$, which also means that the mean of the residuals is zero as well: $\bar{e}=0$;
2. All the other columns contain the explanatory variables and for each one of them this comes to the equation: $\sum_{j=1}^n x_{i,j} e_j$ for all $i=\{1, \dots, k-1\}$. Given that the first property holds, the same equation can be rewritten to:
\begin{equation*}
    \begin{aligned}
    & \sum_{j=1}^n x_{i,j} (e_j - \bar{e}) = \sum_{j=1}^n (x_{i,j} - \bar{x}_i) (e_j - \bar{e}) =
    & \sum_{j=1}^n (x_{i,j} - \bar{x}_i) (e_j - \bar{e}) + \bar{x}_i \sum_{j=1}^n (e_j - \bar{e})
    \end{aligned}
\end{equation*}
The right hand side of this equation equals to zero due to the first property, which implies that:
\begin{equation}
    \sum_{j=1}^n x_{i,j} e_j = \sum_{j=1}^n (x_{i,j} - \bar{x}_i) (e_j - \bar{e}),
    (\#eq:MLROLSProp03)
\end{equation}
which is the covariance between the residuals and the explanatory variable $x_i$. And because the equation \@ref(eq:MLROLSProp02) implies that $\sum_{j=1}^n x_{i,j} e_j=0$, the covariance \@ref(eq:MLROLSProp03) should be equal to zero as well.

Finally, it is easy to show the third property. All we need to do is to take the mean of the response variable:
\begin{equation*}
    \mathrm{E}(\mathbf{y}) = \mathrm{E}(\mathbf{X} \boldsymbol{b} + \boldsymbol{e}) = \mathrm{E}(\mathbf{X} \boldsymbol{b}) + \mathrm{E}(\boldsymbol{e}) ,
\end{equation*}
where the expectation of the error term equals to zero, implying that:
\begin{equation*}
    \mathrm{E}(\mathbf{y}) = \mathrm{E}(\mathbf{X} \boldsymbol{b}) = \mathrm{E}(\hat{\mathbf{y}}) .
\end{equation*}
:::

These three properties are useful because, first, they show that it does not make sense to test whether they hold or not, with OLS they will be satisfied automatically, and second they allow measuring quality of the fit via the squares of regression, which will be discussed in Section \@ref(linearRegressionMultipleQualityOfFit).


### Gauss-Markov theorem {#GaussMarkov}
OLS is a very popular estimation method for linear regression for a variety of reasons. First, it is relatively simple (much simpler than other approaches) and conceptually easy to understand. Second, the estimates of OLS parameters can be found analytically (using formula \@ref(eq:OLSSLREstimates)). Furthermore, there is a mathematical proof that the OLS estimates of parameters are efficient (Subsection \@ref(estimatesPropertiesEfficiency)), consistent (Subsection \@ref(estimatesPropertiesConsistency)) and unbiased (Subsection \@ref(estimatesPropertiesBias)). The theorem that states that is called "Gauss-Markov theorem", here is one of versions of it:

::: theorem
If regression model is correctly specified then OLS will produce Best Linear Unbiased Estimates (BLUE) of parameters.
:::

The term "correctly specified" implies that all main statistical assumptions about the model are satisfied (such as no omitted important variables, no autocorrelation and heteroscedasticity in the residuals, see details in Chapter \@ref(assumptions)). The "BLUE" part means that OLS guarantees the most efficient and the least biased estimates of parameters amongst all possible estimators of a linear model. For example, if we used a criterion of minimisation of Mean Absolute Error (MAE), then the estimates of parameters would be less efficient than in case of OLS. This is because OLS gives "mean" estimates, while the minimum of MAE corresponds to the median (see Subsection \@ref(estimatesPropertiesEfficiency)).

Practically speaking, the theorem implies that when you use OLS, the estimates of parameters will have good statistical properties (given that the model is correctly specified), in some cases better than the estimates obtained using other methods.


## Quality of a fit {#linearRegressionMultipleQualityOfFit}
Building upon the discussion of the quality of the fit in Section \@ref(linearRegressionSimpleQualityOfFit), we can introduce a measure, based on the OLS criterion, \@ref(eq:OLSCriterion), which is called either "Root Mean Squared Error" (RMSE) or a "standard error" or a "standard deviation of error" of the regression:
\begin{equation}
    \hat{\sigma}^2 = \sqrt{\frac{1}{n-k} \sum_{j=1}^n e_j^2 }.
    (\#eq:RMSERegression)
\end{equation}
This is an unbiased estimate of the variance of the error term $\sigma^2$. The denominator of \@ref(eq:RMSERegression) contains the number of degrees of freedom in the model, $n-k$, not the number of observations $n$, so technically speaking this is not a "mean" any more. This is done to correct the in-sample bias (Section \@ref(estimatesPropertiesBias)) of the measure. Standard error does not tell us much about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower it is, the better the model fits the data, given the number of estimated parameters. However, this measure is not aware of the randomness in the true model (Section \@ref(modelsMethods)) and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one.

Here is how this can be calculated for our model, estimated using `alm()` function:
```{r}
sigma(costsModel02)
```
The value of RMSE does not provide any important insights on its own, but it can be compared to the RMSE of another model to decide, which one of the two fits the data better.

Similarly to the simple linear regression, we can calculate the R$^2$ (see Section \@ref(linearRegressionSimpleQualityOfFit)). The problem is that the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will deteriorate. Here is an example:
```{r}
# Record number of observations
n <- nobs(costsModel02)
set.seed(13)
# Generate white noise
SBA_Chapter_11_Costs$noise <- rnorm(n,0,10)
# Add it to the model
costsModel02WithNoise <- alm(overall~size+materials+projects+year+noise,
                             SBA_Chapter_11_Costs, loss="MSE")
```
The code above introduces a new variable, `noise`, which has nothing to do with the `overall` costs. We would expect that this variable would not bring value to the model. And here is the value of determination coefficient of the new model:
```{r}
1 - sum(resid(costsModel02WithNoise)^2) /
    (var(actuals(costsModel02WithNoise))*(n-1))
```
Compare it with the previous one:
```{r}
1 - sum(resid(costsModel02)^2) /
    (var(actuals(costsModel02))*(n-1))
```
The value in the new model will always be higher than in the previous one (or equal to it in some very special cases), no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of estimated parameters is required to make the measure more reasonable. This is what adjusted coefficient of determination does:
\begin{equation}
    R^2_{adj} = 1 - \frac{\hat{\sigma}^2}{\mathrm{V}(y)} = 1 - \frac{(n-1)\mathrm{SSE}}{(n-k)\mathrm{SST}} .
    (\#eq:DeterminationAdjusted)
\end{equation}
So, instead of dividing sums of squares, in the adjusted R$^2$ we divide the entities that are based on degrees of freedom. Given the presence of $k$ in the formula \@ref(eq:DeterminationAdjusted), the coefficient will not necessarily increase with the addition of variables -- when the variable does not contribute in the reduction of SSE of model substantially, R$^2$ will not go up. Furthermore, if one model has higher $\hat{\sigma}^2$ than the other one, then the R$^2_{adj}$ of that model will be lower, which becomes apparent, given that we have $-\hat{\sigma}^2$ in the formula \@ref(eq:DeterminationAdjusted).

Here how the adjusted R$^2$ can be calculated for a model in R:
```{r}
setNames(c(1 - sigma(costsModel02)^2 / var(actuals(costsModel02)),
           1 - sigma(costsModel02WithNoise)^2 / var(actuals(costsModel02WithNoise))),
         c("R^2-adj","R^2-adj, Noise"))
```
What we will typically see in the output above is that the model with the noise will have a lower value of adjusted R$^2$ than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter just due to pure chance. So, you should not fully trust R$^2_{adj}$ either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inefficient estimates of parameters (see Section \@ref(assumptions) for details) and inaccurate forecasts.

### Common mistakes related to quality of a fit
There are several common mistakes that arise when people measure quality of a fit of regression. We have seen these mistakes done by students, but they also appear on social media and sometimes even in scientific papers. Here they are:

1. "Model is good because R$^2$/Adjusted R$^2$ is 0.9876/greater than some arbitrary threshold"
- Neither R$^2$, nor Adjusted R$^2$ tells anything about the quality of the model. They only tell us how well it fits the data. In case of the former, it shows the "percentage of the explained variance in the response variable", but they both do not know that any model has an irreducible error, and thus a high value of R$^2$ does not mean that the model is good. R$^2$ should not be used for model selection and provides little to no useful diagnostic information. R$^2$ adjusted can be used for model comparison, but on its own does not provide useful information either.

2. "Although R$^2$/Adjusted R$^2$ is very low (e.g. 0.05), the model is statistically significant and thus makes sense"
- This argument relates to what we will discuss in Section \@ref(uncertaintyRegressionLine), but we can say now that the low value of the coefficient of determination indicates that the model does not differ much from the straight line (global mean). To that extent, its low value could be alarming.

In general, we think that both R$^2$ close to one and close to zero are alarming, the former implies that the model might overfit the data, while the latter means that it underfits it. But there is no proper threshold value, with which you should compare your coefficient of determination.

3. "Model A is better than model B because its R$^2$ value is higher"
- As discussed in this section, R$^2$ will increase with the increase of the number of parameters, so this statement is in general wrong. It only works if you have two models with exactly the same number of parameters. But usually, we see such statements in the context of variable selection, when models have different number of parameters. If you want to use basic statistics for model selection then at least use RMSE or the adjusted R$^2$. But there is a better way, which we will discuss in Section \@ref(informationCriteria).


## Interpretation of parameters
Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model:
```{r}
coef(costsModel02)
```

Each of the parameters of this model shows an **average** effect of each variable on the overall costs. They have a simple interpretation and show how the response variable will change **on average** with the increase of a variable by 1 unit, keeping all the other variables constant.

For example, the parameter for `size` shows that with the increase of size of the building by on squared meter, the overall cost tends to increase **on average** by `r round(abs(coef(costsModel02)["size"]), digits)` thousand pounds, if all the other variables do not change.

We have made the word "average" boldface three times in this section for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. Any regression model captures mean tendencies and thus the word "average" is very important in the interpretation. In each specific case, the increase of size by 1 squared meter will lead to different increases (and even decreases in some cases) of the overall costs. But if we take the arithmetic mean of those individual effects, it should be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section \@ref(assumptions)).

Finally, it is worth discussing what the interpretation of the intercept in the model is. If we set all the explanatory variables to zero, the overall costs will be equal to the value of the intercept. In our example, where we fitted the basic linear model, the interpretation is meaningless: there is no such thing as a house with no costs, size of zero, and it definitely cannot have negative overall costs. In this case, intercept plays purely technical role, showing where the regression line intersects the y-axis. However, if we were to build a different model, the value might have a meaning. Still, we personally prefer avoiding interpreting the intercept.
