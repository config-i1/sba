# Multiple Linear Regression {#linearRegression}
::: example
One of the problems that construction companies face is getting a good estimate of the budget needed to build something. Many companies tend to underestimate the costs and time the project will take. To address this, a mid-size company called "Eden city" decided to take a more analytical approach to the problem. They have collected the data of their previous projects and need help in building a model that would explain what forms the costs for different types of buildings. Their idea is to use this model during the business plan write-up phase to get an estimate of the future project, which they hope will be better than the ones they used before based on their pure judgment.
:::

In this example, we are interested in the overall costs of construction, which can be impacted by:

- The size of the project,
- The cost of materials,
- The experience of the crew delivering the project.

::: question
What else do you think impacts such costs in theory?
:::




While simple linear regression provides a basic understanding of the idea of capturing the relations between variables, it is obvious that in reality there are more than one external variable that would impact the response variable. This means that instead of \@ref(eq:SLRFormula) we should have:
\begin{equation}
    y_j = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} + \epsilon_j ,
    (\#eq:MLRFormula)
\end{equation}
where $\beta_i$ is a $i$-th parameter for the respective $i$-th explanatory variable and there is $k-1$ of them in the model, meaning that when we want to estimate this model, we will have $k$ unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is:
\begin{equation}
    \mu_{y,j} = \mathrm{E}(y_j | \mathbf{x}_j) = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} ,
    (\#eq:MLRExpectation)
\end{equation}
while in case of a sample estimation of the model we will use:
\begin{equation}
    \hat{y}_j = b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} .
    (\#eq:MLRExpectationSample)
\end{equation}
While the simple linear regression can be represented as a line on the plane with an explanatory variable and a response variable, the multiple linear regression cannot be easily represented in the same way. In case of two explanatory variables the plot becomes three dimensional and the regression line transforms into regression plane.

```{r scatterplot3dmtcars, fig.cap="3D scatterplot of Mileage vs Weight of a car and its Engine Horsepower.", echo=FALSE}
fit <- lm(mpg ~ wt+hp, data=mtcars)
s3d <- scatterplot3d::scatterplot3d(mtcars$wt, mtcars$hp, mtcars$mpg, pch=16, highlight.3d=TRUE,
                    type="h", xlab="Weight", ylab="Horsepower", zlab="Mileage", main="")
s3d$plane3d(fit, lty.box="solid", col="darkblue", draw_polygon=TRUE, polygon_args=list(col=rgb(0,0,0.8,0.2)))
```

Figure \@ref(fig:scatterplot3dmtcars) demonstrates a three dimensional scatterplot with the regression plane, going through the points, similar to how the regression line went through the two dimensional scatterplot \@ref(fig:scatterWeightMPG2). These sorts of plots are already difficult to read, but the situation becomes even more challenging, when more than two explanatory variables are under consideration: plotting 4D, 5D etc is not a trivial task. Still, what can be said about the parameters of the model even if we cannot plot it in the same way, is that they represent slopes for each variable, in a similar manner as $\beta_1$ did in the [simple linear regression](#simpleLinearRegression).


## OLS estimation
In order to show how the estimation of multiple linear regression is done, we need to present it in a more compact form. In order to do that we will introduce the following vectors:
\begin{equation}
    \mathbf{x}'_j = \begin{pmatrix}1 & x_{1,j} & \dots & x_{k-1,j} \end{pmatrix},
    \boldsymbol{\beta} = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{k-1} \end{pmatrix} ,
    (\#eq:MLRVectors)
\end{equation}
where $'$ symbol is the transposition. This can then be substituted in \@ref(eq:MLRFormula) to get:
\begin{equation}
    y_j = \mathbf{x}'_j \boldsymbol{\beta} + \epsilon_j .
    (\#eq:MLRFormulaCompacter)
\end{equation}
But this is not over yet, we can make it even more compact, if we pack all those values with index $t$ in vectors and matrices:
\begin{equation}
    \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_n \end{pmatrix} = 
    \begin{pmatrix} 1 & x_{1,1} & \dots & x_{k-1,1} \\ 1 & x_{1,2} & \dots & x_{k-1,2} \\ \vdots \\ 1 & x_{1,n} & \dots & x_{k-1,n} \end{pmatrix}, 
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}, 
    \boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix} ,
    (\#eq:MLRMatrices)
\end{equation}
where $T$ is the sample size. This leads to the following compact form of multiple linear regression:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} .
    (\#eq:MLRFormulaCompactest)
\end{equation}
Now that we have this compact form of multiple linear regression, we can estimate it using linear algebra. Many statistical textbooks explain how the following result is obtained (this involves taking derivative of SSE \@ref(eq:OLSCriterion) with respect to $\boldsymbol{\beta}$ and equating it to zero):
\begin{equation}
    \hat{\boldsymbol{\beta}} = \mathbf{b} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLS)
\end{equation}
The formula \@ref(eq:MLROLS) is used in all the statistical software, including `lm()` function from `stats` package for R. Here is an example with the same `mtcars` dataset:

```{r}
mtcarsModel01 <- lm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars)
```

The simplest plot that we can produce from this model is fitted values vs actuals, plotting $\hat{y}_j$ on x-axis and $y_j$ on the y-axis:

```{r}
plot(fitted(mtcarsModel01),actuals(mtcarsModel01))
```

The same plot is produced via `plot()` method if we use `alm()` function from `greybox` instead:

```{r mtcarsModel02Plot, fig.cap="Actuals vs fitted values for multiple linear regression model on mtcars data."}
mtcarsModel02 <- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars, loss="MSE")
plot(mtcarsModel02,1)
```

We use `loss="MSE"` in this case, to make sure that the model is estimated via OLS. We will discuss the default estimation method in `alm()`, likelihood, in Section \@ref(likelihoodApproach).

The plot on Figure \@ref(fig:mtcarsModel02Plot) can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Section \@ref(assumptions)). 


## Quality of a fit {#linearRegressionMultipleQualityOfFit}
Building upon the discussion of the quality of the fit in Section \@ref(linearRegressionSimpleQualityOfFit), we can introduce a measure, based on the OLS criterion, \@ref(eq:OLSCriterion), which is called either "Root Mean Squared Error" (RMSE) or a "standard error" or a "standard deviation of error" of the regression:
\begin{equation}
    \hat{\sigma}^2 = \sqrt{\frac{1}{n-k} \sum_{j=1}^n e_j^2 }.
    (\#eq:RMSERegression)
\end{equation}
The denominator of \@ref(eq:RMSERegression) contains the number of degrees of freedom in the model, $n-k$, not the number of observations $n$, so technically speaking this is not a "mean" any more. This is done to correct the in-sample bias (Section \@ref(estimatesPropertiesBias)) of the measure. Standard error does not tell us about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower it is, the better the model fits the data. However, this measure is not aware of the randomness in the true model (Section \@ref(modelsMethods)) and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one.

Here is how this can be calculated for our model, estimated using `alm()` function:
```{r}
sigma(mtcarsModel02)
```
The value of RMSE does not provide any important insights on its own, but it can be compared to the RMSE of another model to decide, which one of the two fits the data better.

Similarly to the simple linear regression, we can calculate the R$^2$ (see Section \@ref(linearRegressionSimpleQualityOfFit)). The problem is that the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will deteriorate. Here is an example:
```{r}
# Record number of observations
n <- nobs(mtcarsModel02)
# Generate white noise
mtcarsData$noise <- rnorm(n,0,10)
# Add it to the model
mtcarsModel02WithNoise <- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb+noise,
                                   mtcarsData, loss="MSE")
```
The code above introduces a new variable, `noise`, which has nothing to do with the `mpg`. We would expect that this variable would not bring value to the model. And here is the value of determination coefficient of the new model:
```{r}
1 - sum(resid(mtcarsModel02WithNoise)^2) /
    (var(actuals(mtcarsModel02WithNoise))*(n-1))
```
Compare it with the previous one: 
```{r}
1 - sum(resid(mtcarsModel02)^2) /
    (var(actuals(mtcarsModel02))*(n-1))
```
The value in the new model will always be higher than in the previous one (or equal to it in some very special cases), no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of variables in the model is required in order to make the measure more reasonable. This is what adjusted coefficient of determination does:
\begin{equation}
    R^2_{adj} = 1 - \frac{\hat{\sigma}^2}{\mathrm{V}(y)} = 1 - \frac{(n-1)\mathrm{SSE}}{(n-k)\mathrm{SST}} .
    (\#eq:DeterminationAdjusted)
\end{equation}
So, instead of dividing sums of squares, in the adjusted R$^2$ we divide the entities that are based on degrees of freedom. Given the presence of $k$ in the formula \@ref(eq:DeterminationAdjusted), the coefficient will not necessarily increase with the addition of variables -- when the variable does not contribute in the reduction of SSE of model substantially, R$^2$ will not go up. Furthermore, if one model has higher $\hat{\sigma}^2$ than the other one, then the R$^2_{adj}$ of that model will be lower, which becomes apparent, given that we have $-\hat{\sigma}^2$ in the formula \@ref(eq:DeterminationAdjusted).

Here how the adjusted R$^2$ can be calculated for a model in R:
```{r}
setNames(c(1 - sigma(mtcarsModel02)^2 / var(actuals(mtcarsModel02)),
           1 - sigma(mtcarsModel02WithNoise)^2 / var(actuals(mtcarsModel02WithNoise))),
         c("R^2-adj","R^2-adj, Noise"))
```
What we will typically see in the output above is that the model with the noise will have a lower value of adjusted R$^2$ than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter. So, you should not fully trust R$^2_{adj}$ either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inaccurate forecasts and inefficient estimates of parameters (see Section \@ref(assumptions) for details).


## Interpretation of parameters
Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model:
```{r}
coef(mtcarsModel02)
```

Each of the parameters of this model shows an **average** effect of each variable on the mileage. They have a simple interpretation and show how the response variable will change **on average** with the increase of a variable by 1 unit, keeping all the other variables constant. For example, the parameter for `wt` (weight) shows that with the increase of weight of a car by 1000 pounds, the mileage would decrease **on average** by `r round(abs(coef(mtcarsModel02)["wt"]),3)` miles per gallon, if all the other variables do not change. I have made the word "average" boldface three times in this paragraph for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. They do not show how it will change for each point. The regression model capture average tendencies and thus the word "average" is very important in the interpretation. In each specific case, the increase of weight by 1 will lead to different decreases (and even increases in some cases). But if we take the arithmetic mean of those individual effects, it will be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section \@ref(assumptions)).
