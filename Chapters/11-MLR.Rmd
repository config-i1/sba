# Multiple Linear Regression {#linearRegression}
::: example
One of the problems that construction companies face is getting a good estimate of the budget needed to build something. Many companies tend to underestimate the costs and time the project will take. To address this, a mid-size company called "Eden city", which specialises on construction of residential buildings, decided to take a more analytical approach to the problem. They have collected the data of their previous projects and needs help in building a model that would explain what forms the costs for different types of buildings. Their idea is to use this model during the business plan write-up phase to get an estimate of the future project, which they hope will be better than the ones they used before based on their pure judgment.
:::

In this example, we are interested in the overall costs of construction, which can be impacted by:

- The size of a building in squared meters,
- The cost of materials,
- Type of building (detached, semi-detached, bungalow etc),
- How many projects the specific crew did before,
- Year when the project was started.

::: question
What else do you think can impact such costs in theory?
:::

```{r echo=FALSE}
load("data/SBA_Chapter_11_Costs.Rdata")
```

This data is available online:

```{r eval=FALSE}
load(url("https://github.com/config-i1/sba/raw/refs/heads/master/data/SBA_Chapter_11_Costs.Rdata"))
```

Based on what we have discussed before, we can do analysis of measures of association and even build simple regression model (or several of them), but we acknowledge that in many real life situations, there are many factors that impact the variable of interest. In the example above, we have listed five explanatory variables that can be connected to the overall costs. This means that a basic bi-variate analysis (one variable vs the other) might not be sufficient. Furthermore, the relations between variables are typically complicated. So analysing, for example, only the relation between the cost of project and the size of building without considering the cost of materials might be misleading.

Here is how the bi-variate relations between the variables in our dataset look like (Figure \@ref(fig:CostsSpread)):

```{r CostsSpread, fig.cap="Spread plot between variables in the building costs dataset."}
spread(SBA_Chapter_11_Costs)
```

While the plot in Figure \@ref(fig:CostsSpread) gives a good idea about the relations, for example, between the size of property and the overall costs (it seems to be linear positive) or between material and the overall costs (linear positive again), it does not tell us much about the complex relation between one variable (overall costs) and several others.

All of this gives a motivation to having a so called "Multiple Linear Regression", the model that expresses the relation between one variable and several of others. Mathematically, this is a straight forward extension of the simple linear regression model from Chapter \@ref(simpleLinearRegression), where we just add variables to the right-hand side of the equation. For example, if we had two variables impacting one (e.g. $size$ of project and cost of $materials$ vs $overall$ cost of the project), we could write:
\begin{equation}
    overall_j = \beta_0 + \beta_1 size_{j} + \beta_2 material_{j} + \epsilon_j ,
    (\#eq:MLRFormulaExample)
\end{equation}
where $beta_0$ is the intercept, and $beta_1$, $beta_2$ are the coefficients for the respective variables. The predicted overall costs can be calculated based on this model by dropping the error term $\epsilon_j$:
\begin{equation*}
    \widehat{overall}_j = \beta_0 + \beta_1 size_{j} + \beta_2 material_{j}.
\end{equation*}
While in the example with the Simple Linear Regression the predicted (or fitted) values implied drawing a line through the cloud of dots on the plane of the two variables, now we are talking about drawing a plane through the point in the three-dimensional space. It can be visualised in the following way (Figure \@ref(fig:scatterplot3dProjectCosts)):

```{r scatterplot3dProjectCosts, fig.cap="3D scatterplot of Overall costs vs size of project and costs of materials.", echo=FALSE}
fit <- lm(overall ~ size+materials, data=SBA_Chapter_11_Costs)
s3d <- scatterplot3d::scatterplot3d(SBA_Chapter_11_Costs$materials, SBA_Chapter_11_Costs$size,
                                    SBA_Chapter_11_Costs$overall,
                                    pch=16, highlight.3d=TRUE,
                                    type="h", xlab="Materials costs", ylab="Size of the building",
                                    zlab="Overall costs", main="")
s3d$plane3d(fit, lty.box="solid", col="darkblue", draw_polygon=TRUE,
            polygon_args=list(col=rgb(0,0,0.8,0.2)))
```

The 3d image in Figure \@ref(fig:scatterplot3dProjectCosts) is already hard to analyse, but at least it gives an idea of how the overall costs change with the change of materials costs and size of buildings. However, it would be impossible to produce a meaningful plot of overall costs from more than two variables. What the figure above gives us is the connection between the simple linear regression (which is just a straight line in the two-dimensional plane) and the multiple one (which is a plane in a multi-dimensional space).

In a more general way, the multiple linear regression can be written as:
\begin{equation}
    y_j = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} + \epsilon_j ,
    (\#eq:MLRFormula)
\end{equation}
where $\beta_i$ is a $i$-th parameter for the respective $i$-th explanatory variable and there is $k-1$ of them in the model, meaning that when we want to estimate this model, we will have $k$ unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is:
\begin{equation}
    \mu_{y,j} = \mathrm{E}(y_j | \mathbf{x}_j) = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} .
    (\#eq:MLRExpectation)
\end{equation}
Furthermore, similar to how we discussed it in Chapter \@ref(simpleLinearRegression), when we want to estimate model \@ref{eq:MLRFormula}, we should substitute all parameters $\beta_j$ with their estimates $b_j$:
\begin{equation}
    \hat{y}_j = b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} .
    (\#eq:MLRExpectationSample)
\end{equation}
Similar to the Simple Linear Regression, each parameter in equation \@ref(eq:MLRExpectationSample) represents the slope for the respective variable, showing how on average the value of the response variable (overall costs in our example) changes with the change of each variable.


## OLS estimation
We have already discussed the idea of the OLS in Section \@ref(OLS) on the example of the Simple Linear Regression. The logic with the multiple one is exactly the same: we want to minimise the sum of squared errors by changing the values of parameters. The main difference now is that we can have more than two parameters and as a result, the formulae become more complicated.

### A bit of maths
There are several ways how we can get the formula for the OLS of the Multiple Linear Regression. We could start with the same SSE value, expanding it based on the equation of the regression \@ref(eq:MLRExpectationSample):
\begin{equation*}
    \mathrm{SSE} = \sum_{j=1}^n (y_j - \hat{y}_j)^2 = \sum_{j=1}^n (y_j - b_0 - b_1 x_{1,j} - b_2 x_{2,j} - \dots - b_{k-1} x_{k-1,j})^2 .
\end{equation*}
Taking derivatives of SSE with respect to $b_0$, $b_1$, $b_2$, $\dots$, $b_{k-1}$ and then equating them to zero, we would get a so-called **System of Normal Equations** (we have discussed it when providing a proof in Section \@ref(OLS)), which in general case has form: 
\begin{equation*}
    \begin{aligned}
        & \sum_{j=1}^n y_j - n b_0 - b_1 \sum_{j=1}^n x_{1,j} - b_2 \sum_{j=1}^n x_{2,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} = 0 \\
        & \sum_{j=1}^n y_j x_{1,j} - b_0 \sum_{j=1}^n x_{1,j} - b_1 \sum_{j=1}^n x^2_{1,j} - b_2 \sum_{j=1}^n x_{2,j} x_{1,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} x_{1,j} = 0 \\
        & \sum_{j=1}^n y_j x_{2,j} - b_0 \sum_{j=1}^n x_{2,j} - b_1 \sum_{j=1}^n x_{1,j} x_{2,j} - b_2 \sum_{j=1}^n x^2_{2,j} - \dots - b_{k-1} \sum_{j=1}^n x_{k-1,j} x_{2,j} = 0 \\
        \vdots
        & \sum_{j=1}^n y_j x_{k-1,j} - b_0 \sum_{j=1}^n x_{k-1,j} - b_1 \sum_{j=1}^n x_{1,j} x_{k-1,j} - b_2 \sum_{j=1}^n x_{2,j} x_{k-1,j} - \dots - b_{k-1} \sum_{j=1}^n x^2_{k-1,j} = 0
    \end{aligned}
\end{equation*}
Solving this system of equations gives us formulae for parameters $b_0$, $b_1$, $b_2$, $\dots$, $b_{k-1}$, that guarantee that the SSE is minimal.

However, there is a more compact and easier in logic way of getting the formulae, but it requires some basic knowledge of linear algebra. To explain it, we need to present the multiple linear regression in a more compact form. In order to do that we will introduce the following vectors:
\begin{equation}
    \mathbf{x}'_j = \begin{pmatrix}1 & x_{1,j} & \dots & x_{k-1,j} \end{pmatrix},
    \boldsymbol{\beta} = \begin{pmatrix}\beta_0 \\ \beta_{1} \\ \vdots \\ \beta_{k-1} \end{pmatrix} ,
    (\#eq:MLRVectors)
\end{equation}
where $'$ symbol is the transposition. This can then be substituted in \@ref(eq:MLRFormula) to get:
\begin{equation}
    y_j = \mathbf{x}'_j \boldsymbol{\beta} + \epsilon_j .
    (\#eq:MLRFormulaCompacter)
\end{equation}
This form is just convenient, but it denotes exactly the same model as in equation \@ref(eq:MLRFormula). All we need to remember in case of the equation \@ref(eq:MLRFormulaCompacter) is that it represents the sum of products of variables by their coefficients, just in a compact way.

But this is not over yet, we can make it even more compact, if we pack all those values with index $j$ in vectors and matrices:
\begin{equation}
    \mathbf{X} = \begin{pmatrix} \mathbf{x}'_1 \\ \mathbf{x}'_2 \\ \vdots \\ \mathbf{x}'_n \end{pmatrix} = 
    \begin{pmatrix} 1 & x_{1,1} & \dots & x_{k-1,1} \\ 1 & x_{1,2} & \dots & x_{k-1,2} \\ \vdots \\ 1 & x_{1,n} & \dots & x_{k-1,n} \end{pmatrix}, 
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}, 
    \boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix} ,
    (\#eq:MLRMatrices)
\end{equation}
where $n$ is the sample size. This leads to the following even more compact form of the multiple linear regression:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} .
    (\#eq:MLRFormulaCompactest)
\end{equation}
If you compare \@ref(eq:MLRFormulaCompactest) with the original one \@ref(eq:MLRFormula):
\begin{equation*}
    y_j = \beta_0 + \beta_1 x_{1,j} + \beta_2 x_{2,j} + \dots + \beta_{k-1} x_{k-1,j} + \epsilon_j ,
\end{equation*}
you will probably see the connection. But the form \@ref(eq:MLRFormulaCompactest) is just more abstract. This abstraction, however, allows us getting an analytical formula for the calculation of the estimates of parameters of the model (remember, we substitute the true values $\beta_j$ by their sample estimates $b_j$):
\begin{equation}
    \mathbf{b} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
    (\#eq:MLROLS)
\end{equation}

::: proof
Sum of squared errors based on the model \@ref(eq:MLRFormulaCompactest) applied to the data can be expressed as:
\begin{equation*}
    \mathrm{SSE} = \mathbf{e}' \mathbf{e} = (\mathbf{y} - \mathbf{X} \mathbf{b})' (\mathbf{y} - \mathbf{X} \mathbf{b}) ,
\end{equation*}
where $\mathbf{e}$ is the estimate of $\boldsymbol{\epsilon}$ and $\mathbf{b}$ is the estimate of $\boldsymbol{\beta}$. This can be expanded by opening brackets to:
\begin{equation}
    \begin{aligned}
        \mathrm{SSE} = & \mathbf{y}'\mathbf{y} - \mathbf{y}' \mathbf{X} \mathbf{b} - \mathbf{b}' \mathbf{X}' \mathbf{y} + \mathbf{b}' \mathbf{X}' \mathbf{X} \mathbf{b} = \\
        & \mathbf{y}'\mathbf{y} - 2 \mathbf{b}' \mathbf{X}' \mathbf{y} + \mathbf{b}' \mathbf{X}' \mathbf{X} \mathbf{b}
    \end{aligned} ,
    (\#eq:MLROLS02)
\end{equation}
which can be done because $\mathbf{y}' \mathbf{X} \mathbf{b}$ is a scalar and $\mathbf{y}' \mathbf{X} \mathbf{b} = (\mathbf{y}' \mathbf{X} \mathbf{b})' = \mathbf{b}' \mathbf{X}' \mathbf{y}$. Now we need to minimise \@ref(eq:MLROLS02) with respect to parameters to find their estimates. This can be done by taking derivative of \@ref(eq:MLROLS02) with respect to $\mathbf{b}$ and equating it to zero:
\begin{equation}
    \frac{\partial \mathrm{SSE}}{\partial \mathbf{b}} = - 2 \mathbf{X}' \mathbf{y} + 2 \mathbf{X}' \mathbf{X} \mathbf{b} = 0.
    (\#eq:MLROLS03)
\end{equation}
After that, we can regroup the elements in \@ref(eq:MLROLS03) to get:
\begin{equation*}
    \mathbf{X}' \mathbf{X} \mathbf{b} = \mathbf{X}' \mathbf{y}.
\end{equation*}
And then, we can multiply both parts of the equation by the inverse of $\mathbf{X}' \mathbf{X}$ to get rid of that part in the left-hand side of the equation:
\begin{equation*}
    (\mathbf{X}' \mathbf{X})^{-1} \mathbf{X}' \mathbf{X} \mathbf{b} = (\mathbf{X}' \mathbf{X})^{-1} \mathbf{X}' \mathbf{y},
\end{equation*}
which then leads to the final formula:
\begin{equation*}
    \mathbf{b} = \left(\mathbf{X}' \mathbf{X}\right)^{-1} \mathbf{X}' \mathbf{y} .
\end{equation*}
:::

### Application
Luckily, you do not need to remember the formula \@ref(eq:MLROLS) and should not need to use it in real life, because it is used in all statistical software, including `lm()` function from `stats` package for R. Here is an example with the same dataset:

```{r}
costsModel01 <- lm(overall~size+materials+projects+year, SBA_Chapter_11_Costs)
```

To better understand what fitting such model to the data implies, we can produce a plot of fitted vs actuals values, with $\hat{y}_j$ on x-axis and $y_j$ on the y-axis:

```{r}
plot(fitted(costsModel01),actuals(costsModel01))
```

The same plot is produced via `plot()` method if we use `alm()` function from `greybox` instead:

```{r costsModel02Plot, fig.cap="Actuals vs fitted values for multiple linear regression model on mtcars data."}
costsModel02 <- alm(overall~size+materials+projects+year, SBA_Chapter_11_Costs, loss="MSE")
plot(costsModel02,1)
```

We use `loss="MSE"` in this case, to make sure that the model is estimated via OLS. We will discuss the default estimation method in `alm()`, likelihood, in Section \@ref(likelihoodApproach).

The plot on Figure \@ref(fig:costsModel02Plot) can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Chapter \@ref(assumptions)). 


### Properties of the OLS Estimators



### Gauss-Markov theorem {#GaussMarkov}
OLS is a very popular estimation method for linear regression for a variety of reasons. First, it is relatively simple (much simpler than other approaches) and conceptually easy to understand. Second, the estimates of OLS parameters can be found analytically (using formula \@ref(eq:OLSSLREstimates)). Furthermore, there is a mathematical proof that the OLS estimates of parameters are efficient (Subsection \@ref(estimatesPropertiesEfficiency)), consistent (Subsection \@ref(estimatesPropertiesConsistency)) and unbiased (Subsection \@ref(estimatesPropertiesBias)). It is called "Gauss-Markov theorem", and it states that:

::: theorem
If regression model is correctly specified then OLS will produce Best Linear Unbiased Estimates (BLUE) of its parameters.
:::

The term "correctly specified" implies that all main statistical assumptions about the model are satisfied (such as no omitted important variables, no autocorrelation and heteroscedasticity in the residuals, see details in Chapter \@ref(assumptions)). The "BLUE" part means that OLS guarantees the most efficient and the least biased estimates of parameters amongst all possible estimators of a linear model. For example, if we used a criterion of minimisation of Mean Absolute Error (MAE), then the estimates of parameters would be less efficient than in case of OLS. This is because OLS gives "mean" estimates, while the minimum of MAE corresponds to the median (see Subsection \@ref(estimatesPropertiesEfficiency)).

Practically speaking, the theorem implies that when you use OLS, the estimates of parameters will have good statistical properties (given that the model is correctly specified), in some cases better than the estimates obtained using other methods.



## Quality of a fit {#linearRegressionMultipleQualityOfFit}
Building upon the discussion of the quality of the fit in Section \@ref(linearRegressionSimpleQualityOfFit), we can introduce a measure, based on the OLS criterion, \@ref(eq:OLSCriterion), which is called either "Root Mean Squared Error" (RMSE) or a "standard error" or a "standard deviation of error" of the regression:
\begin{equation}
    \hat{\sigma}^2 = \sqrt{\frac{1}{n-k} \sum_{j=1}^n e_j^2 }.
    (\#eq:RMSERegression)
\end{equation}
The denominator of \@ref(eq:RMSERegression) contains the number of degrees of freedom in the model, $n-k$, not the number of observations $n$, so technically speaking this is not a "mean" any more. This is done to correct the in-sample bias (Section \@ref(estimatesPropertiesBias)) of the measure. Standard error does not tell us about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower it is, the better the model fits the data. However, this measure is not aware of the randomness in the true model (Section \@ref(modelsMethods)) and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one.

Here is how this can be calculated for our model, estimated using `alm()` function:
```{r}
sigma(costsModel02)
```
The value of RMSE does not provide any important insights on its own, but it can be compared to the RMSE of another model to decide, which one of the two fits the data better.

Similarly to the simple linear regression, we can calculate the R$^2$ (see Section \@ref(linearRegressionSimpleQualityOfFit)). The problem is that the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will deteriorate. Here is an example:
```{r}
# Record number of observations
n <- nobs(costsModel02)
# Generate white noise
SBA_Chapter_11_Costs$noise <- rnorm(n,0,10)
# Add it to the model
costsModel02WithNoise <- alm(overall~size+materials+projects+year+noise,
                             SBA_Chapter_11_Costs, loss="MSE")
```
The code above introduces a new variable, `noise`, which has nothing to do with the `mpg`. We would expect that this variable would not bring value to the model. And here is the value of determination coefficient of the new model:
```{r}
1 - sum(resid(costsModel02WithNoise)^2) /
    (var(actuals(costsModel02WithNoise))*(n-1))
```
Compare it with the previous one: 
```{r}
1 - sum(resid(costsModel02)^2) /
    (var(actuals(costsModel02))*(n-1))
```
The value in the new model will always be higher than in the previous one (or equal to it in some very special cases), no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of variables in the model is required in order to make the measure more reasonable. This is what adjusted coefficient of determination does:
\begin{equation}
    R^2_{adj} = 1 - \frac{\hat{\sigma}^2}{\mathrm{V}(y)} = 1 - \frac{(n-1)\mathrm{SSE}}{(n-k)\mathrm{SST}} .
    (\#eq:DeterminationAdjusted)
\end{equation}
So, instead of dividing sums of squares, in the adjusted R$^2$ we divide the entities that are based on degrees of freedom. Given the presence of $k$ in the formula \@ref(eq:DeterminationAdjusted), the coefficient will not necessarily increase with the addition of variables -- when the variable does not contribute in the reduction of SSE of model substantially, R$^2$ will not go up. Furthermore, if one model has higher $\hat{\sigma}^2$ than the other one, then the R$^2_{adj}$ of that model will be lower, which becomes apparent, given that we have $-\hat{\sigma}^2$ in the formula \@ref(eq:DeterminationAdjusted).

Here how the adjusted R$^2$ can be calculated for a model in R:
```{r}
setNames(c(1 - sigma(costsModel02)^2 / var(actuals(costsModel02)),
           1 - sigma(costsModel02WithNoise)^2 / var(actuals(costsModel02WithNoise))),
         c("R^2-adj","R^2-adj, Noise"))
```
What we will typically see in the output above is that the model with the noise will have a lower value of adjusted R$^2$ than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter. So, you should not fully trust R$^2_{adj}$ either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inaccurate forecasts and inefficient estimates of parameters (see Section \@ref(assumptions) for details).


## Interpretation of parameters
Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model:
```{r}
coef(costsModel02)
```

Each of the parameters of this model shows an **average** effect of each variable on the mileage. They have a simple interpretation and show how the response variable will change **on average** with the increase of a variable by 1 unit, keeping all the other variables constant. For example, the parameter for `wt` (weight) shows that with the increase of weight of a car by 1000 pounds, the mileage would decrease **on average** by `r round(abs(coef(costsModel02)["wt"]),3)` miles per gallon, if all the other variables do not change. I have made the word "average" boldface three times in this paragraph for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. They do not show how it will change for each point. The regression model capture average tendencies and thus the word "average" is very important in the interpretation. In each specific case, the increase of weight by 1 will lead to different decreases (and even increases in some cases). But if we take the arithmetic mean of those individual effects, it will be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section \@ref(assumptions)).
