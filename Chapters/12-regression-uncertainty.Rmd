# Uncertainty in regression {#uncertaintyRegression}

::: remark
In this chapter, we focus on a Simple Linear Regression in our discussion, showing mathematical derivations for a more general case. We note that such simplistic model is rarely correct for the data (not a "true" model), but for the purposes of explanation, we find it more intuitive than the proper multiple linear regression.
:::

Coming back to the example of properties costs from Chapter \@ref(linearRegression), consider a simple linear regression of material vs overall costs, fit to a sub-sample of data. If we fit the line to the first 25 observations we would get a different model than when fitting it to the sample of 50 observations. Figure \@ref(fig:scatterCostsUncertainty) depicts these two situations, where the blue dashed red line corresponds to the bigger sample (fit to the empty red circles in the plot), while the blue solid line represents the model for the smaller sample (blue solid circles).

```{r scatterCostsUncertainty, fig.cap="Material vs overall costs and two regression lines.", echo=FALSE}
costsModelSLRSmall <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:25)
costsModelSLRLarge <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:50)
plot(overall~materials, SBA_Chapter_11_Costs, subset=1:50,
     xlab="Material costs", ylab="Overall costs",
     col=3, pch=1, lwd=2)
points(SBA_Chapter_11_Costs$materials[1:25], SBA_Chapter_11_Costs$overall[1:25],
       col=5, pch=20, lwd=2)
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(costsModelSLRLarge, col=3, lty=2)
text(250,380,paste0(c("overall = ",round(coef(costsModelSLRLarge)[1],2)," + ",
                      round(coef(costsModelSLRLarge)[2],2)," materials + e"),collapse=""), col=3)
abline(costsModelSLRSmall, col=5)
text(215,557,paste0(c("overall = ",round(coef(costsModelSLRSmall)[1],2)," + ",
                      round(coef(costsModelSLRSmall)[2],2)," materials + e"),collapse=""), col=5)
legend("topleft", legend=c("50 observations","25 observations"),
       lwd=2, col=c(3,5), lty=c(2,1))
```

We see that the two lines differ both in terms of their intercepts and their slopes. So, which one of them is correct?

The general view of a spherical statistician in vacuum is that the larger the sample is, the better the model is. While this is correct in general, we would argue that none of the two models is correct. This is because they are both just estimates of the true model (Section \@ref(modelsMethods) on a sample of data and they both inevitably inherit the uncertainty of the data. This means that whatever regression model we estimate on a sample of data, it will always be incorrect in the sense that it can only be considered as an approximation of the truth.

More importantly, the line will always change when the model is fit to the new sample of data, even if we add only one observation. This is because on different samples, we get different estimates of parameters, which comes from one of the fundamental sources of uncertainty (the second one discussed in Section \@ref(sourcesOfUncertainty)). This means that the estimates of parameters are random and might have some distribution of their own.

Exactly the same effect will be observed if we collect a different random sample from the same population, for example, if we worked for a different company that does similar business and collects the same data. In that case, the parameters would be random again, having some sort of a distribution.

To see this point clearer, we fit the same simple regression model to randomly selected subsamples of the original dataset many times and collect the estimates of parameters to plot histograms and see their distribution. This empirical way of capturing uncertainty is called "bootstrap" and we will discuss it in more detail in Chapter \@ref(SectionNeededHere). The R code below shows how this can be done using the `coefbootstrap()` function from the `greybox` package (which implements a simple "Case Resampling" technique):

```{r}
# Fit the model
costsModelSLR <- alm(overall ~ materials, data=SBA_Chapter_11_Costs)
# Set random seed for reproducibility
set.seed(41)
costsModelSLRBootstrap <- coefbootstrap(costsModelSLR)
```

Based on that we can plot the histograms of the estimates of parameters as shown in Figure \@ref(fig:costsModelSLRBootstrap).

```{r costsModelSLRBootstrap, fig.cap="Distribution of bootstrapped parameters of a regression model"}
par(mfcol=c(1,2))
# First histogram
hist(costsModelSLRBootstrap$coefficients[,1],
     xlab="Intercept", main="")
# The estimated coefficient
abline(v=coef(costsModelSLR)[1], col="darkred", lwd=2)
# Second histogram
hist(costsModelSLRBootstrap$coefficients[,2],
     xlab="Slope", main="")
# The estimated coefficient
abline(v=coef(costsModelSLR)[2], col="darkred", lwd=2)
```

Figure \@ref(fig:costsModelSLRBootstrap) shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution and demonstrate the variability of the estimates around the specific values. The vertical lines on the plots show the estimated values of parameters, which hopefully should be close to the ones in the true model. If we were we to repeat this experiment thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections \@ref(CLT) and \@ref(assumptions)).

This example demonstrates that any model applied to the data will always have inherited uncertainty, which should be taken into account. In decision making, you should not rely on just a number from your model, and you should get a general understanding of what the variability about the estimates of parameters is. This has its implications for both parameters analysis and forecasting.


## Covariance matrix of parameters {#uncertaintyRegressionCov}
One of the simplest ways of capturing the uncertainty about the parameters is by calculating the covariance matrix of parameters. It is the matrix that shows the individual and joint variabilities of parameters of the model. It has the following shape:

\begin{equation}
    \mathrm{V}({\boldsymbol{b}}) =
    \begin{pmatrix}
        \mathrm{V}(b_0) & \mathrm{cov}(b_0, b_1) & \dots & \mathrm{cov}(b_0, b_{k-1}) \\
        \mathrm{cov}(b_0, b_1) & \mathrm{V}(b_1) & \dots & \mathrm{cov}(b_1, b_{k-1}) \\
        \vdots & \vdots & \ddots & \vdots \\
        \mathrm{cov}(b_0, b_{k-1}) & \mathrm{cov}(b_1, b_{k-1}) & \dots & \mathrm{V}(b_{k-1}) \\
    \end{pmatrix} ,
    (\#eq:MLRParameterCov)
\end{equation}
where ${\boldsymbol{b}}$ is the vector of all the estimated parameters. The covariance matrix above is similar to the one we discussed at the end of the Section \@ref(dataAnalysisNumerical), equation \@ref(eq:CovarMat), but it is now applied to the estimates of parameters of the model instead of several random variables. The diagonal elements of this matrix contain variances of parameters, showing how each estimate of parameter varies in sample, while the off-diagonal ones contain the covariances, capturing the joint variability of parameters. The square roots of the diagonal of that matrix would give the so called "Standard errors" of parameters (the standard deviations). These are typically used in further analysis.


### Bootstrap
There are different ways how we could calculate this matrix. We could use the bootstrap as we did in the introduction to this section, producing many estimates of the parameters and then calculating the matrix. R has all the necessary functions for that:

```{r}
var(costsModelSLRBootstrap$coefficients)
```

The values themselves do not tell us much about the model and parameters, so to better understand them, we can calculate the correlation matrix to see what is the linear relation between the estimates of parameters:

```{r}
var(costsModelSLRBootstrap$coefficients) |>
    cov2cor()
```

We see that the correlation between the intercept and the parameter for the materials is close to -1, which implies that the two have a strong negative linear relation. This means that when we draw lines through the clouds of points on samples of data (materials vs overall costs), the higher intercept implies that the line should have a lower slope, i.e. the line exhibits rotations around the middle of the data. This sort of behaviour is typical for the case when the two variables have positive correlation (with the increase of the material costs, the overall costs increase as well).


### Analytical solution
The alternative way of calculating the covariance matrix is using a formula. This is not universally available, and works only in case of OLS and a couple of other estimation methods (such as Maximisation of the Likelihood in case of the Normal distribution (see Chapter \@ref(likelihoodApproach)). The following formula is derived for OLS, and would not produce adequate results in case of other estimators:
\begin{equation}
    \mathrm{V}({\boldsymbol{b}}) = \frac{1}{n-k} \sum_{j=1}^n e_j^2 \times \left(\mathbf{X}' \mathbf{X}\right)^{-1}.
    (\#eq:MLRcovarianceMatrix)
\end{equation}
where $\mathbf{X}$ is the matrix of explanatory variables from equation \@ref(eq:MLRMatrices) and $e_j$ is the residual of the model on the observation $j$. One way to understand what this formula does is to keep in mind that $\mathbf{X}' \mathbf{X}$ will produce a covariance matrix of explanatory variables, so roughly the sample variance of the error term is then distributed across the variances and covariances of the explanatory variables, producing the corresponding variances and covariances of the parameters. The function `vcov()` in R, uses exactly this formula in case of the OLS/Likelihood estimation, returning the following:
```{r}
vcov(costsModelSLR)
```

We can see that the values of this covariance matrix differ from the ones produced via bootstrap. This is expected because different methods will always give different results. The analytical solution via \@ref(eq:MLRcovarianceMatrix) is more convenient and easier to calculate, but it relies on more assumptions than the bootstrapped one (see derivations below). The most important one is that *the model is correctly specified* (i.e. we deal with the true model applied to a sample of data). If this is violated, the estimates of the covariance matrix will be biased leading to a variety of issues, some of which will be discussed in Chapter \@ref(assumptions).


### Mathematical derivation
In this subsection, we show the proof that the variance matrix of parameters can be calculated via the formula \@ref(eq:MLRcovarianceMatrix). There are several ways of showing this, we use the one that does not need the values of the explanatory variables to be known. The only thing we need to assume at this moment is that the "true" model is known and is correctly estimated. If it is not, the final formula will stay the same, but it will have different implications.

::: warning
Linear algebra! Tread lightly!
:::

::: proof
We start by expanding the formula for the OLS \@ref(eq:MLROLS) by inserting the "true" model in it (${\mathbf{y} = \mathbf{X}} {\boldsymbol{\beta}} + {\boldsymbol{\epsilon}}$:
\begin{equation}
    \begin{aligned}
    {\boldsymbol{b}} =
        & \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{y}} = \\
        & \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime ({\mathbf{X}} {\boldsymbol{\beta}} + {\boldsymbol{\epsilon}}) = \\
        & \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{X}} {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} = \\
        & {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} .
    \end{aligned}
    (\#eq:MLROLSExpansion)
\end{equation}

This equation tells us that the estimated parameters will have some variability around the true ones, and the extent of that variability will depend on the explanatory variables in $\mathbf{X}$ and the specific values of the error term $\boldsymbol{\epsilon}$.

In bypassing, we can show that the OLS estimates of parameters are unbiased as long as the model is correctly specified:
\begin{equation}
    \begin{aligned}
    \mathrm{E}\left(\boldsymbol{b}\right) =
        & \mathrm{E}\left({\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}\right) = \\
        & {\boldsymbol{\beta}} + \mathrm{E}\left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}\right) = \\
        & {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime \mathrm{E}\left( {\boldsymbol{\epsilon}}\right) = \\
        & \boldsymbol{\beta} ,
    \end{aligned}
    (\#eq:MLRCLSExpansion)
\end{equation}
which holds as long as the expectation of the true error term is zero, and this is always true by the definition of the true model (see discussion in Subsection \@ref(TrueModel)).

More importantly, we can calculate the variance of the estimates of parameters using \@ref(eq:MLROLSExpansion):

\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{V}\left( {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right) .
    (\#eq:MLROLSVariance01)
\end{equation}

In formula \@ref(eq:MLROLSVariance01), the true parameter $\boldsymbol{\beta}$ should be independent of the explanatory variables and the error term by definition of the true model, so the formula can be represented as a sum of variances (the right hand side only). Furthermore, in this basic linear model, we assume that the true parameter does not have any uncertainty, which means that the part related to ${\boldsymbol{\beta}}$ in \@ref(eq:MLROLSVariance01) can be dropped leaving us with:
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{V}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right).
    (\#eq:MLROLSVariance02)
\end{equation}
After that, we can recall that $\mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)=0$, because in the true model, there should be no structure in the error term and thus the explanatory variables and the error term should be unrelated (Subsection \@ref(TrueModel)). This is useful because now we can represent the variance \@ref(eq:MLROLSVariance02) as the expectation of products (this follows directly from equation \@ref(eq:VarianceAlt) that we discussed in Subsection \@ref(dataAnalysisNumerical)):
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \times \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
    (\#eq:MLROLSVariance03)
\end{equation}
Now to expand the expectation of the sum, we can use the following equality, coming directly from the definition of a covariance of two random variables (formula \@ref(eq:CovarianceAlt) from Section \@ref(dataAnalysisNumerical)):
\begin{equation}
    \begin{aligned}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = 
        & \mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right) \times \mathrm{E}\left(\left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right) + \\
        & \mathrm{cov} \left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}, \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
    \end{aligned}
    (\#eq:MLROLSVariance04)
\end{equation}
We can notice that the expectations in \@ref(eq:MLROLSVariance04) should be zero because, as discussed earlier, in the true model the error term is not related to anything. This means that the formula can be simplified to:
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{cov} \left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}, \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
    (\#eq:MLROLSVariance05)
\end{equation}
The second element in the brackets can be rewritten as:
\begin{equation*}
    \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime = {\boldsymbol{\epsilon}} {\mathbf{X}} \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} .
\end{equation*}
Substituting this to \@ref(eq:MLROLSVariance05) and moving explanatory variables from the covariance leads to:
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{X}} \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} \mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right),
    (\#eq:MLROLSVariance06)
\end{equation}
which after some cancellations leads to:
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} \sigma^2 .
    (\#eq:MLROLSVarianceSemiFinal)
\end{equation}
Given that we do not know the true variance of the error term, we need to estimate it. In case of the OLS, it can be done using the formula:
\begin{equation*}
    \hat{\sigma}^2 = \frac{1}{n-k} \sum_{j=1}^n e_j^2 ,
\end{equation*}
which after being insert in \@ref(eq:MLROLSVarianceSemiFinal), leads to:
\begin{equation}
    \mathrm{V}\left( {\boldsymbol{b}} \right) = \frac{1}{n-k} \sum_{j=1}^n e_j^2 \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} .
    (\#eq:MLROLSVarianceFinal)
\end{equation}
where is the estimate of the true variance based on the applied model.
:::

::: remark
Mathematically speaking, $\mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right)$ is a matrix, but because we are dealing with the true model and the true error term, the matrix should be diagonal with only variances in the main diagonal. This is because the error term cannot be correlated with itself (thus all the off-diagonal elements are zero). Furthermore, because we assume that the variance of the error term is the same for all observations, the following should hold:
\begin{equation*}
    \mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right) = \mathbf{I}_{k-1} \sigma^2,
\end{equation*}
where $\mathbf{I}_{k-1}$ is the identity matrix of the size $k-1$ and $\sigma^2$ is the variance of the error term.
:::

We should note that the majority of assumptions above needed for the derivation came from the definition of the true model. But the essential one was that the model is correctly estimated. If this is violated, the estimates of the elements in the covariance matrix will be biased in one way or the other (see Chapter \@ref(assumptions)).


## Confidence intervals of parameters {#uncertaintyRegressionCI}
Given that the estimates of parameters have some uncertainty associated with them, as discussed in the introduction to this Chapter, it makes sense to capture that uncertainty so that decision makers can have a better understanding about the observed effects. Covariance matrix discussed in Section \@ref(uncertaintyRegressionCov) captures that uncertainty, but it is hard to make any decisions based on it.

A simpler way to do that is to construct confidence intervals in a similar way to what we discussed in Section \@ref(confidenceInterval). Visually, this process is shown in Figure \@ref(fig:costsModelSLRCI), which continues the example we discussed before.

```{r costsModelSLRCI, fig.cap="Parameter uncertainty in the estimated model", echo=FALSE}
par(mfcol=c(1,2))
# First histogram
hist(costsModelSLRBootstrap$coefficients[,1],
     xlab="Intercept", main="", probability=TRUE,
     xlim=range(costsModelSLRBootstrap$coefficients[,1])*c(0.9,1.1))
# The estimated coefficient
# abline(v=coef(costsModelSLR)[1], col=3, lwd=2)
# lines(density(costsModelSLRBootstrap$coefficients[,1]), col=5, lwd=2)
xSequence <- mean(costsModelSLRBootstrap$coefficients[,1])+seq(-5,5,0.1)*sd(costsModelSLRBootstrap$coefficients[,1])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelSLRBootstrap$coefficients[,1]),
            sd(costsModelSLRBootstrap$coefficients[,1])), col=5, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelSLRBootstrap$coefficients[,1]),
               sd(costsModelSLRBootstrap$coefficients[,1])), col=5, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelSLRBootstrap$coefficients[,1]),
             sd(costsModelSLRBootstrap$coefficients[,1]))),
     rep(0.03,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelSLRBootstrap$coefficients[,1]),
                        sd(costsModelSLRBootstrap$coefficients[,1])),2), pos=c(2,4))

# Second histogram
hist(costsModelSLRBootstrap$coefficients[,2],
     xlab="Slope", main="", probability=TRUE)
# The estimated coefficient
# abline(v=coef(costsModelSLR)[2], col=3, lwd=2)
# lines(density(costsModelSLRBootstrap$coefficients[,2]), col=5, lwd=2)
xSequence <- mean(costsModelSLRBootstrap$coefficients[,2])+seq(-5,5,0.1)*sd(costsModelSLRBootstrap$coefficients[,2])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelSLRBootstrap$coefficients[,2]),
            sd(costsModelSLRBootstrap$coefficients[,2])), col=5, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelSLRBootstrap$coefficients[,2]),
               sd(costsModelSLRBootstrap$coefficients[,2])), col=5, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelSLRBootstrap$coefficients[,2]),
             sd(costsModelSLRBootstrap$coefficients[,2]))),
     rep(6,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelSLRBootstrap$coefficients[,2]),
                        sd(costsModelSLRBootstrap$coefficients[,2])),2), pos=c(2,4))
```

Figure \@ref(fig:costsModelSLRCI) demonstrates the bootstrapped distributions of parameters (as before), together with the Normal probability density functions on top of them and vertical lines at the tails that represent the confidence bounds. The idea behind this is that due to the Central Limit Theorem (Section \@ref(CLT)) we can assume that estimates of parameters follow the Normal distribution with some mean and variance, and based on that we can get the quantiles, thus inferring, for example, that the true Intercept will lie between `r round(qnorm(c(0.025), mean(costsModelSLRBootstrap$coefficients[,1]), sd(costsModelSLRBootstrap$coefficients[,1])),2)` and `r round(qnorm(c(0.975), mean(costsModelSLRBootstrap$coefficients[,1]), sd(costsModelSLRBootstrap$coefficients[,1])),2)` in the 95% of the cases, if we repeat the resampling experiment many times.

The interpretation of the confidence interval is exactly the same as in the simpler case discussed in the Section \@ref(confidenceInterval). The formula for it is slightly different, because it is constructed for the parameters of the model rather than the mean of the sample, but the logic is exactly the same:
\begin{equation}
    \beta_i \in (b_i + t_{\alpha/2}(n-k) s_{b_i}, b_i + t_{1-\alpha/2}(n-k) s_{b_i}),
    (\#eq:MLRParameterInterval)
\end{equation}
where $s_{b_i}$ is the standard error of the parameter $b_i$. An important note to make is that, as usual with confidence interval construction, we can use the Normal distribution only in the case when the variance of parameters is known. In reality, it is not, so, as discussed in Section \@ref(confidenceInterval), we need to use the Student's t distribution. This is why we have $t_{\alpha/2}(n-k)$ in the equation \@ref(eq:MLRParameterInterval) above.

::: remark
The important note is that in order to be able to use the formula \@ref(eq:MLRParameterInterval), we need the Central Limit Theorem to hold. If it does not, then the estimates of parameters would not follow the Normal distribution, and the Student's t-statistics would not be appropriate to calculate the interval.
:::

::: example
To calculate the confidence interval, using the equation \@ref(eq:MLRParameterInterval), we need to know several things:

1. Significance level $\alpha$ - we define it either based on our preferences or on the task at hand. This should be selected prior to construction of the interval. The typical one is 5%, mainly because the standard human has five fingers on a hand;
2. The value of the estimated parameter $b_i$. We get it after using the Least Squares, any statistical software can give us the estimate.
3. Standard error (or deviation) of the parameter. We would need to use additional formula to get it.
4. Number of degrees of freedom $n-k$, which can be easily calculated based on the sample size $n$ and the number of estimated parameters $k$.
5. Student's t statistics. This can be obtained from statistical tables or the software, given the significance level $\alpha$ and the number of degrees of freedom $n-k$, calculated above.

Consider construction of the interval for the slope parameter `materials` from the regression we discussed earlier. In our case, we know the following:

1. $\alpha=0.05$ because we decided to produce the 95% confidence interval. We could choose the other value, which would result in the interval of a different width;
2. The value of the parameter $b_1$ is `r coef(costsModelSLR)[2]`;
3. We have not discussed yet how to obtain the variance of the parameter analytically, but there is a built-in formula in software that gives it, and/or we could get it via the bootstrap, simply by taking the variance of the values from the distribution in Figure \@ref(fig:costsModelSLRCI). In R, we can get the variance of the parameter for slope (second parameter in the model) via the `vcov()` function, like this:
```{r}
vcov(costsModelSLR)[2,2]
```
Based on that, we can say that the standard error of the slope is approximately `r round(sqrt(vcov(costsModelSLR)[2,2]),4)`.
4. We estimated two parameters, so $k=2$, and the sample size $n$ was 61, which means that in our model has $n-k=61-2=59$ degrees of freedom.
5. To get the Student's t statistics correctly, we need to split the significance level $\alpha$ into two equal part, meaning that we will have 2.5% of values below the lower bound and another 2.5% of values above the upper one. So, we will calculate it for $\alpha/2=0.025$ and $1-\alpha/2=0.975$ with $n-k=59$. In R, we can use the `qt()` function in the folloqing way:
```{r}
qt(c(0.025,0.975), 59)
```

```{r echo=FALSE}
x <- round(coef(costsModelSLR)[2]+qt(c(0.025,0.975), 59)*round(sqrt(vcov(costsModelSLR)[2,2]),4),4)
```

Taking all these values and inserting them in the formula \@ref(eq:MLRParameterInterval), we should get the two numbers, representing the lower and the upper bounds of the 95% confidence interval respectively: (`r paste0(x, collapse=",")``)

In R, the confidence interval can be obtained via the `confint()` function, and it should be close to what we obtained above, but not exactly the same due to rounding:
```{r}
confint(costsModelSLR)
```

```{r echo=FALSE}
x <- confint(costsModelSLR)[2,2:3]
```

The confidence interval for materials above shows, for example, that if we repeat the construction of interval many times on different samples of data, the true value of parameter will lie in 95% of cases between `r x[1]` and `r x[2]`. This gives us an idea about the real effect in the population and how certain we are about it.
:::

We can also present all of this in the following summary (this is based on the `alm()` model, the other functions will produce different ones):
```{r}
summary(costsModelSLR)
```

This summary provides all the necessary information about the model and the estimates of parameters: their mean values in the column "Estimate", their standard errors (square roots of the variances) in "Std. Error", the bounds of confidence interval and finally a star if the interval does not contain zero. If we have that star, then this indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect indeed exists.


## Regression line related uncertainty {#uncertaintyRegressionLine}
Given the uncertainty of estimates of parameters, the regression line itself and the points around it will vary with different samples. This means that in some cases we should not just consider the predicted values of the regression $\hat{y}_j$, but should also consider the uncertainty around them. Similar to what we discussed in Sections \@ref(confidenceInterval) and \@ref(predictionInterval), there are two different interval types we can produce from the regression: confidence and prediction. It is extremely important to get the difference between the two, so here it is again just in case:

1. **Confidence interval** shows where the **expected value** should lie in the population in the x% of the cases if we repeat the calculation for different samples of data many times;
2. **Prediction interval** shows where the x% of the **actual values** should lie.

The former will tell you something about the true *expected costs* of a project, while the latter will tell you about the *actual costs*.

Let's see what this means in the regression context.


### Confidence interval {#uncertaintyRegressionLineConfidence}
In context of univariate statistics that we discussed in Section \@ref(confidenceInterval), the idea was to capture the uncertainty around the mean of the data. In case of regression, we are focusing on the conditional mean, i.e. the expected value, given the values of all explanatory variables for a specific observation. In the example that we have used in this chapter, this would come to understanding what the true expected overall costs should be, for example, for the material costs of 200 thousands of pounds. This should be straight forward to calculate as long as we have the covariance matrix of parameters (Section \@ref(uncertaintyRegressionCov)), which reflects the potential changes of the line with the changes of the sample.

The formula for the construction of the confidence interval is very similar to the one we discussed for the sample mean in Section \@ref(confidenceInterval):
\begin{equation}
    \mu_j \in (\hat{y}_j + t_{\alpha/2}(n-k) s_{\hat{y}_j}, \hat{y}_j + t_{1-\alpha/2}(n-k) s_{\hat{y}_j}),
    (\#eq:confidenceIntervalRegression)
\end{equation}
where $\mu_j$ is the true expectation, and $s_{\hat{y}_j}=\sqrt{\mathrm{V}(\hat{y}_j| \mathbf{x}_j)}$ is the standard deviation of the fitted value. Compare this with the one we had in Section \@ref(confidenceInterval):
\begin{equation*}
    \mu \in (\bar{y} + t_{\alpha/2}(n-1) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(n-1) s_{\bar{y}}).
\end{equation*}
Here are the main differences:

1. We now focus on $\mu_j$, the expectation for a specific observation instead of just global average $\mu$;
2. We have the fitted value from the regression line $\hat{y}_j$ instead of the sample mean $\bar{y}$;
3. We have the standard deviation of the line $s_{\hat{y}_j}$ instead of the standard deviation of the mean $s_{\bar{y}}$;
4. We have $n-k$ degrees of freedom instead of $n-1$ because we estimated $k$ parameters instead of just one (the sample mean).

But the rest is the same and uses exactly the same principles.

::: remark
To be able to construct the confidence interval using the formula above, we need one important assumption to hold. This assumption is that the Central Limit Theorem holds (Section \@ref(CLT)). If it does not then the distribution of the estimates of parameters would not be Normal and as a result, the distribution of the conditional expectations would not be Normal either. In that case, we would need to use some other tools (such as bootstrap) to construct the interval.
:::

In R, the confidence interval can be constructed for each observation via the `predict()` function with `interval="confidence"`. It is based on the covariance matrix of parameters, extracted via `vcov()` method in R (discussed in Section \@ref(uncertaintyRegressionCov)). Note that the interval can be produced not only for the in-sample value, but for the holdout as well. Here is an example with `alm()` function (Figure \@ref(fig:costModelConfidenceInterval)):
```{r costModelConfidenceInterval, fig.cap="Fitted values and confidence interval for the costs model."}
costsModelSLRCI <- predict(costsModelSLR, interval="confidence")
plot(costsModelSLRCI, main="",
     xlab="Observation", ylab="Overall Costs")
```

It is hard to read this graph because the interval is narrow, and the actual values in it do not bring much value. But if we zoom in to a specific observation, it would be more readable (Figure \@ref(fig:costModelConfidenceIntervalZoom)):

```{r costModelConfidenceIntervalZoom, fig.cap="Fitted values and confidence interval for the costs model."}
# Produce the predicted value and the confidence interval
costsModelSLRCI200 <- predict(costsModelSLR,
                              interval="confidence",
                              newdata=data.frame(materials=200))
# Plot the material costs vs fitted
plot(200, costsModelSLRCI200$mean,
     xlab="Material costs", ylab="Overall costs",
     ylim=c(costsModelSLRCI200$lower, costsModelSLRCI200$upper),
     pch=16)
# Add confidence interval
abline(h=costsModelSLRCI200$upper, col="grey", lwd=2, lty=2)
abline(h=costsModelSLRCI200$lower, col="grey", lwd=2, lty=2)
```

What the image in Figure \@ref(fig:costModelConfidenceIntervalZoom) shows is that according to the estimated regression, the expected overall costs for the project where the material costs are £200k are `r round(costsModelSLRCI200$mean, 2)`. But given the uncertainty of the estimates of parameters, the true expected costs should lie between `r round(costsModelSLRCI200$lower, 2)` and `r round(costsModelSLRCI200$upper, 2)` in 95% if we repeat the confidence interval construction many times.

Another way of visualising the confidence interval is on the actuals vs fitted plot:
```{r costModelConfidenceIntervalAvsF, fig.cap="Actuals vs Fitted and confidence interval for the costs model."}
plot(fitted(costsModelSLR), actuals(costsModelSLR),
     xlab="Fitted",ylab="Actuals")
abline(a=0,b=1,col=5,lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRCI$lower[order(fitted(costsModelSLR))], 
      col=3, lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRCI$upper[order(fitted(costsModelSLR))], 
      col=3, lwd=2)
```

Figure \@ref(fig:costModelConfidenceIntervalAvsF) demonstrates the actuals vs fitted plot, together with the 95% confidence interval around the line, demonstrating where the line would be expected to be in 95% of the cases if we re-estimate the model many times. We also see that the uncertainty of the regression line is lower in the middle of the data, but expands in the tails. Conceptually, this happens because the regression line, estimated via OLS, always passes through the average point of the data $(\bar{x},\bar{y})$ and the variability in this point is lower than the variability in the tails.


### Prediction interval {#uncertaintyRegressionLinePrediction}
In case of prediction interval, we are interested in understanding where the actual values lie, not the expected one. For our example, that would mean that we want to know the span of overall costs for projects that have material costs of 200 in the x% of the cases (e.g. 95%).

The construction of the interval relies on the formula, which is very similar to the one used for the confidence interval:
\begin{equation}
    y_j \in (\hat{y}_j + z_{\alpha/2} s_y^2, \hat{y}_j + z_{1-\alpha/2} s_y^2),
    (\#eq:predictionIntervalRegression)
\end{equation}
where $s_y^2$ is the standard deviation of the actual values, conditional on the values of the explanatory variables. Its derivation is shown in Subsection \@ref(uncertaintyRegressionLineMaths). The main difference with the confidence interval is that we need to rely on the assumption about the error term in the model.

::: remark
In the formula above, the important assumption is that the error term follows the Normal distribution. We can no longer rely on the CLT, because it only applies to the estimates of parameters. If we cannot assume that the error term follows the Normal distribution, we would need to use either a different one, or to construct the interval using some non-parametric methods.
:::

In R, the interval construction can be done via the very same `predict()` function with `interval="prediction"`:
```{r}
costsModelSLRPI <- predict(costsModelSLR, interval="prediction")
```

Based on this, we can produce an image similar to \@ref(fig:costModelConfidenceInterval) and \@ref(fig:costModelConfidenceIntervalAvsF).

```{r costModelPI, fig.cap="Fitted values and prediction interval for the stopping distance model.", echo=FALSE}
par(mfcol=c(1,2))
plot(costsModelSLRPI, ylab="Values", xlab="Observations",
     main="Actuals and Fitted over observations")
plot(fitted(costsModelSLR), actuals(costsModelSLR),
     xlab="Fitted",ylab="Actuals",main="Actuals vs Fitted",
     ylim=range(costsModelSLRPI$lower,costsModelSLRPI$upper))
abline(a=0,b=1,col=5,lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRPI$lower[order(fitted(costsModelSLR))], 
      col=3, lty=2, lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRPI$upper[order(fitted(costsModelSLR))], 
      col=3, lty=2, lwd=2)
```

Figure \@ref(fig:costModelPI) shows the prediction interval for values over observations and for actuals vs fitted. While the first image is hard to read, we can see that the prediction interval is wider than the confidence one. The second image is more informative, showing that there are some points lying outside of the 95% prediction interval (there are 2 observations outside it). The statistical principles guarantee that asymptotically, if we increase the sample size and repeat the experiment many times, we would have 95% of observations lying inside the interval.

In forecasting, prediction interval has a bigger importance than the confidence interval. This is because we are typically interested in capturing the uncertainty about the observations, not about the estimate of a line. Typically, the prediction interval would be constructed for some holdout data, which we did not have at the model estimation phase. In the example with costs, we could see what would happen if the material costs were, for example, £200k:

```{r costModelForecast, fig.cap="Forecast of the overall costs given the material costs of 200."}
costsModelSLRForecast <- predict(costsModelSLR,newdata=data.frame(materials=200),
                                    interval="prediction")
plot(costsModelSLRForecast)
```

Figure \@ref(fig:costModelForecast) shows the point forecast (the expected overall costs of a project if the material ones are £200k) and the 95% prediction interval (we expect that in 95% of the cases, the overall costs would be between `r paste(round(c(costsModelSLRForecast$lower,costsModelSLRForecast$upper),3),collapse=" and ")` thousands of pounds.


### Derivations of the variance in regression {#uncertaintyRegressionLineMaths}
The uncertainty of the regression line builds upon the uncertainty of parameters and can be measured via the conditional variance using the formula:
\begin{equation}
    \mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j}) ,
    (\#eq:regressionLineUncertaintyVariance01)
\end{equation}
which after some simplifications leads to:
\begin{equation}
    \mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \sum_{l=0}^{k-1} \mathrm{V}(b_j) x^2_{l,j} + 2 \sum_{l=1}^{k-1} \sum_{i=0}^{l-1}  \mathrm{cov}(b_i,b_l) x_{i,j} x_{l,j} ,
    (\#eq:regressionLineUncertaintyVariance02)
\end{equation}
where $x_{0,j}=1$. Alternatively, this can be calculated using the compact form of the multiple regression model \@ref(eq:MLRFormulaCompacter):
\begin{equation*}
    y_j = \mathbf{x}'_j \boldsymbol{\beta} + \epsilon_j .
\end{equation*}
the fitted values for which are:
\begin{equation*}
    \hat{y_j} = \mathbf{x}'_j \boldsymbol{\beta} .
\end{equation*}
Taking the variance of the fitted in that form would involve the covariance matrix of parameters from Section \@ref(uncertaintyRegressionCov):
\begin{equation*}
    \mathrm{V}(\hat{y_j}| \mathbf{x}_j) = \mathbf{x}'_j \mathrm{V}({\boldsymbol{b}}) \mathbf{x}_j.
\end{equation*}
In any case, we see that the variance of the regression line relies on the variances and covariances of parameters. It can then be used in the construction of the confidence interval for the regression line. Given that each estimate of parameter $b_i$ will follow normal distribution with a fixed mean and variance due to CLT (Section \@ref(CLT)), the predicted value $\hat{y}_j$ will follow normal distribution as well. This is because the multiplication of the Normal distribution by a number (the value of an explanatory variable) gives the Normal distribution as well, and the addition of Normal distribution is another one, but with different parameters. So, we can say that:
\begin{equation*}
    \mu_j \sim \mathcal{N}(\hat{y}_j, \mathrm{V}(\hat{y}_j| \mathbf{x}_j))
\end{equation*}
We used this property to derive the formula for the confidence interval around the line.

If we are interested in the uncertainty of the observations, we can refer to prediction interval. In this case we rely on the Normality assumption for the actual values themselves:
\begin{equation*}
    y_j = (\hat{y}_j + \epsilon_j) \sim \mathcal{N}(\hat{y}_j, s_y^2) ,
\end{equation*}
because we assume that $\epsilon_j \sim \mathcal{N}(0, {\sigma}^2)$, where ${\sigma}^2$ is the variance of the error term and $s_y^2 = \mathrm{V}(y_j| \mathbf{x}_j)$ is the variance of the response variable conditional on the values of the explanatory variables. The latter can be calculated as:
\begin{equation}
    \mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} + e_j) ,
    (\#eq:regressionLineUncertaintyVariance03)
\end{equation}
which can be simplified to (if assumptions of regression model hold, see Section \@ref(assumptions)):
\begin{equation}
    \mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(\hat{y}_j | \mathbf{x}_j) + \hat{\sigma}^2,
    (\#eq:regressionLineUncertaintyVariance04)
\end{equation}
where the variance $\mathrm{V}(\hat{y}_j | \mathbf{x}_j)$ was calculated above for the confidence interval in formula \@ref(eq:regressionLineUncertaintyVariance02). Given that the variance \@ref(eq:regressionLineUncertaintyVariance04) is larger than the variance \@ref(eq:regressionLineUncertaintyVariance02), the prediction interval will always be wider than the confidence one.




## Hypothesis testing {#uncertaintyRegressionHypothesis}
Another way to look at the uncertainty of parameters is to test a statistical hypothesis. As it was discussed in Section \@ref(hypothesisTesting), I personally think that hypothesis testing is a less useful instrument for these purposes than the confidence interval and that it might be misleading in some circumstances. Nonetheless, it has its merits and can be helpful if an analyst knows what they are doing. In order to test the hypothesis, we need to follow the procedure, described in Section \@ref(hypothesisTesting).

### Regression parameters
The classical hypotheses for the parameters are formulated in the following way:
\begin{equation}
    \begin{aligned}
        \mathrm{H}_0: \beta_i = 0 \\
        \mathrm{H}_1: \beta_i \neq 0
    \end{aligned} .
    (\#eq:regressionHypothesis01)
\end{equation}
This formulation of hypotheses comes from the idea that we want to check if the effect estimated by the regression is indeed there (i.e. statistically significantly different from zero). Note however, that as in any other hypothesis testing, if you fail to reject the null hypothesis, this only means that you do not know, we do not have enough evidence to conclude anything. This **does not mean** that there is no effect and that the respective variable can be removed from the model. In case of simple linear regression, the null and alternative hypothesis can be represented graphically as shown in Figure \@ref(fig:costModelHypotheses).

```{r costModelHypotheses, fig.cap="Graphical presentation of null and alternative hypothesis in regression context", echo=FALSE}
par(mfcol=c(1,2))
plot(cars$speed, cars$dist,
     xlab="Speed", ylab="Stopping Distance", main=TeX("H$_0$: $\\beta_i = 0$"))
abline(h=mean(cars$dist),col="red")
plot(cars$speed, cars$dist,
     xlab="Speed", ylab="Stopping Distance", main=TeX("H$_1$: $\\beta_i \\neq 0$"))
abline(costsModelSLR,col="blue")
```

The graph on the left in Figure \@ref(fig:costModelHypotheses) demonstrates how the true model could look if the null hypothesis was true - it would be just a straight line, parallel to x-axis. The graph on the right demonstrates the alternative situation, when the parameter is not equal to zero. We do not know the true model, and hypothesis testing does not tell us, whether the hypothesis is true or false, but if we have enough evidence to reject H$_0$, then we might conclude that we see an effect of one variable on another in the data. Note, as discussed in Section \@ref(hypothesisTesting), the null hypothesis is always wrong, and it will inevitably be rejected with the increase of sample size.

Given the discussion in the previous subsection, we know that the parameters of regression model will follow normal distribution, as long as all [assumptions](#assumptions) are satisfied (including those for [CLT](#CLT)). We also know that because the standard errors of parameters are estimated, we need to use Student's distribution, which takes the uncertainty about the variance into account. Based on this, we can say that the following statistics will follow t with $n-k$ degrees of freedom:
\begin{equation}
    \frac{b_i - 0}{s_{b_i}} \sim t(n-k) .
    (\#eq:regressionHypothesisTest01)
\end{equation}
After calculating the value and comparing it with the critical t-value on the selected significance level or directly comparing p-value based on \@ref(eq:regressionHypothesisTest01) with the significance level, we can make conclusions about the hypothesis.

The context of regression provides a great example, why we never accept hypothesis and why in the case of "Fail to reject H$_0$", we should not remove a variable (unless we have more fundamental reasons for doing that). Consider an example, where the estimated parameter $b_1=0.5$, and its standard error is $s_{b_1}=1$, we estimated a simple linear regression on a sample of 30 observations, and we want to test, whether the parameter in the population is zero (i.e. hypothesis \@ref(eq:regressionHypothesis01)) on 1% significance level. Inserting the values in formula \@ref(eq:regressionHypothesisTest01), we get: 
\begin{equation*}
    \frac{|0.5 - 0|}{1} = 0.5,
\end{equation*}
with the critical value for two-tailed test of $t_{0.01}(30-2)\approx 2.76$. Comparing t-value with the critical one, we would conclude that we fail to reject H$_0$ and thus the parameter is not statistically different from zero. But what would happen if we check another hypothesis:
\begin{equation*}
    \begin{aligned}
        \mathrm{H}_0: \beta_1 = 1 \\
        \mathrm{H}_1: \beta_1 \neq 1
    \end{aligned} .
\end{equation*}
The procedure is the same, the calculated t-value is:
\begin{equation*}
    \frac{|0.5 - 1|}{1} = 0.5,
\end{equation*}
which leads to exactly the same conclusion as before: on 1% significance level, we fail to reject the new H$_0$, so the value is not distinguishable from 1. So, which of the two is correct? The correct answer is "we do not know". The non-rejection region just tells us that uncertainty about the parameter is so high that it also include the value of interest (0 in case of the classical regression analysis). If we constructed the confidence interval for this problem, we would not have such confusion, as we would conclude that on 1% significance level the true parameter lies in the region $(-2.26, 3.26)$ and can be any of these numbers.

In R, if you want to test the hypothesis for parameters, I would recommend using `lm()` function for regression:
```{r}
lmSpeedDistance <- lm(dist~speed,cars)
summary(lmSpeedDistance)
```

This output tells us that when we consider the parameter for the variable speed, we reject the standard H$_0$ on the pre-selected 1% significance level (comparing the level with p-value in the last column of the output). Note that we should first select the significance level and only then conduct the test, otherwise we would be bending reality for our needs.

### Regression line
Finally, in regression context, we can test another hypothesis, which becomes useful, when a lot of parameters of the model are very close to zero and seem to be insignificant on the selected level:
\begin{equation}
    \begin{aligned}
        \mathrm{H}_0: \beta_1 = \beta_2 = \dots = \beta_{k-1} = 0 \\
        \mathrm{H}_1: \beta_1 \neq 0 \vee \beta_2 \neq 0 \vee \dots \vee \beta_{k-1} \neq 0
    \end{aligned} ,
    (\#eq:regressionHypothesis02)
\end{equation}
which translates into normal language as "H$_0$: all parameters (except for intercept) are equal to zero; H$_1$: at least one parameter is not equal to zero". This hypothesis is only needed, when you have a model with many statistically insignificant variables and want to see if the model explains anything. This is done using F-test, which can be calculated based on sums of squares:
\begin{equation*}
    F = \frac{ SSR / (k-1)}{SSE / (n-k)} \sim F(k-1, n-k) ,
\end{equation*}
where the sums of squares are divided by their degrees of freedom. The test is conducted in the similar manner as any other test (see Section \@ref(hypothesisTesting)): after choosing the significance level, we can either calculate the critical value of F for the specified degrees of freedom, or compare it with the p-value from the test to make a conclusion about the null hypothesis. 

This hypothesis is not very useful, when the parameter are significant and coefficient of determination is high. It only becomes useful in difficult situations of poor fit. The test on its own does not tell if the model is adequate or not. And the F value and related p-value is not comparable with respective values of other models. Graphically, this test checks, whether in the true model the slope of the straight line on the plot of actuals vs fitted is different from zero. An example with the same stopping distance model is provided in Figure \@ref(fig:speedDistanceHypothesesF).

```{r speedDistanceHypothesesF, fig.cap="Graphical presentation of F test for regression model.", echo=FALSE}
testModel <- alm(actuals(costsModelSLR)~fitted(costsModelSLR))
plot(fitted(costsModelSLR), actuals(costsModelSLR),
     xlab="Fitted", ylab="Actuals")
abline(testModel,col="blue")
abline(h=mean(actuals(costsModelSLR)),col="red")
```

What the test is tries to get insight about, is whether in the true model the blue line coincides with the red line (i.e. the slope is equal to zero, which is only possible, when all parameters are zero). If we have enough evidence to reject the null hypothesis, then this means that the slopes are different on the selected significance level.

Here is an example with the speed model discussed above with the significance level of 1%:

```{r}
lmSpeedDistanceF <- summary(lmSpeedDistance)$fstatistic
# F value
lmSpeedDistanceF[1]
# F critical
qf(0.99,lmSpeedDistanceF[2],lmSpeedDistanceF[3])
# p-value from the test
1-pf(lmSpeedDistanceF[1],lmSpeedDistanceF[2],lmSpeedDistanceF[3])
```

In the output above, the critical value is lower than the calculated, so we can reject the H$_0$, which means that there is something in the model that explains the variability in the variable `dist`. Alternatively, we could focus on p-value. We see that the it is lower than the significance level of 1%, so we reject the H$_0$ and come to the same conclusion as above.
