# Uncertainty in regression {#uncertaintyRegression}

Coming back to the example of properties costs from Chapter \@ref(linearRegression), consider a simple linear regression of material vs overall costs, fit to a sub-sample of data. If we fit the line to the first 25 observations we would get a different model than when fitting it to the sample of 50 observations. Figure \@ref(fig:scatterCostsUncertainty) depicts these two situations, where the blue dashed red line corresponds to the bigger sample (fit to the empty red circles in the plot), while the blue solid line represents the model for the smaller sample (blue solid circles).

```{r scatterCostsUncertainty, fig.cap="Material vs overall costs and two regression lines.", echo=FALSE}
costsModelSLRSmall <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:25)
costsModelSLRLarge <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:50)
plot(overall~materials, SBA_Chapter_11_Costs, subset=1:50,
     xlab="Material costs", ylab="Overall costs",
     col=3, pch=1, lwd=2)
points(SBA_Chapter_11_Costs$materials[1:25], SBA_Chapter_11_Costs$overall[1:25],
       col=5, pch=20, lwd=2)
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(costsModelSLRLarge, col=3, lty=2)
text(250,380,paste0(c("overall = ",round(coef(costsModelSLRLarge)[1],2)," + ",
                      round(coef(costsModelSLRLarge)[2],2)," materials + e"),collapse=""), col=3)
abline(costsModelSLRSmall, col=5)
text(215,557,paste0(c("overall = ",round(coef(costsModelSLRSmall)[1],2)," + ",
                      round(coef(costsModelSLRSmall)[2],2)," materials + e"),collapse=""), col=5)
legend("topleft", legend=c("50 observations","25 observations"),
       lwd=2, col=c(3,5), lty=c(2,1))
```

We see that the two lines differ both in terms of their intercepts and their slopes. So, which one of them is correct?

The general view of a spherical statistician in vacuum is that the larger the sample is, the better the model is. While this is correct in general, we would argue that none of the two models is correct. This is because they are both just estimates of the true model (Section \@ref(modelsMethods) on a sample of data and they both inevitably inherit the uncertainty of the data. This means that whatever regression model we estimate on a sample of data, it will always be incorrect in the sense that it can only be considered as an approximation of the truth.

More importantly, the line will always change when the model is fit to the new sample of data, even if we add only one observation. This is because on different samples, we get different estimates of parameters, which comes from one of the fundamental sources of uncertainty (the second one discussed in Section \@ref(sourcesOfUncertainty)). This means that the estimates of parameters are random and might have some distribution of their own.

Exactly the same effect will be observed if we collect a different random sample from the same population, for example, if we worked for a different company that does similar business and collects the same data. In that case, the parameters would be random again, having some sort of a distribution.

To see this point clearer, we fit the same simple regression model to randomly selected subsamples of the original dataset many times and collect the estimates of parameters to plot histograms and see their distribution. This empirical way of capturing uncertainty is called "bootstrap" and we will discuss it in more detail in Chapter \@ref(SectionNeededHere). The R code below shows how this can be done using the `coefbootstrap()` function from the `greybox` package (which implements a simple "Case Resampling" technique):

```{r}
# Fit the model
costsModelSLR <- alm(overall ~ materials, data=SBA_Chapter_11_Costs)
# Set random seed for reproducibility
set.seed(41)
costsModelSLRBootstrap <- coefbootstrap(costsModelSLR)
```

Based on that we can plot the histograms of the estimates of parameters as shown in Figure \@ref(fig:costsModelSLRBootstrap).

```{r costsModelSLRBootstrap, fig.cap="Distribution of bootstrapped parameters of a regression model"}
par(mfcol=c(1,2))
# First histogram
hist(costsModelSLRBootstrap$coefficients[,1],
     xlab="Intercept", main="")
# The estimated coefficient
abline(v=coef(costsModelSLR)[1], col="darkred", lwd=2)
# Second histogram
hist(costsModelSLRBootstrap$coefficients[,2],
     xlab="Slope", main="")
# The estimated coefficient
abline(v=coef(costsModelSLR)[2], col="darkred", lwd=2)
```

Figure \@ref(fig:costsModelSLRBootstrap) shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution and demonstrate the variability of the estimates around the specific values. The vertical lines on the plots show the estimated values of parameters, which hopefully should be close to the ones in the true model. If we were we to repeat this experiment thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections \@ref(CLT) and \@ref(assumptions)).

This example demonstrates that any model applied to the data will always have inherited uncertainty, which should be taken into account. In decision making, you should not rely on just a number from your model, and you should get a general understanding of what the variability about the estimates of parameters is. This has its implications for both parameters analysis and forecasting.


## Confidence intervals {#uncertaintyRegressionCI}
Given that the estimates of parameters have some uncertainty associated with them, as discussed in the introduction to this Chapter, it makes sense to capture that uncertainty so that decision makers can have a better understanding about the observed effects. The simplest way to do that is to construct confidence intervals in a similar way to what we discussed in Section \@ref(confidenceInterval). Visually, this process is shown in Figure \@ref(fig:costsModelSLRCI), which continues the example we discussed before.

```{r costsModelSLRCI, fig.cap="Parameter uncertainty in the estimated model", echo=FALSE}
par(mfcol=c(1,2))
# First histogram
hist(costsModelSLRBootstrap$coefficients[,1],
     xlab="Intercept", main="", probability=TRUE,
     xlim=range(costsModelSLRBootstrap$coefficients[,1])*c(0.9,1.1))
# The estimated coefficient
# abline(v=coef(costsModelSLR)[1], col=3, lwd=2)
# lines(density(costsModelSLRBootstrap$coefficients[,1]), col=5, lwd=2)
xSequence <- mean(costsModelSLRBootstrap$coefficients[,1])+seq(-5,5,0.1)*sd(costsModelSLRBootstrap$coefficients[,1])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelSLRBootstrap$coefficients[,1]),
            sd(costsModelSLRBootstrap$coefficients[,1])), col=5, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelSLRBootstrap$coefficients[,1]),
               sd(costsModelSLRBootstrap$coefficients[,1])), col=5, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelSLRBootstrap$coefficients[,1]),
             sd(costsModelSLRBootstrap$coefficients[,1]))),
     rep(0.03,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelSLRBootstrap$coefficients[,1]),
                        sd(costsModelSLRBootstrap$coefficients[,1])),2), pos=c(2,4))

# Second histogram
hist(costsModelSLRBootstrap$coefficients[,2],
     xlab="Slope", main="", probability=TRUE)
# The estimated coefficient
# abline(v=coef(costsModelSLR)[2], col=3, lwd=2)
# lines(density(costsModelSLRBootstrap$coefficients[,2]), col=5, lwd=2)
xSequence <- mean(costsModelSLRBootstrap$coefficients[,2])+seq(-5,5,0.1)*sd(costsModelSLRBootstrap$coefficients[,2])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelSLRBootstrap$coefficients[,2]),
            sd(costsModelSLRBootstrap$coefficients[,2])), col=5, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelSLRBootstrap$coefficients[,2]),
               sd(costsModelSLRBootstrap$coefficients[,2])), col=5, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelSLRBootstrap$coefficients[,2]),
             sd(costsModelSLRBootstrap$coefficients[,2]))),
     rep(6,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelSLRBootstrap$coefficients[,2]),
                        sd(costsModelSLRBootstrap$coefficients[,2])),2), pos=c(2,4))
```

Figure \@ref(fig:costsModelSLRCI) demonstrates the bootstrapped distributions of parameters (as before), together with the Normal probability density functions on top of them and vertical lines at the tails that represent the confidence bounds. The idea behind this is that due to the Central Limit Theorem (Section \@ref(CLT)) we can assume that estimates of parameters follow the Normal distribution with some mean and variance, and based on that we can get the quantiles, thus inferring, for example, that the true Intercept will lie between `r round(qnorm(c(0.025), mean(costsModelSLRBootstrap$coefficients[,1]), sd(costsModelSLRBootstrap$coefficients[,1])),2)` and `r round(qnorm(c(0.975), mean(costsModelSLRBootstrap$coefficients[,1]), sd(costsModelSLRBootstrap$coefficients[,1])),2)` in the 95% of the cases, if we repeat the resampling experiment many times. So, the interpretation of the confidence interval is exactly the same as in the simpler case discussed in the Section \@ref(confidenceInterval).




In order to take this uncertainty into account, we could construct confidence intervals for the estimates of parameters, using the principles discussed in Section \@ref(confidenceInterval). This way we would hopefully have some idea about the uncertainty of the parameters, and not just rely on average values. If we assume that [CLT](#CLT) holds, we could use the t statistics for the calculation of the quantiles of distribution (we need to use t because we do not know the variance of estimates of parameters). But in order to do that, we need to have variances of estimates of parameters. One of possible ways of getting them would be the bootstrap used in the example above. However, this is a computationally expensive operation, and there is a more efficient procedure, which however only works with linear regression models either estimated using OLS or via Maximum Likelihood Estimation assuming Normal distribution (see Section \@ref(likelihoodApproach)). In these conditions the covariance matrix of parameters can be calculated using the following formula:
\begin{equation}
    \mathrm{V}(\hat{\boldsymbol{\beta}}) = \frac{1}{n-k} \sum_{j=1}^n e_j^2 \times \left(\mathbf{X}' \mathbf{X}\right)^{-1}.
    (\#eq:MLRcovarianceMatrix)
\end{equation}
This matrix will contain variances of parameters on the diagonal and covariances between the parameters on off-diagonals. In this specific case, we only need the diagonal elements. We can take square root of them to obtain standard errors of parameters, which can then be used to construct confidence intervals for each parameter $i$ via:
\begin{equation}
    \beta_i \in (b_i + t_{\alpha/2}(n-k) s_{b_i}, b_i + t_{1-\alpha/2}(n-k) s_{b_i}),
    (\#eq:MLRParameterInterval)
\end{equation}
where $s_{b_i}$ is the standard error of the parameter $b_i$. All modern software does all these calculations automatically, so we do not need to do them manually. Here is an example:

```{r}
vcov(costsModelSLR)
```
This is the covariance matrix of parameters, the diagonal elements of which are then used in the `confint()` method:

```{r}
confint(costsModelSLR)
```
The confidence interval for speed above shows, for example, that if we repeat the construction of interval many times, the true value of parameter speed will lie in 95% of cases between 3.08 and 4.78. This gives an idea about the real effect in the population. We can also present all of this in the following summary (this is based on the `alm()` model, the other functions will produce different summaries):
```{r}
summary(costsModelSLR)
```

This summary provide all the necessary information about the estimates of parameters: their mean values in the column "Estimate", their standard errors in "Std. Error", the bounds of confidence interval and finally a star if the interval does not contain zero. This typically indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect really exists.


## Hypothesis testing {#uncertaintyRegressionHypothesis}
Another way to look at the uncertainty of parameters is to test a statistical hypothesis. As it was discussed in Section \@ref(hypothesisTesting), I personally think that hypothesis testing is a less useful instrument for these purposes than the confidence interval and that it might be misleading in some circumstances. Nonetheless, it has its merits and can be helpful if an analyst knows what they are doing. In order to test the hypothesis, we need to follow the procedure, described in Section \@ref(hypothesisTesting).

### Regression parameters
The classical hypotheses for the parameters are formulated in the following way:
\begin{equation}
    \begin{aligned}
        \mathrm{H}_0: \beta_i = 0 \\
        \mathrm{H}_1: \beta_i \neq 0
    \end{aligned} .
    (\#eq:regressionHypothesis01)
\end{equation}
This formulation of hypotheses comes from the idea that we want to check if the effect estimated by the regression is indeed there (i.e. statistically significantly different from zero). Note however, that as in any other hypothesis testing, if you fail to reject the null hypothesis, this only means that you do not know, we do not have enough evidence to conclude anything. This **does not mean** that there is no effect and that the respective variable can be removed from the model. In case of simple linear regression, the null and alternative hypothesis can be represented graphically as shown in Figure \@ref(fig:speedDistanceHypotheses).

```{r speedDistanceHypotheses, fig.cap="Graphical presentation of null and alternative hypothesis in regression context", echo=FALSE}
par(mfcol=c(1,2))
plot(cars$speed, cars$dist,
     xlab="Speed", ylab="Stopping Distance", main=TeX("H$_0$: $\\beta_i = 0$"))
abline(h=mean(cars$dist),col="red")
plot(cars$speed, cars$dist,
     xlab="Speed", ylab="Stopping Distance", main=TeX("H$_1$: $\\beta_i \\neq 0$"))
abline(costsModelSLR,col="blue")
```

The graph on the left in Figure \@ref(fig:speedDistanceHypotheses) demonstrates how the true model could look if the null hypothesis was true - it would be just a straight line, parallel to x-axis. The graph on the right demonstrates the alternative situation, when the parameter is not equal to zero. We do not know the true model, and hypothesis testing does not tell us, whether the hypothesis is true or false, but if we have enough evidence to reject H$_0$, then we might conclude that we see an effect of one variable on another in the data. Note, as discussed in Section \@ref(hypothesisTesting), the null hypothesis is always wrong, and it will inevitably be rejected with the increase of sample size.

Given the discussion in the previous subsection, we know that the parameters of regression model will follow normal distribution, as long as all [assumptions](#assumptions) are satisfied (including those for [CLT](#CLT)). We also know that because the standard errors of parameters are estimated, we need to use Student's distribution, which takes the uncertainty about the variance into account. Based on this, we can say that the following statistics will follow t with $n-k$ degrees of freedom:
\begin{equation}
    \frac{b_i - 0}{s_{b_i}} \sim t(n-k) .
    (\#eq:regressionHypothesisTest01)
\end{equation}
After calculating the value and comparing it with the critical t-value on the selected significance level or directly comparing p-value based on \@ref(eq:regressionHypothesisTest01) with the significance level, we can make conclusions about the hypothesis.

The context of regression provides a great example, why we never accept hypothesis and why in the case of "Fail to reject H$_0$", we should not remove a variable (unless we have more fundamental reasons for doing that). Consider an example, where the estimated parameter $b_1=0.5$, and its standard error is $s_{b_1}=1$, we estimated a simple linear regression on a sample of 30 observations, and we want to test, whether the parameter in the population is zero (i.e. hypothesis \@ref(eq:regressionHypothesis01)) on 1% significance level. Inserting the values in formula \@ref(eq:regressionHypothesisTest01), we get: 
\begin{equation*}
    \frac{|0.5 - 0|}{1} = 0.5,
\end{equation*}
with the critical value for two-tailed test of $t_{0.01}(30-2)\approx 2.76$. Comparing t-value with the critical one, we would conclude that we fail to reject H$_0$ and thus the parameter is not statistically different from zero. But what would happen if we check another hypothesis:
\begin{equation*}
    \begin{aligned}
        \mathrm{H}_0: \beta_1 = 1 \\
        \mathrm{H}_1: \beta_1 \neq 1
    \end{aligned} .
\end{equation*}
The procedure is the same, the calculated t-value is:
\begin{equation*}
    \frac{|0.5 - 1|}{1} = 0.5,
\end{equation*}
which leads to exactly the same conclusion as before: on 1% significance level, we fail to reject the new H$_0$, so the value is not distinguishable from 1. So, which of the two is correct? The correct answer is "we do not know". The non-rejection region just tells us that uncertainty about the parameter is so high that it also include the value of interest (0 in case of the classical regression analysis). If we constructed the confidence interval for this problem, we would not have such confusion, as we would conclude that on 1% significance level the true parameter lies in the region $(-2.26, 3.26)$ and can be any of these numbers.

In R, if you want to test the hypothesis for parameters, I would recommend using `lm()` function for regression:
```{r}
lmSpeedDistance <- lm(dist~speed,cars)
summary(lmSpeedDistance)
```

This output tells us that when we consider the parameter for the variable speed, we reject the standard H$_0$ on the pre-selected 1% significance level (comparing the level with p-value in the last column of the output). Note that we should first select the significance level and only then conduct the test, otherwise we would be bending reality for our needs.

### Regression line
Finally, in regression context, we can test another hypothesis, which becomes useful, when a lot of parameters of the model are very close to zero and seem to be insignificant on the selected level:
\begin{equation}
    \begin{aligned}
        \mathrm{H}_0: \beta_1 = \beta_2 = \dots = \beta_{k-1} = 0 \\
        \mathrm{H}_1: \beta_1 \neq 0 \vee \beta_2 \neq 0 \vee \dots \vee \beta_{k-1} \neq 0
    \end{aligned} ,
    (\#eq:regressionHypothesis02)
\end{equation}
which translates into normal language as "H$_0$: all parameters (except for intercept) are equal to zero; H$_1$: at least one parameter is not equal to zero". This hypothesis is only needed, when you have a model with many statistically insignificant variables and want to see if the model explains anything. This is done using F-test, which can be calculated based on sums of squares:
\begin{equation*}
    F = \frac{ SSR / (k-1)}{SSE / (n-k)} \sim F(k-1, n-k) ,
\end{equation*}
where the sums of squares are divided by their degrees of freedom. The test is conducted in the similar manner as any other test (see Section \@ref(hypothesisTesting)): after choosing the significance level, we can either calculate the critical value of F for the specified degrees of freedom, or compare it with the p-value from the test to make a conclusion about the null hypothesis. 

This hypothesis is not very useful, when the parameter are significant and coefficient of determination is high. It only becomes useful in difficult situations of poor fit. The test on its own does not tell if the model is adequate or not. And the F value and related p-value is not comparable with respective values of other models. Graphically, this test checks, whether in the true model the slope of the straight line on the plot of actuals vs fitted is different from zero. An example with the same stopping distance model is provided in Figure \@ref(fig:speedDistanceHypothesesF).

```{r speedDistanceHypothesesF, fig.cap="Graphical presentation of F test for regression model.", echo=FALSE}
testModel <- alm(actuals(costsModelSLR)~fitted(costsModelSLR))
plot(fitted(costsModelSLR), actuals(costsModelSLR),
     xlab="Fitted", ylab="Actuals")
abline(testModel,col="blue")
abline(h=mean(actuals(costsModelSLR)),col="red")
```

What the test is tries to get insight about, is whether in the true model the blue line coincides with the red line (i.e. the slope is equal to zero, which is only possible, when all parameters are zero). If we have enough evidence to reject the null hypothesis, then this means that the slopes are different on the selected significance level.

Here is an example with the speed model discussed above with the significance level of 1%:

```{r}
lmSpeedDistanceF <- summary(lmSpeedDistance)$fstatistic
# F value
lmSpeedDistanceF[1]
# F critical
qf(0.99,lmSpeedDistanceF[2],lmSpeedDistanceF[3])
# p-value from the test
1-pf(lmSpeedDistanceF[1],lmSpeedDistanceF[2],lmSpeedDistanceF[3])
```

In the output above, the critical value is lower than the calculated, so we can reject the H$_0$, which means that there is something in the model that explains the variability in the variable `dist`. Alternatively, we could focus on p-value. We see that the it is lower than the significance level of 1%, so we reject the H$_0$ and come to the same conclusion as above.


## Regression line uncertainty {#uncertaintyRegressionLine}
Given the uncertainty of estimates of parameters, the regression line itself and the points around it will be uncertain. This means that in some cases we should not just consider the predicted values of the regression $\hat{y}_j$, but also the uncertainty around them.

The uncertainty of the regression line builds upon the uncertainty of parameters and can be measured via the conditional variance in the following way:
\begin{equation}
    \mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j}) ,
    (\#eq:regressionLineUncertaintyVariance01)
\end{equation}
which after some simplifications leads to:
\begin{equation}
    \mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \sum_{l=0}^{k-1} \mathrm{V}(b_j) x^2_{l,j} + 2 \sum_{l=1}^{k-1} \sum_{i=0}^{l-1}  \mathrm{cov}(b_i,b_l) x_{i,j} x_{l,j} ,
    (\#eq:regressionLineUncertaintyVariance02)
\end{equation}
where $x_{0,j}=1$. As we see, the variance of the regression line involves variances and covariances of parameters. This variance can then be used in the construction of the confidence interval for the regression line. Given that each estimate of parameter $b_i$ will follow normal distribution with a fixed mean and variance due to [CLT](#CLT), the predicted value $\hat{y}_j$ will follow normal distribution as well. This can be used in the construction of the confidence interval, in a manner similar to the one discussed in Section \@ref(confidenceInterval):
\begin{equation}
    \mu_j \in (\hat{y}_j + t_{\alpha/2}(n-k) s_{\hat{y}_j}, \hat{y}_j + t_{1-\alpha/2}(n-k) s_{\hat{y}_j}),
    (\#eq:confidenceIntervalRegression)
\end{equation}
where $s_{\hat{y}_j}=\sqrt{\mathrm{V}(\hat{y}_j| \mathbf{x}_j)}$.

In R, this interval can be constructed via the function `predict()` with `interval="confidence"`. It is based on the covariance matrix of parameters, extracted via `vcov()` method in R (it was discussed in a previous subsection). Note that the interval can be produced not only for the in-sample value, but for the holdout as well. Here is an example with `alm()` function:
```{r speedDistanceConfidenceInterval, fig.cap="Fitted values and confidence interval for the stopping distance model."}
costsModelSLRCI <- predict(costsModelSLR,interval="confidence")
plot(costsModelSLRCI, main="",
     xlab="Observation", ylab="Distance")
```

The same fitted values and interval can be presented differently on the actuals vs fitted plot:
```{r speedDistanceConfidenceIntervalAvsF, fig.cap="Actuals vs Fitted and confidence interval for the stopping distance model."}
plot(fitted(costsModelSLR),actuals(costsModelSLR),
     xlab="Fitted",ylab="Actuals")
abline(a=0,b=1,col="darkblue",lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRCI$lower[order(fitted(costsModelSLR))], 
      col="darkred", lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRCI$upper[order(fitted(costsModelSLR))], 
      col="darkred", lwd=2)
```

Figure \@ref(fig:speedDistanceConfidenceIntervalAvsF) demonstrates the actuals vs fitted plot, together with the 95% confidence interval around the line, demonstrating where the line would be expected to be in 95% of the cases if we re-estimate the model many times. We also see that the uncertainty of the regression line is lower in the middle of the data, but expands in the tails. Conceptually, this happens because the regression line, estimated via OLS, always passes through the average point of the data $(\bar{x},\bar{y})$ and the variability in this point is lower than the variability in the tails.

If we are not interested in the uncertainty of the regression line, but rather in the uncertainty of the observations, we can refer to prediction interval. The variance in this case is:
\begin{equation}
    \mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} + e_j) ,
    (\#eq:regressionLineUncertaintyVariance03)
\end{equation}
which can be simplified to (if assumptions of regression model hold, see Section \@ref(assumptions)):
\begin{equation}
    \mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(\hat{y}_j| \mathbf{x}_j) + \hat{\sigma}^2,
    (\#eq:regressionLineUncertaintyVariance04)
\end{equation}
where $\hat{\sigma}^2$ is the variance of the residuals $e_j$. As we see from the formula \@ref(eq:regressionLineUncertaintyVariance04), the variance in this case is larger than \@ref(eq:regressionLineUncertaintyVariance02), which will result in wider interval than the confidence one. We can use normal distribution for the construction of the interval in this case (using formula similar to \@ref(eq:confidenceIntervalRegression)), as long as we can assume that $\epsilon_j \sim \mathcal{N}(0,\sigma^2)$.

In R, this can be done via the very same `predict()` function with `interval="prediction"`:
```{r}
costsModelSLRPI <- predict(costsModelSLR,interval="prediction")
```
Based on this, we can construct graphs similar to \@ref(fig:speedDistanceConfidenceInterval) and \@ref(fig:speedDistanceConfidenceIntervalAvsF).


```{r speedDistancePI, fig.cap="Fitted values and prediction interval for the stopping distance model.", echo=FALSE}
par(mfcol=c(1,2))
plot(costsModelSLRPI, ylab="Values", xlab="Observations",
     main="Actuals and Fitted over observations")
plot(fitted(costsModelSLR),actuals(costsModelSLR),
     xlab="Fitted",ylab="Actuals",main="Actuals vs Fitted",
     ylim=range(costsModelSLRPI$lower,costsModelSLRPI$upper))
abline(a=0,b=1,col="darkblue",lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRPI$lower[order(fitted(costsModelSLR))], 
      col="darkorange", lty=2, lwd=2)
lines(sort(fitted(costsModelSLR)),
      costsModelSLRPI$upper[order(fitted(costsModelSLR))], 
      col="darkorange", lty=2, lwd=2)
```

Figure \@ref(fig:speedDistancePI) shows the prediction interval for values over observations and for actuals vs fitted. As we see, the interval is wider in this case, covering only 95% of observations (there are 2 observations outside it).

In forecasting, prediction interval has a bigger importance than the confidence interval. This is because we are typically interested in capturing the uncertainty about the observations, not about the estimate of a line. Typically, the prediction interval would be constructed for some holdout data, which we did not have at the model estimation phase. In the example with stopping distance, we could see what would happen if the speed of a car was, for example, 30mph:

```{r speedDistanceForecast, fig.cap="Forecast of the stopping distance for the speed of 30mph."}
costsModelSLRForecast <- predict(costsModelSLR,newdata=data.frame(materials=250),
                                    interval="prediction")
plot(costsModelSLRForecast)
```

Figure \@ref(fig:speedDistanceForecast) shows the point forecast (the expected stopping distance if the speed of car was 30mph) and the 95% prediction interval (we expect that in 95% of the cases, the cars will have the stopping distance between `r paste(round(c(costsModelSLRForecast$lower,costsModelSLRForecast$upper),3),collapse=" and ")` feet.
