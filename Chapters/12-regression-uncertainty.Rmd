# Uncertainty in regression {#uncertaintyRegression}

Coming back to the example of real estate costs from Chapter \@ref(linearRegression), consider a simple linear regression of material vs overall costs, fit to a sub-sample of data. If we fit the line to the first 25 observations we would get a different model than when fitting it to the sample of 50 observations. Figure \@ref(fig:scatterCostsUncertainty) depicts these two situations, where the blue dashed red line corresponds to the bigger sample (fit to the empty red circles in the plot), while the blue solid line represents the model for the smaller sample (blue solid circles).

```{r scatterCostsUncertainty, fig.cap="Material vs overall costs and two regression lines.", echo=FALSE}
costsModelMLRSmall <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:25)
costsModelMLRLarge <- lm(overall ~ materials, data=SBA_Chapter_11_Costs, subset=1:50)
plot(overall~materials, SBA_Chapter_11_Costs, subset=1:50,
     xlab="Material costs", ylab="Overall costs",
     col=2, pch=1, lwd=2)
points(SBA_Chapter_11_Costs$materials[1:25], SBA_Chapter_11_Costs$overall[1:25],
       col=4, pch=20, lwd=2)
abline(h=0, col="grey")
abline(v=0, col="grey")
abline(costsModelMLRLarge, col=2, lty=2)
text(250,380,paste0(c("overall = ",round(coef(costsModelMLRLarge)[1], 2)," + ",
                      round(coef(costsModelMLRLarge)[2], 2)," materials + e"),collapse=""), col=2)
abline(costsModelMLRSmall, col=4)
text(215,610,paste0(c("overall = ",round(coef(costsModelMLRSmall)[1], 2)," + ",
                      round(coef(costsModelMLRSmall)[2], 2)," materials + e"),collapse=""), col=4)
legend("topleft", legend=c("50 observations","25 observations"),
       lwd=2, col=c(2,4), lty=c(2,1))
```

We see that the two lines differ both in terms of their intercepts and their slopes. So, which one of them is correct?

The general view of a spherical statistician in vacuum is that the larger the sample is, the better the model is. While this is correct in general, we would argue that none of the two models is correct. This is because they are both just estimates of the true model (Section \@ref(modelsMethods) on a sample of data and they both inevitably inherit the uncertainty of the data. This means that whatever regression model we estimate on a sample of data, it will always be incorrect in the sense that it can only be considered as an approximation of the truth.

More importantly, the line will always change when the model is fit to the new sample of data, even if we add only one observation. This is because on different samples, we get different estimates of parameters, which comes from one of the fundamental sources of uncertainty (the second one discussed in Section \@ref(sourcesOfUncertainty)). This means that the estimates of parameters are random and might have some distribution of their own.

Exactly the same effect will be observed if we collect a different random sample from the same population, for example, if we worked for a different company that does similar business and collects the same data. In that case, the parameters would be random again, having some sort of a distribution.

To see this point clearer, we fit the same simple regression model to randomly selected subsamples of the original dataset many times and collect the estimates of parameters to plot histograms and see their distribution. This empirical way of capturing uncertainty is called "bootstrap" and we will discuss it in more detail in Chapter \@ref(SectionNeededHere). The R code below shows how this can be done using the `coefbootstrap()` function from the `greybox` package (which implements a simple "Case Resampling" technique):

```{r}
# Fit the model
costsModelMLR <- alm(overall~materials+size+projects+year, data=SBA_Chapter_11_Costs, loss="MSE")
# Set random seed for reproducibility
set.seed(41)
costsModelMLRBootstrap <- coefbootstrap(costsModelMLR)
```

Based on that we can plot the histograms of the estimates of parameters as shown in Figure \@ref(fig:costsModelMLRBootstrap).

```{r costsModelMLRBootstrap, fig.cap="Distribution of bootstrapped parameters of a regression model"}
par(mfcol=c(1,2))
# First histogram
hist(costsModelMLRBootstrap$coefficients[,1],
     xlab="Intercept", main="", col=17)
# The estimated coefficient
abline(v=coef(costsModelMLR)[1], col=2, lwd=2)
# Second histogram
hist(costsModelMLRBootstrap$coefficients[,2],
     xlab="Material costs", main="", col=17)
# The estimated coefficient
abline(v=coef(costsModelMLR)[2], col=2, lwd=2)
```

Figure \@ref(fig:costsModelMLRBootstrap) shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution and demonstrate the variability of the estimates around the specific values. The vertical lines on the plots show the estimated values of parameters, which hopefully should be close to the ones in the true model. If we were we to repeat this experiment thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections \@ref(CLT) and Chapter \@ref(assumptions)).

This example demonstrates that any model applied to the data will always have inherited uncertainty, which should be taken into account. In decision making, you should not rely on just a number from your model, and you should get a general understanding of what the variability about the estimates of parameters is. This has its implications for both parameters analysis and forecasting.


## Covariance matrix of parameters {#uncertaintyRegressionCov}
One of the simplest ways of capturing the uncertainty about the parameters is by calculating the covariance matrix of parameters. It is the matrix that shows the individual and joint variabilities of parameters of the model. It has the following shape:

\begin{equation}
\mathrm{V}({\boldsymbol{b}}) =
\begin{pmatrix}
\mathrm{V}(b_0) & \mathrm{cov}(b_0, b_1) & \dots & \mathrm{cov}(b_0, b_{k-1}) \\
\mathrm{cov}(b_0, b_1) & \mathrm{V}(b_1) & \dots & \mathrm{cov}(b_1, b_{k-1}) \\
\vdots & \vdots & \ddots & \vdots \\
\mathrm{cov}(b_0, b_{k-1}) & \mathrm{cov}(b_1, b_{k-1}) & \dots & \mathrm{V}(b_{k-1}) \\
\end{pmatrix} ,
(\#eq:MLRParameterCov)
\end{equation}
where ${\boldsymbol{b}}$ is the vector of all the estimated parameters. The covariance matrix above is similar to the one we discussed at the end of Section \@ref(dataAnalysisNumerical), equation \@ref(eq:CovarMat), but it is now applied to the estimates of parameters of the model instead of some random variables. The diagonal elements of this matrix contain variances of parameters, showing how each estimate of parameter varies in sample, while the off-diagonal ones contain the covariances, capturing the joint variability of parameters. The square roots of the diagonal of that matrix would give the so called "Standard errors" of parameters (the standard deviations). These are typically used in further analysis.


### Bootstrap-based matrix
There are different ways how we could calculate this matrix. We could use the bootstrap as we did in the introduction to this section, producing many estimates of the parameters and then calculating the matrix. R has all the necessary functions for that:

```{r}
var(costsModelMLRBootstrap$coefficients) |> round(4)
```

The values themselves do not tell us much about the model and parameters, so to better understand them, we can calculate the correlation matrix to see what is the linear relation between the estimates of parameters:

```{r}
var(costsModelMLRBootstrap$coefficients) |>
    cov2cor() |> round(4)
```

We see, for example, that the correlation between the intercept and the parameter for the `year` is close to -1, which implies that the two have a strong negative linear relation. This means that when we draw lines through the clouds of points on samples of data (year vs overall costs), the higher intercept implies that the line should have a lower slope, i.e. the line exhibits rotations around the middle of the data for these two variables. Still, this does not provide much useful information, but the matrix itself can be used for further derivations (see next Section).


### Analytical solution
The alternative way of calculating the covariance matrix is using a formula. This is not universally available, and works only in case of OLS and a couple of other estimation methods (such as Maximisation of the Likelihood in case of the Normal distribution, see Chapter \@ref(likelihoodApproach)). The following formula is derived for OLS, relies on several important assumptions (see derivations in Subsection \@ref(uncertaintyRegressionCovMaths)), and would not produce adequate results in case of other estimators:
\begin{equation}
\mathrm{V}({\boldsymbol{b}}) = \frac{1}{n-k} \sum_{j=1}^n e_j^2 \times \left(\mathbf{X}' \mathbf{X}\right)^{-1}.
(\#eq:MLRcovarianceMatrix)
\end{equation}
where $\mathbf{X}$ is the matrix of explanatory variables from equation \@ref(eq:MLRMatrices) and $e_j$ is the residual of the model on the observation $j$. One way to understand what this formula does, is to keep in mind that $\mathbf{X}' \mathbf{X}$ will produce a matrix, similar to the covariance matrix of explanatory variables, so roughly the sample variance of the error term is then distributed across the variances and covariances of the explanatory variables, producing the corresponding variances and covariances of the parameters. The function `vcov()` in R, uses exactly this formula in case of the OLS/Likelihood estimation, returning the following:
```{r}
vcov(costsModelMLR) |> round(4)
```

We can see that the values of this covariance matrix differ from the ones produced via bootstrap. This is expected because different methods will always give different results. The analytical solution via \@ref(eq:MLRcovarianceMatrix) is more convenient and easier to calculate, but it relies on more assumptions than the bootstrapped one (see derivations below), the most important of them being that *the model is correctly specified* (i.e. we deal with the true model applied to a sample of data). If this is violated, the estimates of the covariance matrix will be biased leading to a variety of issues, discussed in Chapter \@ref(assumptions).


### Mathematical derivation {#uncertaintyRegressionCovMaths}
In this subsection, we show the proof that the variance matrix of parameters can be calculated via the formula \@ref(eq:MLRcovarianceMatrix). There are several ways of showing this, we use the one that does not need the values of the explanatory variables to be known. The important thing we need to assume at this moment is that the "true" model is known and is correctly estimated. If it is not, the final formula will stay the same, but it will have different implications.

::: warning
Linear algebra! Tread lightly!
:::

::: proof
We start by expanding the formula for the OLS \@ref(eq:MLROLS) by inserting the "true" model in it (${\mathbf{y} = \mathbf{X}} {\boldsymbol{\beta}} + {\boldsymbol{\epsilon}}$:
\begin{equation}
\begin{aligned}
{\boldsymbol{b}} =
& \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{y}} = \\
& \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime ({\mathbf{X}} {\boldsymbol{\beta}} + {\boldsymbol{\epsilon}}) = \\
& \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{X}} {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} = \\
& {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} .
\end{aligned}
(\#eq:MLROLSExpansion)
\end{equation}

This equation tells us that the estimated parameters will have some variability around the true ones, and the extent of that variability will depend on the explanatory variables in $\mathbf{X}$ and the specific values of the error term $\boldsymbol{\epsilon}$.

In bypassing, we can show that the OLS estimates of parameters are unbiased (i.e. their expectation will be equal to the true parameters) as long as the model is correctly specified:
\begin{equation}
\begin{aligned}
\mathrm{E}\left(\boldsymbol{b}\right) =
& \mathrm{E}\left({\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}\right) = \\
& {\boldsymbol{\beta}} + \mathrm{E}\left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}\right) = \\
& {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime \mathrm{E}\left( {\boldsymbol{\epsilon}}\right) = \\
& \boldsymbol{\beta} ,
\end{aligned}
(\#eq:MLRCLSExpansion)
\end{equation}
which holds as long as the expectation of the true error term is zero, and this is always true by the definition of the true model (see discussion in Subsection \@ref(TrueModel)).

More importantly, we can calculate the variance of the estimates of parameters using \@ref(eq:MLROLSExpansion):

\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{V}\left( {\boldsymbol{\beta}} + \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right) .
(\#eq:MLROLSVariance01)
\end{equation}

In formula \@ref(eq:MLROLSVariance01), the true parameter $\boldsymbol{\beta}$ should be independent of the explanatory variables and the error term by definition of the true model, so the formula can be represented as a sum of variances (the right hand side only). Furthermore, in this basic linear model, we assume that the true parameter does not have any uncertainty, which means that the part related to ${\boldsymbol{\beta}}$ in \@ref(eq:MLROLSVariance01) can be dropped leaving us with:
\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{V}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right).
(\#eq:MLROLSVariance02)
\end{equation}
After that, we can recall that $\mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)=0$, because in the true model, there should be no structure in the error term and thus the explanatory variables and the error term should be unrelated (Subsection \@ref(TrueModel)). This is useful because now we can represent the variance \@ref(eq:MLROLSVariance02) as the expectation of products (this follows directly from equation \@ref(eq:VarianceAlt) that we discussed in Subsection \@ref(dataAnalysisNumerical)):
\begin{equation*}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \times \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
(\#eq:MLROLSVariance03)
\end{equation*}
Now to expand the expectation of the sum, we can use the following equality, coming directly from the definition of a covariance of two random variables (formula \@ref(eq:CovarianceAlt) from Section \@ref(dataAnalysisNumerical)):
\begin{equation}
\begin{aligned}
\mathrm{V}\left( {\boldsymbol{b}} \right) = 
& \mathrm{E}\left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right) \times \mathrm{E}\left(\left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right) + \\
& \mathrm{cov} \left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}, \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
\end{aligned}
(\#eq:MLROLSVariance04)
\end{equation}
We can notice that the expectations in \@ref(eq:MLROLSVariance04) should be zero because, as discussed earlier, in the true model, the error term is not related to anything. This means that the formula can be simplified to:
\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \mathrm{cov} \left(\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}}, \left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime \right).
(\#eq:MLROLSVariance05)
\end{equation}
The second element in the brackets can be rewritten as:
\begin{equation*}
\left( \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\boldsymbol{\epsilon}} \right)^\prime = {\boldsymbol{\epsilon}} {\mathbf{X}} \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} .
\end{equation*}
Substituting this to \@ref(eq:MLROLSVariance05) and moving explanatory variables from the covariance gives:
\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} {\mathbf{X}}^\prime {\mathbf{X}} \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} \mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right),
(\#eq:MLROLSVariance06)
\end{equation}
which after some cancellations leads to:
\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} \sigma^2 .
(\#eq:MLROLSVarianceSemiFinal)
\end{equation}
Given that we do not know the true variance of the error term, we need to estimate it. In case of the OLS, it can be done using the formula (from equation \@ref(eq:RMSERegression)):
\begin{equation*}
\hat{\sigma}^2 = \frac{1}{n-k} \sum_{j=1}^n e_j^2 ,
\end{equation*}
which after being inserted in \@ref(eq:MLROLSVarianceSemiFinal), leads to:
\begin{equation}
\mathrm{V}\left( {\boldsymbol{b}} \right) = \frac{1}{n-k} \sum_{j=1}^n e_j^2 \left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1} .
(\#eq:MLROLSVarianceFinal)
\end{equation}
:::

::: remark
Mathematically speaking, $\mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right)$ is a matrix, but because we are dealing with the true model and the true error term, the matrix should be diagonal with only variances in the main diagonal. This is because the error term cannot be correlated with itself (thus all the off-diagonal elements are zero). Furthermore, because we assume that the variance of the error term is the same for all observations, the following should hold:
\begin{equation*}
\mathrm{cov} \left( {\boldsymbol{\epsilon}}, {\boldsymbol{\epsilon}} \right) = \mathbf{I}_{k-1} \sigma^2,
\end{equation*}
where $\mathbf{I}_{k-1}$ is the identity matrix of the size $k-1$ and $\sigma^2$ is the variance of the error term.
:::

We should note that the majority of assumptions above needed for the derivation came from the definition of the true model. But the essential one was that **the model is correctly estimated**. If this is violated, the estimates of the elements in the covariance matrix will be biased in one way or the other (see Chapter \@ref(assumptions)).


## Confidence intervals of parameters {#uncertaintyRegressionCI}
Given that the estimates of parameters have some uncertainty associated with them, as discussed in the introduction to this chapter, it makes sense to capture that uncertainty so that decision makers can have a better understanding about the observed effects. Covariance matrix discussed in Section \@ref(uncertaintyRegressionCov) captures that uncertainty, but it is hard to make any decision based on it.

A simpler way to do that is to construct confidence intervals in a similar way to how we did that in Section \@ref(confidenceInterval). Visually, this process is shown in Figure \@ref(fig:costsModelMLRCI), which continues the example we discussed earlier.

```{r costsModelMLRCI, fig.cap="Parameter uncertainty in the estimated model", echo=FALSE}
par(mfcol=c(1,2))
# First histogram
hist(costsModelMLRBootstrap$coefficients[,1],
     xlab="Intercept", main="", probability=TRUE,
     xlim=range(costsModelMLRBootstrap$coefficients[,1])*c(1.1,1.15),
     col=17)
# The estimated coefficient
# abline(v=coef(costsModelMLR)[1], col=3, lwd=2)
# lines(density(costsModelMLRBootstrap$coefficients[,1]), col=5, lwd=2)
xSequence <- mean(costsModelMLRBootstrap$coefficients[,1])+seq(-5,5,0.1)*sd(costsModelMLRBootstrap$coefficients[,1])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelMLRBootstrap$coefficients[,1]),
            sd(costsModelMLRBootstrap$coefficients[,1])), col=2, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelMLRBootstrap$coefficients[,1]),
               sd(costsModelMLRBootstrap$coefficients[,1])), col=4, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelMLRBootstrap$coefficients[,1]),
             sd(costsModelMLRBootstrap$coefficients[,1]))),
     rep(0.00015,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelMLRBootstrap$coefficients[,1]),
                        sd(costsModelMLRBootstrap$coefficients[,1])),2), pos=c(2,4))

# Second histogram
hist(costsModelMLRBootstrap$coefficients[,2],
     xlab="Materials costs", main="", probability=TRUE,
     xlim=range(costsModelMLRBootstrap$coefficients[,2])*c(0.75,1.1),
     col=17)
# The estimated coefficient
# abline(v=coef(costsModelMLR)[2], col=3, lwd=2)
# lines(density(costsModelMLRBootstrap$coefficients[,2]), col=5, lwd=2)
xSequence <- mean(costsModelMLRBootstrap$coefficients[,2])+seq(-5,5,0.1)*sd(costsModelMLRBootstrap$coefficients[,2])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelMLRBootstrap$coefficients[,2]),
            sd(costsModelMLRBootstrap$coefficients[,2])), col=2, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelMLRBootstrap$coefficients[,2]),
               sd(costsModelMLRBootstrap$coefficients[,2])), col=4, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelMLRBootstrap$coefficients[,2]),
             sd(costsModelMLRBootstrap$coefficients[,2]))),
     rep(2,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelMLRBootstrap$coefficients[,2]),
                        sd(costsModelMLRBootstrap$coefficients[,2])),2), pos=c(2,4))
```

Figure \@ref(fig:costsModelMLRCI) demonstrates the bootstrapped distributions of parameters (as before), together with the Normal probability density functions on top of them and vertical lines at the tails that represent the confidence bounds. The idea behind this is that due to the Central Limit Theorem (Section \@ref(CLT)) we can assume that estimates of parameters follow the Normal distribution with some mean and variance, and based on that we can get the quantiles, thus inferring, for example, that the true intercept will lie between `r round(qnorm(c(0.025), mean(costsModelMLRBootstrap$coefficients[,1]), sd(costsModelMLRBootstrap$coefficients[,1])),2)` and `r round(qnorm(c(0.975), mean(costsModelMLRBootstrap$coefficients[,1]), sd(costsModelMLRBootstrap$coefficients[,1])),2)` in the 95% of the cases, if we repeat the resampling experiment many times.

The interpretation of the confidence interval is exactly the same as in the simpler case discussed in the Section \@ref(confidenceInterval). The formula for it is slightly different, because it is constructed for the parameters of the model rather than the mean of the sample, but the logic is exactly the same:
\begin{equation}
\beta_i \in (b_i + t_{\alpha/2}(n-k) s_{b_i}, b_i + t_{1-\alpha/2}(n-k) s_{b_i}),
(\#eq:MLRParameterInterval)
\end{equation}
where $s_{b_i}$ is the standard error of the parameter $b_i$. An important note to make is that, as usual with confidence interval construction, we can use the Normal distribution only in the case when the variance of parameters is known. In reality, it is not, so, as discussed in Section \@ref(confidenceInterval), we need to use the Student's t distribution. This is why we have $t_{\alpha/2}(n-k)$ in the equation \@ref(eq:MLRParameterInterval) above.

::: remark
The important note is that in order to be able to use the formula \@ref(eq:MLRParameterInterval), we need the Central Limit Theorem to hold. If it does not, then the estimates of parameters would not follow the Normal distribution, and the Student's t-statistics would not be appropriate to calculate the interval.
:::

To calculate the confidence interval, using the equation \@ref(eq:MLRParameterInterval), we need to know several things:

1. Significance level $\alpha$ - we define it either based on our preferences or on the task at hand. This should be selected prior to construction of the interval. The typical one is 5%, mainly because the standard human has five fingers on a hand;
2. The value of the estimated parameter $b_i$. We get it after using the Least Squares, any statistical software can give us the estimate.
3. Standard error (or deviation) of the parameter. This is obtained from the covariance matrix of parameters discussed in Section \@ref(uncertaintyRegressionCov).
4. Number of degrees of freedom $n-k$, which can be easily calculated based on the sample size $n$ and the number of estimated parameters $k$.
5. Student's t statistics. We get it from statistical tables or the software, given the significance level $\alpha$ and the number of degrees of freedom $n-k$ above.

::: example
Consider construction of the interval for the slope parameter for the variable `materials` from the regression we discussed earlier. In our case, we know the following:

1. $\alpha=0.05$ because we decided to produce the 95% confidence interval. We could choose the other value, which would result in the interval of a different width;
2. The value of the parameter $b_1$ is `r round(coef(costsModelMLR)[2],digits)`;
3. To get the variance of the parameter, we can use the formula discussed in Section \@ref(uncertaintyRegressionCov). In R, we can get the variance of the parameter for the `materials` (second parameter in the model) via the `vcov()` function, like this:
```{r}
vcov(costsModelMLR)[2,2]
```
Based on that, we can say that the standard error of the parameter (square root of the variance) is approximately `r round(sqrt(vcov(costsModelMLR)[2,2]),digits)`.
4. We estimated five parameters (four for variables and an intercept), so $k=5$, and the sample size $n$ was 61, which means that in our model has $n-k=61-5=56$ degrees of freedom.
5. To get the Student's t statistics correctly, we need to split the significance level $\alpha$ into two equal part, meaning that we expect to have 2.5% of values below the lower bound and another 2.5% of values above the upper one. So, we will calculate it for $\alpha/2=0.025$ and $1-\alpha/2=0.975$ with $n-k=56$. In R, we can use the `qt()` function in the following way:
```{r}
qt(c(0.025,0.975), 56)
```

```{r echo=FALSE}
x <- round(round(coef(costsModelMLR)[2], digits)+qt(c(0.025,0.975), 56)*round(sqrt(vcov(costsModelMLR)[2,2]), digits), digits)
```

Taking all these values and inserting them in the formula \@ref(eq:MLRParameterInterval), we should get the two numbers, representing the lower and the upper bounds of the 95% confidence interval respectively: (`r paste0(x, collapse=",")`)

In R, the confidence interval can be obtained directly via the `confint()` function, and it should be close to what we obtained above, but not exactly the same due to rounding:
```{r}
confint(costsModelMLR) |> round(4)
```

```{r echo=FALSE}
x <- round(confint(costsModelMLR)[2,2:3], digits)
```

The confidence interval for `materials` above shows, for example, that if we repeat the construction of interval many times on different samples of data, the true value of parameter will lie in 95% of cases between `r x[1]` and `r x[2]`. This gives us an idea about the real effect in the population and how certain we are about it.
:::

We can also present all of this in the following summary (this is based on the `alm()` model, the other functions will produce different outputs):
```{r}
summary(costsModelMLR)
```

This summary provides all the necessary information about the model and the estimates of parameters:

- their mean values are in the column "Estimate",
- their standard errors (square roots of the variances) are in "Std. Error",
- the bounds of confidence interval are in the last two columns,
- and, finally, a star drawn next to the interval if it does not contain zero.

If we have that star, then this indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect indeed exists. We can regulate the confidence level by defining the `level` parameter. For example, if we want the 99% one, we should add `level=0.99` in the summary:

```{r eval=FALSE}
summary(costsModelMLR, level=0.99)
```



## Regression line related uncertainty {#uncertaintyRegressionLine}
Given the uncertainty of estimates of parameters, the regression line itself will vary with different samples. This means that in some cases we should not just consider the predicted values of the regression $\hat{y}_j$, but should also consider the uncertainty around them.

Similar to what we discussed in Sections \@ref(confidenceInterval) and \@ref(predictionInterval), there are two different interval types we can produce from the regression: confidence and prediction. It is extremely important to get the difference between the two, so here it is again just in case:

1. **Confidence interval** shows where the **expected value** should lie in the population in the x% of the cases if we repeat the calculation for different samples of data many times;
2. **Prediction interval** shows where the x% of the **actual values** should lie.

The former will tell you something about the true *expected costs* of a project, while the latter will tell you about the *actual costs*. The word "should" indicates that this is not necessarily always the case - the final outcome depends on the specific data (e.g. sample size) and the model (whether its assumptions hold).


### Confidence interval {#uncertaintyRegressionLineConfidence}
In the context of univariate statistics that we discussed in Section \@ref(confidenceInterval), the idea was to capture the uncertainty around the mean of the data. In case of regression, we are focusing on the conditional mean, i.e. the expected value, given the values of all explanatory variables for a specific observation. In the example that we have used in this chapter, this would come to understanding what the true expected overall costs should be, for example, for the material costs of 200 thousands of pounds, size of 90, 3 projects and the year 2008. This should be straight forward to calculate as long as we have the covariance matrix of parameters (Section \@ref(uncertaintyRegressionCov)).

The formula for the construction of the confidence interval is very similar to the one we discussed for the sample mean in Section \@ref(confidenceInterval):
\begin{equation}
\mu_j \in (\hat{y}_j + t_{\alpha/2}(n-k) s_{\hat{y}_j}, \hat{y}_j + t_{1-\alpha/2}(n-k) s_{\hat{y}_j}),
(\#eq:confidenceIntervalRegression)
\end{equation}
where $\mu_j$ is the true expectation, and $s_{\hat{y}_j}=\sqrt{\mathrm{V}(\hat{y}_j| \mathbf{x}_j)}$ is the standard deviation of the fitted value. Compare this with the one we had in Section \@ref(confidenceInterval):
\begin{equation*}
\mu \in (\bar{y} + t_{\alpha/2}(n-1) s_{\bar{y}}, \bar{y} + t_{1-\alpha/2}(n-1) s_{\bar{y}}).
\end{equation*}
Here are the main differences:

1. We now focus on $\mu_j$, the expectation for a specific observation instead of just global average $\mu$;
2. We have the fitted value from the regression line $\hat{y}_j$ instead of the sample mean $\bar{y}$;
3. We have the standard deviation of the line $s_{\hat{y}_j}$ instead of the standard deviation of the mean $s_{\bar{y}}$;
4. We have $n-k$ degrees of freedom instead of $n-1$ because we estimated $k$ parameters instead of just one (the sample mean).

But the rest is the same and uses exactly the same principles.

::: remark
To be able to construct the confidence interval using the formula above, we, once again, need one important assumption to hold. This assumption is that the *Central Limit Theorem holds* (Section \@ref(CLT)). If it does not, then the distribution of the estimates of parameters would not be Normal and as a result, the distribution of the conditional expectations would not be Normal either. In that case, we would need to use some other tools (such as bootstrap) to construct the interval. Luckily, with OLS, the CLT typically holds on samples of 50 observations and more.
:::

In R, the confidence interval can be constructed for each observation via the `predict()` function with `interval="confidence"`. It is based on the covariance matrix of parameters, extracted via `vcov()` method in R (discussed in Section \@ref(uncertaintyRegressionCov)). Note that the interval can be produced not only for the in-sample value, but for the holdout as well. Here is an example with `alm()` function (Figure \@ref(fig:costModelConfidenceInterval)):

```{r costModelConfidenceInterval, fig.cap="Fitted values and confidence interval for the costs model."}
# Produce predictions
costsModelMLRCI <- predict(costsModelMLR, interval="confidence")
plot(costsModelMLRCI, main="", type="p",
     xlab="Observation", ylab="Overall Costs")
```

It is hard to read this graph because the interval is narrow, and the actual values in it do not help in reading it. But if we zoom in to a specific observation, it would be more readable (Figure \@ref(fig:costModelConfidenceIntervalZoom)):

```{r costModelConfidenceIntervalZoom, fig.cap="Fitted values and confidence interval for the costs model."}
# Produce the predicted value and the confidence interval
costsModelMLRCIZoom <- predict(costsModelMLR,
                               interval="confidence",
                               newdata=data.frame(materials=200, size=90,
                                                  projects=3, year=2008))
# Plot the material costs vs fitted
plot(200, costsModelMLRCIZoom$mean,
     xlab="Material costs", ylab="Overall costs",
     ylim=c(costsModelMLRCIZoom$lower, costsModelMLRCIZoom$upper),
     pch=16)
# Add confidence interval
abline(h=c(costsModelMLRCIZoom$lower,
           costsModelMLRCIZoom$upper),
       col=4, lwd=2, lty=2)
```

What the image in Figure \@ref(fig:costModelConfidenceIntervalZoom) shows is that according to the estimated regression, for the project where the material costs were £200k, size of the property was 90 m$^2$, the crew did 3 projects and the new project was done in the year 2008, the expected overall costs would be `r round(costsModelMLRCIZoom$mean, digits)`. But given the uncertainty of the estimates of parameters, the true expected costs should lie between `r round(costsModelMLRCIZoom$lower, digits)` and `r round(costsModelMLRCIZoom$upper, digits)` in 95% of the cases if we repeat the confidence interval construction many times.

Another way of visualising the confidence interval is by plotting the actuals vs fitted figure:
```{r costModelConfidenceIntervalAvsF, fig.cap="Actuals vs Fitted and confidence interval for the costs model."}
plot(fitted(costsModelMLR), actuals(costsModelMLR),
     xlab="Fitted",ylab="Actuals")
abline(a=0, b=1, col=3, lwd=2, lty=2)
lines(sort(fitted(costsModelMLR)),
      costsModelMLRCI$lower[order(fitted(costsModelMLR))], 
      col=4, lwd=2, lty=2)
lines(sort(fitted(costsModelMLR)),
      costsModelMLRCI$upper[order(fitted(costsModelMLR))], 
      col=4, lwd=2, lty=2)
```

Figure \@ref(fig:costModelConfidenceIntervalAvsF) demonstrates the actuals vs fitted plot, together with the 95% confidence interval around the line, demonstrating where the true line would be expected to be in 95% of the cases if we re-estimate the model many times on different samples. We also see that the uncertainty of the regression line is lower in the middle of the data than in the tails. Conceptually, this happens because the regression line, estimated via OLS, always passes through the average point of the data $(\bar{x},\bar{y})$ and the variability in this point is lower than the variability in the tails.


### Prediction interval {#uncertaintyRegressionLinePrediction}
In case of prediction interval, we are interested in understanding where the actual values lie, not the expected one. For our example, that would mean that we want to know the span of overall costs in the x% of the cases (e.g. 95%) for a project constructed in 2008 that has material costs of £200k, size of the property of 90 squared meters, done by the crew that did 3 projects before it.

The construction of the interval relies on the formula, which is very similar to the one used for the confidence interval:
\begin{equation}
y_j \in (\hat{y}_j + z_{\alpha/2} s_y^2, \hat{y}_j + z_{1-\alpha/2} s_y^2),
(\#eq:predictionIntervalRegression)
\end{equation}
where $s_y^2$ is the standard deviation of the actual values, conditional on the values of the explanatory variables. Its derivation is shown in Subsection \@ref(uncertaintyRegressionLineMaths). The main difference with the confidence interval is that we now need to rely on the *assumption about the distribution of the error term* in the model.

::: remark
In the formula above, the important assumption is that **the error term follows the Normal distribution**. We can no longer rely on the CLT, because it only applies to the estimates of parameters. If we cannot assume that the error term follows the Normal distribution, we would need to use either a different one, or to construct the interval using some non-parametric methods.
:::

In R, the interval construction can be done via the very same `predict()` function with `interval="prediction"`:
```{r}
costsModelMLRPI <- predict(costsModelMLR, interval="prediction")
```

Based on this, we can produce an images similar to \@ref(fig:costModelConfidenceInterval) and \@ref(fig:costModelConfidenceIntervalAvsF).

```{r costModelPI, fig.cap="Fitted values and prediction interval for the stopping distance model.", echo=FALSE}
par(mfcol=c(1,2))
plot(costsModelMLRPI, ylab="Values", xlab="Observations",
     main="Actuals and Fitted over observations",
     type="p")
plot(fitted(costsModelMLR), actuals(costsModelMLR),
     xlab="Fitted",ylab="Actuals",main="Actuals vs Fitted",
     ylim=range(costsModelMLRPI$lower,costsModelMLRPI$upper))
abline(a=0, b=1, col=3, lwd=2, lty=2)
lines(sort(fitted(costsModelMLR)),
      costsModelMLRPI$lower[order(fitted(costsModelMLR))], 
      col=4, lty=2, lwd=2)
lines(sort(fitted(costsModelMLR)),
      costsModelMLRPI$upper[order(fitted(costsModelMLR))], 
      col=4, lty=2, lwd=2)
```


Figure \@ref(fig:costModelPI) shows the prediction interval for values over observations and for actuals vs fitted. While the first image is hard to read, we can see that the prediction interval is wider than the confidence one. The second image is more informative, showing that there are some points lying outside of the 95% prediction interval (there are 3 observations outside it). The statistical principles guarantee that asymptotically, if we increase the sample size and repeat the experiment many times, we would have 95% of observations lying inside the interval if the regression assumptions hold.

In forecasting, prediction interval has a bigger importance than the confidence interval. This is because we are typically interested in capturing the uncertainty about the observations, not about the estimate of a line. Typically, the prediction interval would be constructed for some holdout data, which we did not have at the model estimation phase. In the example with costs, we could see what the prediction interval would be in the setting we discussed earlier (materials=200, size=90, projects=3 and year=2008):

```{r costModelForecast, fig.cap="Forecast of the overall costs given the material costs of 200, size of 90 squared meters, 3 projects and year 2008."}
costsModelMLRForecast <- predict(costsModelMLR,
                                 newdata=data.frame(materials=200, size=90,
                                                    projects=3, year=2008),
                                 interval="prediction")
plot(costsModelMLRForecast)
```

Figure \@ref(fig:costModelForecast) shows the point forecast (the expected overall costs of a project given the values of explanatory variables) and the 95% prediction interval (we expect that in 95% of the cases, the overall costs would be between `r paste(round(c(costsModelMLRForecast$lower,costsModelMLRForecast$upper), digits),collapse=" and ")` thousands of pounds.


### Derivations of the variance in regression {#uncertaintyRegressionLineMaths}
The uncertainty of the regression line builds upon the uncertainty of parameters and can be measured via the conditional variance using the formula:
\begin{equation*}
\mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j}) ,
(\#eq:regressionLineUncertaintyVariance01)
\end{equation*}
which after some simplifications leads to:
\begin{equation}
\mathrm{V}(\hat{y}_j| \mathbf{x}_j) = \sum_{l=0}^{k-1} \mathrm{V}(b_j) x^2_{l,j} + 2 \sum_{l=1}^{k-1} \sum_{i=0}^{l-1}  \mathrm{cov}(b_i,b_l) x_{i,j} x_{l,j} ,
(\#eq:regressionLineUncertaintyVariance02)
\end{equation}
where $x_{0,j}=1$. Alternatively, this can be calculated using the compact form of the multiple regression model \@ref(eq:MLRFormulaCompacter):
\begin{equation*}
y_j = \mathbf{x}'_j \boldsymbol{\beta} + \epsilon_j .
\end{equation*}
the fitted values for which are:
\begin{equation*}
\hat{y_j} = \mathbf{x}'_j \boldsymbol{\beta} .
\end{equation*}
Taking the variance of the fitted in that form would involve the covariance matrix of parameters from Section \@ref(uncertaintyRegressionCov):
\begin{equation*}
\mathrm{V}(\hat{y_j}| \mathbf{x}_j) = \mathbf{x}'_j \mathrm{V}({\boldsymbol{b}}) \mathbf{x}_j.
\end{equation*}
In any case, we see that the variance of the regression line relies on the variances and covariances of parameters. It can then be used in the construction of the confidence interval for the regression line.

Given that each estimate of parameter $b_i$ should follow the Normal distribution with a fixed mean and variance due to CLT (Section \@ref(CLT)), the predicted value $\hat{y}_j$ will follow it as well. This is because the multiplication of the Normal distribution by a number (the value of an explanatory variable) gives the Normal distribution as well, and the addition of Normal distributions produces another one, but with different parameters. So, we can say that:
\begin{equation*}
\mu_j \sim \mathcal{N}(\hat{y}_j, \mathrm{V}(\hat{y}_j| \mathbf{x}_j)) .
\end{equation*}
We used this property to derive the formula for the confidence interval around the line.

If we are interested in capturing the uncertainty of the actual values, we can refer to prediction interval. In this case we rely on the Normality assumption for the actual values themselves:
\begin{equation*}
y_j = (\hat{y}_j + \epsilon_j) \sim \mathcal{N}(\hat{y}_j, s_y^2) ,
\end{equation*}
because we assume that $\epsilon_j \sim \mathcal{N}(0, {\sigma}^2)$, where ${\sigma}^2$ is the variance of the error term and $s_y^2 = \mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(\hat{y}_j| \mathbf{x}_j) + \hat{\sigma}^2$ is the estimated of the variance of the response variable conditional on the values of the explanatory variables. The latter can be calculated as:
\begin{equation}
\mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \dots + b_{k-1} x_{k-1,j} + e_j) ,
(\#eq:regressionLineUncertaintyVariance03)
\end{equation}
which can be simplified to (if assumptions of regression model hold, see Chapter \@ref(assumptions)):
\begin{equation}
\mathrm{V}(y_j| \mathbf{x}_j) = \mathrm{V}(\hat{y}_j | \mathbf{x}_j) + \hat{\sigma}^2,
(\#eq:regressionLineUncertaintyVariance04)
\end{equation}
where the variance $\mathrm{V}(\hat{y}_j | \mathbf{x}_j)$ was calculated above for the confidence interval in formula \@ref(eq:regressionLineUncertaintyVariance02). Given that the variance \@ref(eq:regressionLineUncertaintyVariance04) is larger than the variance \@ref(eq:regressionLineUncertaintyVariance02), the prediction interval will always be wider than the confidence one.


## Hypothesis testing {#uncertaintyRegressionHypothesis}
In the regression context, an analyst is often interested in understanding whether a specific effect exists or not. This is usually done using the hypothesis testing instrument, which we will discuss in this section. But before we do that, we will approach this problem from a slightly non-conventional perspective, using the connection between hypothesis testing and confidence interval.

What does "effect exists" mean in statistical terms? It means that we found some evidence based on our data and model that the value of a parameter is significantly different from zero on some pre-selected level. Formally, this hypothesis is formulated as:
\begin{equation}
\begin{aligned}
\mathrm{H}_0: \beta_i = 0 \\
\mathrm{H}_1: \beta_i \neq 0
\end{aligned} .
(\#eq:regressionHypothesis01)
\end{equation}
where $\beta_i$ is the true parameter in the population. The idea is to make some conclusions about what happens in population based on a specific sample of data **and** the estimated model.

Coming back to the example in Section \@ref(uncertaintyRegressionCI), the whole hypothesis testing process implies that the distribution of the estimates of parameter for `materials` lies either consistently above zero or consistently below it (in our case, it is above). Figure \@ref(fig:costsModelMLRCImaterials) demonstrates the distribution we obtained from the bootstrap in the beginning of this chapter and the 95% confidence interval based on the normal approximation of the distribution.

```{r costsModelMLRCImaterials, fig.cap="Parameter uncertainty in the estimated model for the materials variable.", echo=FALSE}
# Second histogram
hist(costsModelMLRBootstrap$coefficients[,2],
     xlab="Material costs", main="", probability=TRUE, col=17)
# The estimated coefficient
# abline(v=coef(costsModelMLR)[2], col=3, lwd=2)
# lines(density(costsModelMLRBootstrap$coefficients[,2]), col=5, lwd=2)
xSequence <- mean(costsModelMLRBootstrap$coefficients[,2])+seq(-5,5,0.1)*sd(costsModelMLRBootstrap$coefficients[,2])
lines(xSequence,
      dnorm(xSequence,
            mean(costsModelMLRBootstrap$coefficients[,2]),
            sd(costsModelMLRBootstrap$coefficients[,2])), col=2, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               mean(costsModelMLRBootstrap$coefficients[,2]),
               sd(costsModelMLRBootstrap$coefficients[,2])), col=4, lwd=2)
text(c(qnorm(c(0.025,0.975),
             mean(costsModelMLRBootstrap$coefficients[,2]),
             sd(costsModelMLRBootstrap$coefficients[,2]))),
     rep(2,2),
     labels=round(qnorm(c(0.025,0.975),
                        mean(costsModelMLRBootstrap$coefficients[,2]),
                        sd(costsModelMLRBootstrap$coefficients[,2])),digits), pos=c(2,4))
```

The 95% confidence level corresponds to the 5% significance level. We see from Figure \@ref(fig:costsModelMLRCImaterials) that the variability of the estimate of the parameter is substantially above zero. This probably means that if we increase the sample size and continue constructing the interval, the true value will be somewhere inside and it must be not equal to zero. We use the word "*probably*" here because we could always make a mistake based on a specific sample or because the model was incorrectly specified. Still, we would be inclined to conclude on the 5% significance level that the effect is indeed non-zero.

::: remark
In hypothesis testing, if we formulate the alternative one as inequality (as it was done in \@ref(eq:regressionHypothesis01)), we cannot say whether the effect is positive or negative. We can only conclude whether it is or it is not zero on the pre-specified significance level.
:::

If we bring this idea further, we can also conclude that the effect is not equal to 0.4 (that value is outside of the interval and in the left tail of the distribution), and surely not 1.5 (it is in the right tail). We can say that by directly looking at the confidence interval, and, arguably, it provides more information than a critical value or a p-value (we will discuss them later in this section), which would only compare the sample value with zero. But this also shows us what the hypothesis testing is all about: checking whether the tested value is inside the non-rejection interval or not. But can we make solid conclusions about the effect based on the results of the hypothesis testing?

Figure \@ref(fig:costsModelMLRCIWithZero) shows the distribution of estimates of some hypothetical parameter in a regression model together with the 95% confidence interval.

```{r costsModelMLRCIWithZero, fig.cap="Parameter uncertainty of a hypothetical parameter.", echo=FALSE}
# Second histogram
set.seed(41)
meanValue <- 0.5
sdValue <- 1
hist(rnorm(1000, meanValue, sdValue),
     xlab="Slope", main="", probability=TRUE,
     ylim=c(0,0.8), col=17)
# The estimated coefficient
# abline(v=coef(costsModelMLR)[2], col=3, lwd=2)
# lines(density(costsModelMLRBootstrap$coefficients[,2]), col=5, lwd=2)
xSequence <- 1+seq(-5,5,0.1)*0.75
lines(xSequence,
      dnorm(xSequence,
            meanValue,
            sdValue), col=2, lwd=2)
abline(v=qnorm(c(0.025,0.975),
               meanValue,
               sdValue), col=4, lwd=2)
text(c(qnorm(c(0.025,0.975),
             meanValue,
             sdValue)),
     rep(0.5,2),
     labels=round(qnorm(c(0.025,0.975),
                        meanValue,
                        sdValue), digits), pos=c(2,4))
```

According to Figure \@ref(fig:costsModelMLRCIWithZero), the zero is included in the interval, so we would fail to reject the hypothesis that the effect is zero on the 5% significance level. This is because there is a chance that this distribution would shift to the left with the increase of the sample size and would become centered around zero. Does this mean that there is no effect? No, because we do not know what happens in population, we are making conclusions based on the limited sample and a specific model.

We can also notice that if we were to formulate a different hypothesis, for example, that the parameter is equal to one, we would fail to reject it on the 5% significance level as well. In fact, we would fail to reject the hypotheses on the 5% significance level that the true parameter equals to 0.5, 1.5 and anything between `r round(qnorm(0.025, meanValue, sdValue), digits)` and `r round(qnorm(0.975, meanValue, sdValue), digits)`. Neither of this tells us anything definite about the true value of parameter. The true conclusion of hypothesis testing in this example is that **we do not know** what the true value of the effect is.

We personally think that the confidence interval shows this idea more clearly, than the conventional hypothesis testing, not hiding anything.

Now we will discuss the classical hypothesis testing in regression.


### Regression parameters
The logic of hypothesis testing in regression is very similar to the one discussed in Section \@ref(hypothesisTesting). To make it more practical, consider the regression that we have already estimated in this chapter and the coefficient for the variable `materials`, which equals to `r round(coef(costsModelMLR)[2], digits)`. To test the hypothesis about a parameter in regression, we need to follow the procedure, described in Section \@ref(hypothesisTesting).

First step is to formulate the null and alternative hypotheses. As discussed earlier in this section, the conventional way of doing that is by checking whether the true value of parameter is zero in the population:
\begin{equation*}
\begin{aligned}
\mathrm{H}_0: \beta_i = 0 \\
\mathrm{H}_1: \beta_i \neq 0
\end{aligned} .
\end{equation*}

Visually, the null and alternative hypotheses can be represented as shown in Figure \@ref(fig:costModelHypotheses).

```{r costModelHypotheses, fig.cap="Graphical presentation of null and alternative hypothesis in regression context.", echo=FALSE}
costsModelSLR <- alm(overall~materials, data=SBA_Chapter_11_Costs, loss="MSE")
par(mfcol=c(1,2))
plot(SBA_Chapter_11_Costs$materials, SBA_Chapter_11_Costs$overall,
     xlab="Material costs", ylab="Overall costs", main=TeX("H$_0$: $\\beta_i = 0$"),
     col=1)
abline(h=mean(SBA_Chapter_11_Costs$overall), col=2, lwd=2)
plot(SBA_Chapter_11_Costs$materials, SBA_Chapter_11_Costs$overall,
     xlab="Material costs", ylab="Overall costs", main=TeX("H$_1$: $\\beta_i \\neq 0$"),
     col=1)
abline(costsModelSLR, col=4, lwd=2)
```

The image on the left in Figure \@ref(fig:costModelHypotheses) demonstrates how the true model could look if the null hypothesis was true - it would be just a straight line, parallel to x-axis. This would imply that with the increase of the material costs, the overall costs do not change. The image on the right in Figure \@ref(fig:costModelHypotheses) demonstrates the alternative situation, when the parameter is not equal to zero. We do not know the true model, and hypothesis testing does not tell us whether the hypothesis is true or false, but if we have enough evidence to reject H$_0$, then we might conclude that we see an effect of one variable on the other one in the data.

After formulating the hypothesis, we select the significance level. Fro this exercise, we will choose 5%.

Then, we need to select the test to use. As discussed earlier in this chapter, if we can assume that the Central Limit Theorem holds (typically, for the linear regression estimated using the OLS, on samples of more than 50 observations, it does), we can use the Normal distribution. But given that the standard deviations of parameters are not known, and are estimated, we need to use Student's t distribution. The formula for calculating the t-statistics is fundamentally similar to the one in Section \@ref(hypothesisTestingBasics):
\begin{equation}
t = \frac{|b_i - 0|}{s_{b_i}} ,
(\#eq:ttestFormulaReg)
\end{equation}
where $b_i$ is the estimate of the parameter $\beta_i$ and $s_{b_i}$ is the standard error of the parameter obtained from the covariance matrix from Section \@ref(uncertaintyRegressionCov). The standard deviation of the parameter is:
```{r}
vcov(costsModelMLR)[2,2] |> sqrt()
```
Inserting the values in formula \@ref(eq:ttestFormulaReg) gives us the t-statistics value for our example:
\begin{equation*}
t = \frac{|0.871 - 0|}{0.311} \approx 2.797
\end{equation*}
Now we need to get the critical value of the t-statistics for the selected significance value and $n-k$ degrees of freedom, which in our case are equal to 61-5=56. We chose the significance value of 5%, and we conduct a two-sided test, so we need to split it into two parts (this would be similar to constructing a confidence interval), so we should look at the value of statistics for the 2.5% level. In R, we can get it by running:
```{r}
qt(c(0.025, 0.975), 56)
```
In this code, we consciously provide the values of the statistics for both tails to keep the connection with the interval. If the calculated value was inside this interval, we would fail to reject the hypothesis. In our case, 2.797 lies outside of these bounds, so we can reject the H$_0$ and conclude that based on our model and the data, we have enough evidence to say that there is an effect of material costs on the overall costs of the project.

::: remark
The easier way of conducting the two sided test is to compare the t-statistics with the absolute of the critical value. In our case, we would be comparing 2.797 with `r round(abs(qt(0.975, 56)), digits)`, and we would also conclude that the calculated value lies in the tails of distribution and thus we can reject the H$_0$.
:::

An alternative way of testing the hypothesis is by calculating the p-value and comparing it with the significance level. In that case, we would reject the H$_0$ if the p-value is lower than the level. For our example, the p-value can be calculated using the `pt()` function in R:
```{r}
pt(2.797, 56, lower.tail=FALSE)
```
Given that we conduct the two-tail test, this p-value needs to be multiplied by 2 to get approximately `r round(pt(2.797, 56, lower.tail=FALSE)*2, digits+1)`. The conclusion that we can draw from this is that we reject the null hypothesis on the 5% significance level.

All of this is done automatically by R if we estimate the model using the basic `lm()` function from the `stats` package:
```{r}
lm(overall~materials+size+projects+year, data=SBA_Chapter_11_Costs) |>
    summary()
```

The output above shows the estimates of parameters, the standard errors, the t-values and p-values (called "Pr(>|t|)").

::: remark
When working with p-values, there is a temptation to not select the significance level before conducting the test or changing it after obtaining the results. This is one of the typical mistakes that can lead to so called p-hacking, where the interpretation of the results of the experiment are amended to fit better to preferences of analyst. This temptation should be resisted! This and other mistakes related to p-values and hypothesis testing were discussed in Subsection \@ref(hypothesisTestingMistakes).
:::

The context of regression also provides a great example, why we never accept the null hypothesis and why in the case of "Fail to reject H$_0$", we should not remove a variable (unless we have more fundamental reasons for doing that). Consider an example, where the estimated parameter $b_1=0.5$, and its standard error is $s_{b_1}=1$, we estimated a simple linear regression on a sample of 30 observations, and we want to test, whether the parameter in the population is zero (hypothesis \@ref(eq:regressionHypothesis01)) on 5% significance level. Inserting the values in formula \@ref(eq:ttestFormulaReg), we get: 
\begin{equation*}
\frac{|0.5 - 0|}{1} = 0.5,
\end{equation*}
with the critical value for two-tailed test of $t_{0.025}(30-2)\approx 2.05$. Comparing t-value with the critical one, we would conclude that we fail to reject H$_0$ and thus the parameter is not statistically different from zero. There is a temptation to remove it from the model, but this would be fundamentally wrong. And here is why.

Consider testing another hypothesis for the same parameter:
\begin{equation*}
\begin{aligned}
\mathrm{H}_0: \beta_1 = 1 \\
\mathrm{H}_1: \beta_1 \neq 1
\end{aligned} .
\end{equation*}
The procedure is the same, the calculated t-value is:
\begin{equation*}
\frac{|0.5 - 1|}{1} = 0.5,
\end{equation*}
which leads to exactly the same conclusion as before: on 5% significance level, we fail to reject the new H$_0$, so the value is not distinguishable from 1. So, which of the two conclusions is correct? Is it zero or is it one?

The correct answer is "**we do not know**". The non-rejection region just tells us that the uncertainty about the parameter is so high that it also includes the value of interest (0 in case of the classical regression analysis or 1 in the case of the second hypothesis). If we constructed the confidence interval for this problem, we would not have such confusion, as we would conclude that on 5% significance level the true parameter lies in the region $(-1.46, 2.46)$ and can be any of the numbers between them.


### Regression line
Finally, in regression context, we can test another hypothesis, which becomes useful, when many parameters of the model are very close to zero and seem to be insignificant on the selected level:
\begin{equation}
    \begin{aligned}
        \mathrm{H}_0: \beta_1 = \beta_2 = \dots = \beta_{k-1} = 0 \\
        \mathrm{H}_1: \beta_1 \neq 0 \vee \beta_2 \neq 0 \vee \dots \vee \beta_{k-1} \neq 0
    \end{aligned} ,
(\#eq:regressionHypothesis02)
\end{equation}
which translates into normal language as:
\begin{equation*}
    \begin{aligned}
        \mathrm{H}_0: \text{ all parameters (except for intercept) are equal to zero}\\
        \mathrm{H}_1: \text{ at least one parameter is not equal to zero}
    \end{aligned} .
\end{equation*}
This hypothesis is only needed when you have a model with many statistically insignificant variables and want to see if overall the model explains anything. This is done using F-test, which can be calculated based on sums of squares (discussed in Subsection \@ref(linearRegressionSimpleQualityOfFitSSE)):
\begin{equation*}
F = \frac{ SSR / (k-1)}{SSE / (n-k)} \sim F(k-1, n-k) ,
\end{equation*}
where the sums of squares are divided by their degrees of freedom. The test is conducted in the similar manner as any other test (see Section \@ref(hypothesisTesting)): after choosing the significance level, we calculate the F-value and then either select the critical value of F for the specific degrees of freedom, or compare the significance level with the p-value from the test to make a conclusion about the null hypothesis.

There are several things to consider about the F-test in regression:

- It is not very useful, when at least one parameter is statistically significant. It only becomes useful in difficult situations of poor fit.
- The test on its own does not tell if the model is good or bad, adequate or not, etc.
- And the F value and related p-value are not comparable with respective values of another models.

Visually, this test checks, whether in the true model the slope of the line on the plot of actuals vs fitted is different from zero. An example with the same costs model is provided in Figure \@ref(fig:costsModelHypothesesF).

```{r costsModelHypothesesF, fig.cap="Graphical presentation of F-test for regression model.", echo=FALSE}
plot(fitted(costsModelMLR), actuals(costsModelMLR),
     xlab="Fitted", ylab="Actuals")
abline(a=0, b=1, col=4, lwd=2)
abline(h=mean(actuals(costsModelMLR)),col=2, lwd=2)
```

What the F-test is about, is whether in the true model (population data) the blue line coincides with the red line (i.e. the slope is equal to zero, which is only possible, when all parameters are zero). If given the data and the model, we have enough evidence to reject the null hypothesis, then this means that the slopes are probably different on the selected significance level.

Here is an example with the costs model discussed in this chapter with the pre-selected significance level of 5%:

```{r}
# Estimate the regression using lm 
costsModelMLRLm <- lm(overall~materials+size+projects+year,
                      data=SBA_Chapter_11_Costs)
# Get summary statistics
costsModelMLRF <- summary(costsModelMLRLm)$fstatistic
# Extract the F-value
costsModelMLRF[1]
# Calculate the critical value of F
qf(0.95, costsModelMLRF[2], costsModelMLRF[3])
```

In the output above, the critical value is lower than the calculated, so we can reject the H$_0$, which means that there is something in the model that explains the variability of the overall costs. Alternatively, we could focus on the p-value:
```{r}
# p-value from the test
pf(costsModelMLRF[1], costsModelMLRF[2], costsModelMLRF[3], lower.tail=FALSE)
```
We see that the the p-value is lower than the significance level of 5%, so we reject the H$_0$ on that level and come to the same conclusion as above. All of this is also provided by default in the summary of the model estimated using the `lm()` function:
```{r}
summary(costsModelMLRLm)
```

The summary for the `alm()` does not provide this information because it has a different philosophy behind it (likelihood, which we will discuss in Chapter \@ref(likelihoodApproach)).
