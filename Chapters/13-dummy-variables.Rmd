# Regression with categorical variables {#dummyVariables}
So far we assumed that the explanatory variables in the model are numerical (e.g. length of a tree or years of experience). But is it possible somehow to capture the effect of categorical variables on the variable of interest? Do we expect, for example, sales of silver mobile phone to differ from the sales of the pink one? We probably do, and this needs to be taken into account by a model.

Consider the same example with the overall costs of construction from Chapter \@ref(linearRegression). The type of building there is a categorical variable, which takes values:

1. Detached;
2. Semi-detached;
3. Other.

If we analyse the spread plot diagram for the original dataset, we will notice that the overall costs seem to change depending on the property type (Figure \@ref(fig:propertiesSpread)):

```{r propertiesSpread, fig.cap="Spread plots of the variables in the dataset."}
spread(SBA_Chapter_11_Costs)
```

In fact, judging by the boxplots in Figure \@ref(fig:propertiesSpread), we can see that material costs, sizes of properties and overall costs differ depending on the type of the property. So, this categorical variable has an affect on the project and needs to be taken into account.

If we had a sample large enough, we could split it into three sub-samples depending on the value of the variable and estimate three regression models. Unfortunately, this is neither always possible nor meaningful. If we fit three different models to the data, that would imply that the effect of size, materials costs, number of projects and year when the project started would differ depending on the type of the building. While this might hold for some relations, it is not universally the case. Besides, having three smaller samples instead of one big one, would imply the increased uncertainty about the estimates of parameters of a model (discussed in Chapter \@ref(uncertaintyRegression)), thus impacting the managerial decisions based on our model. So, is there a better way?

The answer to this, is "yes". This can be done by transforming variables from the categorical scale to the set of binary variables (i.e. taking values of zero or one). These binary variables are called "**dummy variables**" in the regression context. They denote when a specific attribute is met in the sample.

In this chapter we will learn how this information can be incorporated in a regression model and can be used for analysis and for forecasting.


## Dummy variables for the intercept
As we remember from Section \@ref(scales), the variables in categorical scale do not have distance or natural zero. This means that if we encode the values in numbers (e.g. "detached" - "1", "semi-detached" - "2", "other" - "3"), then these numbers will not have any proper mathematical meaning - they will only represent specific values (and order in case of ordinal scale), but we would be limited in what we can do with these values. As a reminder, the mean value in this case does not make any sense, and the median only makes sense for the ordinal scale.

To overcome this limitation, we could create a set of dummy variables, each of which would be equal to one if the specific object has the value of the original variable and zero otherwise. For example, for the property 7 in the dataset that we have used since Chapter \@ref(linearRegression), we have:

```{r}
SBA_Chapter_11_Costs[7,]
```

We see that the type of this building was "detached". So when we create the dummy variable "type_detached", we would encode it being equal to one for this and all the other observations that have this type, and to zero otherwise. We could do that manually using the following command in R:

```{r}
type_detached <- (SBA_Chapter_11_Costs$type=="detached")*1
```

which would create the variable `type_detached`, containing zeroes and ones. But this is a tedious way of creating dummy variables, there is a better one - using the `model.matrix()` function in R:

```{r}
# -1 is needed to tell the function to drop the intercept
house_types <- model.matrix(~type-1, SBA_Chapter_11_Costs)
head(house_types)
```

The resulting matrix contains three dummy variables, denoting "detached", "semi-detached" and "other" types of properties. If we visualise the overall costs vs the material costs for each one of them on the same plot, we might see whether there is a difference in relations between the variables. Here is one of possible ways of doing that (Figure \@ref(fig:costsPropertiesTypes)):

```{r costsPropertiesTypes, fig.cap="Scatterplot of material vs overall costs for the three property types."}
# The general plot
plot(SBA_Chapter_11_Costs[,c("materials","overall")])
# Plot for the detached houses
points(SBA_Chapter_11_Costs[house_types[,1]==1,c("materials","overall")],
       pch=16, col=2)
# We added LOWESS lines to see whether the relations change
lines(lowess(SBA_Chapter_11_Costs[house_types[,1]==1,c("materials","overall")]),
      col=2)
# Semi-detached
points(SBA_Chapter_11_Costs[house_types[,2]==1,c("materials","overall")],
       pch=16, col=3)
lines(lowess(SBA_Chapter_11_Costs[house_types[,2]==1,c("materials","overall")]),
      col=3)
# Other
points(SBA_Chapter_11_Costs[house_types[,3]==1,c("materials","overall")],
       pch=16, col=4)
lines(lowess(SBA_Chapter_11_Costs[house_types[,3]==1,c("materials","overall")]),
      col=4)
# Create the legend for convenience
legend("topleft", legend=levels(SBA_Chapter_11_Costs$type),
       col=c(2,3,4), lwd=1, pch=16)
```

It is hard to tell whether the relation changes (i.e. whether the material costs impact the overall costs differently for each of the property types), but it looks like the overall cost of semi-detached properties is higher than detached and other ones. If we were to construct several independent models, we would expect that on average, the semi-detached properties would cost more than the others, which should translate to the higher value of the intercept if we draw the line just through the cloud of the green points. The "other" should have the lowest intercept, because the points are concentrated at the bottom of the plot. Finally, the "detached" should be somewhere in the middle.

To make this switch more natural, we can introduce the dummy variables in regression model. What we could do in this case is have an intercept for one of the categories (for example, for `other`) and then have some parameters that would either increase or decrease it depending on the type of the property. Luckily, R can introduce them automatically, and we do not need to do anything additional to make it work. We only need to make sure that the respective categorical variable (`type` in our case) is encoded as a `factor`:

```{r}
class(SBA_Chapter_11_Costs$type)
```

And then we can add it in the regression model:
```{r}
costsModelWithType <- alm(overall~size+materials+projects+year+type,
                          SBA_Chapter_11_Costs, loss="MSE")
```

What R does in this case is expands the categorical variable in a set of dummies and uses the first level as a baseline, dropping it from the model (we'll discuss why in a moment). In our data, this level is "detached":

```{r}
levels(SBA_Chapter_11_Costs$type)
```

The summary of the produced model shows the values of the parameters for each of the levels of `type`:

```{r}
summary(costsModelWithType)
```

Each of the parameters for the variables `typesemi-detached` and `typeother` show how the intercept will change in comparison with the baseline (which is `detached`) if we have this type of property, i.e. whether the line will be above or below the line for the detached properties and by how much.

To understand this clearer, consider the example where we have a `type="detached"` property. In that case, the dummy variables `typesemi-detached` and `typeother` both would be equal to zero, which means that the respective parameters in the output above will be ignored. As a result, the intercept for that line would be `r round(coef(costsModelWithType)[1], digits)`. On the other hand, if we have the `semi-detached` property, the intercept will be `r round(coef(costsModelWithType)[1], digits)`  `r round(coef(costsModelWithType)["typesemi-detached"], digits)`= `r round(coef(costsModelWithType)[1], digits) +  round(coef(costsModelWithType)["typesemi-detached"], digits)`.

Now, why does R drop one of the levels? The simple explanation is that if we have those three dummy variables in the `house_types` variable, one of them can be considered redundant, because if we know that the property is neither detached, nor semi-detached, then it must be "other". See example of the fifth project:

```{r}
house_types[5,]
```

So, having all levels is simply unnecessary. But there is a mathematical explanation as well. It relates to the so called "dummy variables trap".

### Dummy variables trap
Consider a regression model without the dummy variables that we previously estimated in Section \@ref(OLSMLR):
```{r}
costsModel01 <- lm(overall~size+materials+projects+year, SBA_Chapter_11_Costs)
summary(costsModel01)
```

As discussed in Section \@ref(linearRegressionPrediction), this model can be written as:
\begin{equation}
    overall_j = 614.3227 + 1.3471 size_j + 0.8706 materials_j - 1.5921 projects_j - 0.1602 year_j + \epsilon_j .
    (\#eq:regressionOverall02)
\end{equation}
We can notice in the formula above, that we have a variable right after each of the parameters. In fact, the intercept can also be represented in the same form, the main difference is that it is multiplied by 1 instead of anything else: $614.3227 = 614.3227 \times 1$.

Now if we add all the three dummy variables in the model, for each specific observation, we will have an issue called "perfect multicollinearity" (we will discuss it in more detail in Section \@ref(assumptionsXregMulti)), where the included variables are linearly related and lead to estimation issues in the model. In our case, we can say that the sum of the three dummy variables for any specific observation equals to one (which is also that latent variable for the intercept that we mentioned above). So, by including all of them in the model together with the intercept, we will not be able to distinguish the specific effect of one variable on the response variable from the other one. Having the linear combination detached + semi-detached + other = 1, the estimates of the intercept and the three parameters for the `type` levels are not uniquely defined and can be anything as long as their combination leads to the line going through the specific segments of points.

To avoid the dummy variables trap, we can either drop one of levels (which is done by R automatically, as described above), or remove the intercept. The latter, however, has some consequences, because the intercept plays an important technical role, characterising where the line intersects the y-axis. If it is dropped, the model will imply that the intersection is happening at the origin (where all variables are equal to zero), and this might cause issues in the quality of a fit (see Section \@ref(linearRegressionMultipleQualityOfFit)).

Finally, it is recommended in general not to drop dummy variables one by one, if for some reason you decide that some of them are not helping. If, for example, we decide not to include `detached` and only have the model with `semi-detached`, then the meaning of the dummy variables will change - we will not be able to distinguish the detached from other properties. Furthermore, while some dummy variables might not seem important (or significant) in regression, their combination might improve the model, and dropping some of them might be damaging for the model in terms of its predictive power. So, it is more common either to include all levels (but one to avoid the trap) of categorical variable or not to include any of them.


### What about the variables in the ordinal scale?
In comparison with the nominal scale, the ordinal one has the "order" property. This means that we can say which of the levels can be placed higher than the others. If we evaluate all the property projects on the scale of "big", "medium" and "small", trying to capture the effort needed to construct specific buildings, that would be the ordinal scale. The logic temptation in this case is to assign some numbers to each of the elements of this scale, e.g. 1 for "big", 2 for "medium" and 3 for "small. The next step which is taken by some analysts is to include this new variable in regression as is. But there is an important aspect in such decision which needs to be taken into account: the ordinal scale does not have distance. This means that we could also assign 5, 8 and 13 to the specific levels, and we would not loose any original information. So, if you decide to include the variable in the regression model, you need to have very good motivation why the change from "big" to "medium" should have exactly the same effect as the change from "medium" to "small". Typically, this is not the case, and shifting the assigned numbers to capture such switch is too hard and fruitless. So, in general, **you should not include the ordinal variables in the regression model as is**, and you need to transform them into a set of dummy variables, similar to how it is done for the nominal scale.



## Categorical variables for the slope
In reality, we can have more complicated situations, when the change of price would lead to different changes in sales for different types of t-shirts. In this case, we are talking about an **interaction effect** between price and colour. The following artificial example demonstrates the situation:
```{r}
tShirtsInteraction <- cbind(rnorm(150,20,2),0,0,0)
tShirtsInteraction[1:50,2] <- tShirtsInteraction[1:50,1]
tShirtsInteraction[1:50+50,3] <- tShirtsInteraction[1:50+50,1]
tShirtsInteraction[1:50+50*2,4] <- tShirtsInteraction[1:50+50*2,1]
tShirtsInteraction <- cbind(1000 + tShirtsInteraction %*% c(-2.5, -1.5, -0.5, -4) +
                              rnorm(150,0,5), tShirtsInteraction)
colnames(tShirtsInteraction) <- c("sales","price","price:colourRed",
                                  "price:colourGreen","price:colourBlue")
```
This artificial data can be plotted in the following way to show the effect:
```{r tShirtsInteractionScatterPlot, fig.cap="Scatterplot of Sales vs Price of t-shirts of different colour, interaction effect."}
plot(tShirtsInteraction[,2:1])
abline(a=1000, b=-2.5-1.5, col="red")
abline(a=1000, b=-2.5-0.5, col="green")
abline(a=1000, b=-2.5-4, col="blue")
```

The plot on Figure \@ref(fig:tShirtsInteractionScatterPlot) shows that there are three categories of data and that for each of it, the price effect will be different: the increase in price by one unit leads to the faster reduction of sales for the blue t-shirts than for the others. Compare this with Figure \@ref(fig:tShirtsScatterPlot), where we had the difference only in intercepts. This implies a different model:
\begin{equation}
    sales_j = \beta_0 + \beta_1 price_j + \beta_2 price_j \times colourRed_j + \beta_3 price_j \times colourGreen_j + \epsilon_j .
    (\#eq:regressionDummies03)
\end{equation}
Notice that we still include only two dummy variables out of three in order to avoid the dummy variables trap. What is new in this case is the multiplication of price by the dummy variables. This trick allows changing the slope of price, depending on the colour of t-shirt. For example, here what the model \@ref(eq:regressionDummies03) would look like for the three types of colours:

1. Red colour: $sales_j = \beta_0 + \beta_1 price_j + \beta_2 price_j + \epsilon_j$ or $sales_j = \beta_0 + (\beta_1 + \beta_2) price_j + \epsilon_j$;
2. Green colour: $sales_j = \beta_0 + \beta_1 price_j + \beta_3 price_j + \epsilon_j$ or $sales_j = \beta_0 + (\beta_1 + \beta_3) price_j + \epsilon_j$;
3. Blue colour: $sales_j = \beta_0 + \beta_1 price_j + \epsilon_j$.

In R, the interaction effect can be introduced explicitly in the formula via `:` symbol if you have a proper factor variable:
```{r}
```

Note that the interpretation of parameters in such model will be different, because now the `price` shows the baseline effect for the blue t-shirts, while the interaction effects show how this effect will change for other colours. So, for example, in order to see what would be the effect of price change on sales of red t-shirts, we need to sum up the parameter for `price` and `price:colourRed`. We then can say that if price of red t-shirt increases by £1, the sales will decrease on average by
