[["index.html", "Statistics for Business Analytics Preface", " Statistics for Business Analytics Ivan Svetunkov 2021-12-11 Preface Have you encountered the term “Business Analytics” in your life? If not, then you are probably wondering what it means. If yes, then again, you are probably wondering what it means. It is a term that is used nowadays instead of such terms as “Operations Research” and “Management Science.” It is a discipline that covers a variety of qualitative and quantitative methods that can be used in practice for real life decisions. It uses methods and approaches from different scientific areas, including statistics, forecasting, optimisation, operations management etc. This textbook is focused on the core of quantitative side of the discipline - statistics. While there are many books on statistics, the author failed to find one that would be focused on the application side, discuss the application of statistics both for analysis and forecasting and would rely on modern statistical approaches. This textbook relies heavily on the greybox package for R, which focuses on forecasting using regression models. In order to run examples from the textbook, you would need to install this package (Svetunkov, 2021a): install.packages(&quot;greybox&quot;) A very important thing to note is that this textbook does not use tidyverse packages. I like base R, and, to be honest, I am sure that tidyverse packages are great, but I have never needed them in my research. So, I will not use pipeline operators, tibble or tsibble objects and ggplot2. It is assumed throughout the textbook that you can do all those nice tricks on your own if you want to. If you want to get in touch with me, there are lots of ways to do that: comments section on any page of my website, my Russian website, vk.com, Facebook, Linkedin, Twitter. You can also find me on ResearchGate, StackExchange and StackOverflow, although I’m not really active there. Finally, I also have GitHub account. You can use the following to cite the online version of this book: Svetunkov, I. (2021) Statistics for Business Analytics: Lancaster, UK. openforecast.org/sba. Accessed on [current date]. If you use LaTeX, the following can be used instead: @MISC{SvetunkovSBA, title = {Statistics for Business Analytics}, author = {Ivan Svetunkov}, howpublished = {OpenForecast}, note = {(version: [current date])}, url = {https://openforecast.org/sba/}, year = {2021} } License This textbook is licensed under Creative Common License by-nc-sa 4.0, which means that you can share, copy, redistribute and remix the content of the textbook for non-commercial purposes as long as you give appropriate credit to the author and provide the link to the original license. If you remix, transform, or build upon the material, you must distribute your contributions under the same CC-BY-NC-SA 4.0 license. See the explanation on the Creative Commons website. References "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Whenever we talk about statistics, analytics or forecasting, we deal with models that are constructed based on available information. So, it is important to understand what a model is, what types of models exist and how to measure information in order to use it in models afterwards. This is what we will discuss in this chapter. "],["what-is-model.html", "1.1 What is model?", " 1.1 What is model? Reality is complex. Everything is connected with everything, and it is difficult to isolate an object or a phenomenon from the other objects or phenomena and their environment. Furthermore, due to this complexity of reality, we cannot work with itself, we need to simplify it, leave only the most important parts and analyse them. This process of simplification implies that we create a model of reality, work with it and make conclusions about the reality based on it. Pidd (2010) defines model as “an external and explicit representation of part of reality as seen by the people who wish to use that model to understand, to change, to manage and to control that part of reality.” Let us analyse this definition. It is external and explicit, because if you only think about something, it is not a model. The unclear view on an object is not a model, it needs to be formulated. It is a representation of part of reality, because it is not possible to represent the reality at full - it is too complex, as discussed above. Technically speaking, the model without a purpose is still a model, but Pidd (2010) points out in his definition that without it, the model becomes useless (thus “to understand, to change, to manage and to control”). Finally, as seen by people is an important element that shows that models are always subjective. One and the same question can be answered with different models based on preferences of analyst. This definition is wide and covers different types of models, starting from simple graphical ones and ending with complex imitations. In fact, there are four fundamental types of models (they are ordered by the increase of complexity): Textual, Visual, Mathematical, Imitation. Textual model is just a description of an object or a process. An instruction of how to assemble a chair is an example of a textual model. Any classification will be a textual model as well, so the list of four types of models is a textual model. Visual model is a graphical or a schematic representation of an object or a process. An example of such a model is provided in Figure 1.1. Figure 1.1: Chair assembly instruction. Found on Reddit. Mathematical model is a model that is represented using equations. It is more complex than the previous two, because it requires an understanding of mathematics. At the same time it can be more precise than the previous two models in terms of capturing the structure of reality and making predictions about it. The mass-energy equivalence equation is an example of such model: \\[\\begin{equation*} E = m c^2 . \\end{equation*}\\] A mathematical model in turn can be either deterministic or stochastic. The former one assumes that there is no randomness in it, while the latter implies that the randomness exists and can be modelled in one way or another. The related classification of models based on the amount of randomness is: White box - the deterministic model. An example of such model is a linear programming, which assumes that there is no randomness in the data; Grey box - the model that assumes some randomness, but for which the structure is known (or assumed). Any statistical model can be considered as a grey box: typically, we have an understanding of how elements in it interact with each other and how the result is obtained; Black box - the model with randomness, for which we do not know what is happening inside. An example of such model is an artificial neural network. Finally, the imitation model is a simplified reproduction of a real object or a process. This can be, for example, a physical model of a building standing in a room of an architect, or a mental arrangement in psychology. In this textbook, we will deal with only first three types of models, focusing on the third one. When constructing mathematical models, we will inevitably deal with variables, with factors that are potentially related to each other and reflect some aspects of the real object or phenomenon. These variables can be split into two categories: Input, or external, or exogenous, or explanatory variables - those that are provided to us and are assumed to impact the variable (or several variables) of interest; Output, or internal, or endogenous, or response variable(s) - the variable of the main interest, which is assumed to be related by a set of explanatory variables. The models that have only one response variable are called “univariate” models. But in some cases, we might have several response variables (for example, sales of several similar products). We would then deal with a multivariate model. In the literature, you might meet a different definition of univariate / multivariate models. For example, some consider a model with several variables multivariate, even if it has only one response and several explanatory ones. But throughout this textbook we use the definitions above, focused on response variable. 1.1.1 Models, methods et al.  There are several other definitions that will be useful throughout this textbook: Statistical model (or ‘stochastic model,’ or just ‘model’ in this textbook) is a ‘mathematical representation of a real phenomenon with a complete specification of distribution and parameters’ (Svetunkov and Boylan, 2019). Very roughly, the statistical model is something that contains a structure (defined by its parameters) and a noise that follows some distribution. True model is the idealistic statistical model that is correctly specified (has all the necessary components in correct form), and applied to the data in population. By this definition, true model is never reachable in reality, but it is achievable in theory if for some reason we know what components and variables and in what form should be in the model, and have all the data in the world. The notion itself is important when discussing how far the model that we use is from the true one. Estimated model (aka ‘applied model’ or ‘used model’) is the statistical model that was constructed and estimated on the available sample of data. This typically differs from the true model, because the latter is not known. Even if the specification of the true model is known for some reason, the parameters of the estimated model will differ from the true parameters due to sampling randomness, but will hopefully converge to the true ones if the sample size increases. Data generating process (DGP) is an artificial statistical model, showing how the data could be generated in theory. This notion is utopic and can be used in simulation experiments in order to check, how the selected model with the specific estimator behave in a specific setting. In real life, the data is not generated from any process, but is usually based on complex interactions between different agents in a dynamic environment. Note that I make a distinction between DGP and true model, because I do not think that the idea of something being generated using a mathematical formula is helpful. Many statisticians will not agree with me on this distinction. Forecasting method is a mathematical procedure that generates point and / or interval forecasts, with or without a statistical model (Svetunkov and Boylan, 2019). Very roughly, forecasting method is just a way of producing forecasts that does not explain how the components of time series interact with each other. It might be needed in order to filter out the noise and extrapolate the structure. Mathematically in the simplest case the true model can be presented in the form: \\[\\begin{equation} y_t = \\mu_{y,t} + \\epsilon_t, \\tag{1.1} \\end{equation}\\] where \\(y_t\\) is the actual observation, \\(\\mu_{y,t}\\) is the structure in the data and \\(\\epsilon_t\\) is the noise with zero mean, unpredictable element, which arises because of the effect of a lot of small factors and \\(t\\) is the time index. An example would be the daily sales of beer in a pub, which has some seasonality (we see growth in sales every weekends), some other elements of structure plus the white noise (I might go to a different pub, reducing the sales of beer by one pint). So what we typically want to do in forecasting is to capture the structure and also represent the noise with a distribution with some parameters. When it comes to applying the chosen model to the data, it can be presented as: \\[\\begin{equation} y_t = \\hat{\\mu}_{y,t} + e_t, \\tag{1.2} \\end{equation}\\] where \\(\\hat{\\mu}_{y,t}\\) is the estimate of the structure and \\(e_t\\) is the estimate of the white noise (also known as “residuals”). As you see even if the structure is correctly captured, the main difference between (1.1) and (1.2) is that the latter is estimated on a sample, so we can only approximate the true structure with some degree of precision. If we generate the data from the model (1.1), then we can talk about the DGP, keeping in mind that we are talking about an artificial experiment, for which we know the true model and the parameters. This can be useful if we want to see how different models and estimators behave in different conditions. The simplest forecasting method can be represented with the equation: \\[\\begin{equation} \\hat{y}_t = \\hat{\\mu}_{y,t}, \\tag{1.3} \\end{equation}\\] where \\(\\hat{y}_t\\) is the point forecast. This equation does not explain where the structure and the noise come from, it just shows the way of producing point forecasts. In addition, we will discuss in this textbook two types of models: Additive, where (most) components are added to one another; Multiplicative, where the components are multiplied. (1.1) is an example of additive error model. A general example of multiplicative error model is: \\[\\begin{equation} y_t = \\mu_{y,t} \\varepsilon_t, \\tag{1.4} \\end{equation}\\] where \\(\\varepsilon_t\\) is some noise again, which in the reasonable cases should take only positive values and have mean of one. We will discuss this type of model later in the textbook. We will also see several examples of statistical models, forecasting methods, DGPs and other notions and discuss how they relate to each other. Note that throughout this textbook we will use index \\(t\\) to denote the time and index \\(i\\) to denote the cross-sectional elements. So, for example, \\(y_t\\) will mean the response variable changing over time, while \\(y_i\\) will mean the response variable changing from one object to another (for instance, from one person to another). References "],["scales.html", "1.2 Scales of information", " 1.2 Scales of information Whenever we work with information, we need to understand how to measure it properly. If we cannot do that, then we cannot construct any model and make proper decisions, supported by evidence. For example, if a person feels ill but we cannot say what the temperature of their body is, then we cannot decide, whether anything needs to be done to reduce it. If we can measure something then we can model it and produce forecasts. Continuing our example, if the temperature is 39°C, then we can conclude that the person is sick and needs to take Paracetamol or some other pills that would reduce the temperature. So, whenever we collect some sort of information about a system’s behaviour or about a process, we will inevitably deal with scales of information and it is important to understand what we are dealing with in order to process that information correctly. There are four fundamental scales: Nominal, Ordinal, Interval, Ratio. The first two form the so called “categorical” scale, while the latter two are typically united in the “numerical” one. Each one of these scales can have one of the following characteristics: Description, Order, Distance, Natural zero, Natural unit. The last characteristics is typically ignored analytics and forecasting as it does not provide any useful information. But as for the other four, they provide important properties to the scales of information, giving them more flexibility. Here we discuss the scales in detail. 1.2.1 Nominal scale This is the scale that only has “description” characteristics. It does not have an order, a distance or a natural zero. There is only one operation that can be done in this scale, and it is comparison, whether the value is “equal” or “not equal” to something. An example of data measured in such scale is the following question in a survey: What is your nationality? Russian, English, Greek, Swiss, Belgian, Lebanese, Indonesian, Other. In this case after collecting the data we can only say whether each respondent is Russian or not, English or not etc. So, the only thing that can be done with the data measured in this scale is to produce a basic summary, showing how many people selected one option or another. Among the statistical instruments, only the mode is useful, as it shows which of the options was selected the most. If there are several variables measured in nominal scale, we can calculate some measures of association to see if there are any patterns in respondents behaviour (e.g. those who select “Russian” would prefer Vodka, while those who selected “Belgian” will tend to drink “Beer”). When it comes to constructing models, the nominal scale is typically transformed in a set of dummy variables, which will be discussed later in regression analysis of this textbook. If you are not sure, whether your data is measured in nominal or another scale, you can do a simple test: if changing the places of two values does not break the scale, then this is the nominal one. For example, in the question above, moving “Greek” to the first place will not change anything, so this is indeed the nominal scale. Another example of nominal scale is the number on the T-shirt of football players. They are only descriptive, and if two players change numbers, this will not change anything (although it might confuse football fans). 1.2.2 Ordinal scale In addition to description, the ordinal scale has the “order.” It is possible to say that one value can be placed higher or lower than the other on a scale (thus, permitting operations “greater” and “smaller” in addition to the “equal” and “not equal”) However, it is not possible to say how far the elements are placed from each other, so the number of operations in the scale is still limited. Here is an example of a survey question with such scale: How old are you? Too young, Young, Not too young, Not too old Old, Too old. In this scale above we have a natural order, and when collecting the data in this scale we can conclude, whether a respondent is older than another one or not. Sometimes ordinal scales look confusing and seem to be of a higher level than they are, here is an example: How old are you? Younger than 16, 16 - 25, 26 - 40, 41 - 60 Older than 60. This is still an ordinal scale, because it has the natural order, and because we cannot measure the distance between the value: if, for example we subtract “16 - 25” from “26 - 40,” we will not get anything meaningful. The ordinal scale, being more complex than the nominal one, allows using some additional statistical instruments (besides the mode), such as quantiles of distribution, including median. Unfortunately, the arithmetic mean is not applicable to the data in ordinal scale, because of the absence of distance. Even if you encode every answer in numbers, the resulting average will not be meaningful. Indeed, if in the question above with the five options, we use the numbers (“1” for the first option, “2” for the second one, etc.) and take average, the resulting number of, for example, 3.75 will not mean anything, as there is no element in the scale that would correspond to that number. When it comes to measuring relations between two variables in ordinal scale, we can use Kendall’s \\(\\tau\\) correlation coefficient, Goodman-Kruskal’s \\(\\gamma\\) and Yule’s Q. These are discussed in detail in Section 6. As for using the variables in ordinal scale in modelling, the typical thing to do would be to create a set of dummy variables, similarly to how it is done for variables in nominals scale. As for the identification of scale, if in doubt, you can do any transformation of elements of scale without the loss of its meaning. For example, if we assign numbers from 1 to 5 to the responses above, we can square each one of them and get 1, 4, 9, 16 and 25, which would not change the original scale, but only encode the answers differently (select “16” for the option “41 - 60”). 1.2.3 Interval scale This scale is even more complex than the previous two, as in addition to description and order it also has a distance. This permits doing addition and subtraction to the elements of scale, which are meaningful operations in this case. Arithmetic mean and standard deviation become available in this scale in addition to all those used in lower level scales discussed above. The classical example of a variable measured in this scale is the temperature. Indeed, we can not only say if the temperature of one person is higher than the temperature of the other one, but also by how much: 39°C - 37°C = 2°C, which is a meaningful number in the scale. The only limitation in this scale is that there is no natural zero. 0°C does not mean the absence of temperature, but rather means the point at which water starts freezing. If we switch to Fahrenheit (although why would anyone do that?!), then the 0°F would correspond to the point, where the mixture of ice, water, and ammonium chloride used to stabilise back in 1724, when Fahrenheit proposed the scale. Almost all descriptive statistics are meaningful in this scale (see Section 2). This includes, but is not limited with mean, variance, skewness, kurtosis. Coefficient of variation and other statistics, where division is done by an actual value or mean, are not meaningful, because the scale does not have a meaningful zero. For example, switching from Celsius to Fahrenheit would change the value of coefficient of variation, although the distribution of the variable will stay the same. Furthermore, some error measures (see Chapter 2 of Svetunkov, 2021b) cannot be used for the measurement of accuracy of models for this scale (for example, MAPE cannot be used as it assumes meaningful zero). The relation between two variables in interval scale can be measured by Pearson’s correlation coefficient. The scale can be used in the model as is. Finally, when it comes to the identification of scale, only linear transformations are permitted for the variables without the loss of its properties. This means that if we measure temperature of two respondents and then do their linear transformations via \\(y=a+bx\\), then the characteristics of scale will not be broken: it will still have description, order and distance with the same meaning as prior to the transformation. In the example of temperature, this is how you switch, for example, from Celsius to Fahrenheit (\\(y=a+bx\\)). 1.2.4 Ratio scale The most complex of the four, this ratio has a natural zero (in addition to all the other characteristics). It permits any operations to the values of scale, including product, division, and non-linear transformations. Coefficient of variation can be used together in addition to all the previous instruments. An example of the information measured in this scale is the height of respondents in meters. You can compare two respondents via their height and not only say that one is higher than the other, but also by how much and how many times. All these operations will be meaningful in this scale. If you need to check, whether the variables is indeed in ratio scale, note that only the transformation via multiplication would maintain the meaning of the scale. For example, height measured in meters can be transformed into height in feet via the multiplication by approximately 3.28. If you add a constant to the values of scale, it will break it. All the statistics and error metrics work for the variables measured in this scale. Furthermore, being the most complex, this scale also permits usage of all correlation coefficients. Finally, the variables measured in this scale can be either integer or continuous. This might cause some confusions, because the integer numbers sometimes look suspiciously similar to the values of ordinal scale, but the tools of identification discussed above might help. If a company needs to buy 7 planes, then this is an integer variable measured in ratio scale: 7 planes is more than 6 planes by one plane, and zero planes means that there are no planes (all the characteristics of ratio scale). Furthermore, squaring the number of planes breaks the distance between them (\\(7^2 - 6^2 \\neq 1^2\\)), while linear transformation breaks the scale (\\(7\\times 2 + 3\\) has a completely different meaning in the scale than just 7). References "],["sourcesOfUncertainty.html", "1.3 Sources of uncertainty", " 1.3 Sources of uncertainty When estimating any model on a sample of data, we will inevitably have to deal with uncertainty. Consider an example, when we want to estimate the average height of a person in the room. We could take heights of all the people in the room, then take average and we would get our answer. But what would happen with that average if another person comes in the room? We would need to do additional measures and re-estimate the average, and inevitably it will be different from the one we had before. This example demonstrates one of the classical sources of uncertainty - the one caused by estimation on a sample of data. Furthermore, we might be interested in predicting the weight of a person based on their height. The two variables will be related, but would not have a functional relation: with the increase of height we expect that a person will weigh more, but this only holds on average. So, based on a sample of data, we could estimate the relation between the two variables and then having a height of a person, we could predict their expected weight. Their individual weight will inevitably vary from one person to another. This is the second source of uncertainty, appearing because of the individual discrepancies from one person to another. Finally, the model of weight from height could be wrong for different reasons. For example, there might be plenty of other factors that would impact the weight of person that we have not taken into account. In fact, we never know the true model (see Section 1.1.1), so this is the third source of uncertainty, the one around the model form. These three sources of uncertainty have been summarised for the first time in Chatfield (1996). Whenever we need to construct any type of model, we will deal with: Uncertainty about the data, e.g. the error term \\(\\epsilon_t\\) (see Section 5); Uncertainty about estimates of parameters (see Section 9); Uncertainty about the model form (see Section 14). In this textbook we will discuss all of them, slowly moving from (1) to (3), introducing more advanced techniques for model building. References "],["dataAnalysis.html", "Chapter 2 Preliminary data analysis", " Chapter 2 Preliminary data analysis One of the basic thing that is worth doing before starting any modelling is the preliminary data analysis. This can be done either using numerical or graphical analysis. The former is useful when you want to have a summary information about the data without trying to find detailed information about it. The latter is useful when you can spend more time, investigating relations and issues in the data. In many cases, they compliment each other. "],["dataAnalysisNumerical.html", "2.1 Numerical analysis", " 2.1 Numerical analysis In this section we will use the classical mtcars dataset from datasets package for R. It contains 32 observations with 11 variables. While all the variables are numerical, some of them are in fact categorical variables encoded as binary ones. We can check the description of the dataset in R: ?mtcars Judging by the explanation in the R documentation, the following variables are categorical: vs - Engine (0 = V-shaped, 1 = straight), am - Transmission (0 = automatic, 1 = manual). In addition, the following variables are integer numeric ones: cyl - Number of cylinders, hp - Gross horsepower, gear - Number of forward gears, carb - Number of carburetors. All the other variables are continuous numeric. Takign this into account, we will create a data frame, encoding the categorical variables as factors for further analysis: mtcarsData &lt;- data.frame(mtcars) mtcarsData$vs &lt;- factor(mtcarsData$vs,levels=c(0,1),labels=c(&quot;V-shaped&quot;,&quot;Straight&quot;)) mtcarsData$am &lt;- factor(mtcarsData$am,levels=c(0,1),labels=c(&quot;automatic&quot;,&quot;manual&quot;)) Given that we only have two options in those variables, it is not compulsory to do this encoding, but it will help us in the further analysis. We can start with the basic summary statistics. We remember from the scales of information (Section 1.2) that the nominal variables can be analysed only via frequencies, so this is what we can produce for them: table(mtcarsData$vs) ## ## V-shaped Straight ## 18 14 table(mtcarsData$am) ## ## automatic manual ## 19 13 These tables are called contingency tables, they show the frequency of appearance of values of variables. Based on this, we can conclude that the cars with V-shaped engine are met more often in the dataset than the cars with the Straight one. In addition, the automatic transmission prevails in the data. The related statistics which is useful for analysis of categorical variables is called mode. It shows which of the values happens most often in the data. Judging by the frequencies above, we can conclude that the mode for the first variable is the value “V-shaped.” All of this is purely descriptive information, which does not provide us much. We could probably get more information if we analysed the contingency table based on these two variables: table(mtcarsData$vs,mtcarsData$am) ## ## automatic manual ## V-shaped 12 6 ## Straight 7 7 For now, we can only conclude that the cars with V-shaped engine and automatic transmission are met more often than the other cars in the dataset. Next, we can look at the numerical variables. As we recall from Section 1.2, this scale supports all operations, so we can use quantiles, mean, standard deviation etc. Here how we can analyse, for example, the variable mpg: setNames(mean(mtcarsData$mpg),&quot;mean&quot;) ## mean ## 20.09062 quantile(mtcarsData$mpg) ## 0% 25% 50% 75% 100% ## 10.400 15.425 19.200 22.800 33.900 setNames(median(mtcarsData$mpg),&quot;median&quot;) ## median ## 19.2 The output above produces: Mean - the average value of mpg in the dataset, \\(\\bar{y}=\\frac{1}{n}\\sum_{j=1}^n y_j\\). Quantiles - the values that show, below which values the respective proportions of the dataset lie. For example, 25% of observations have mpg less than 15.425. The 25%, 50% and 75% quantiles are also called 1st, 2nd and 3rd quartiles respectively. Median, which splits the sample in two halves. It corresponds to the 50% quantile. If median is greater than mean, then this typically means that the distribution of the variable is skewed (it has some rare observations that have large values). Making a step back, we also need to mention the variance, which shows the overall variability of the variable around its mean: \\[\\begin{equation} \\mathrm{V}(y)= \\frac{1}{n-1}\\sum_{j=1}^n (y_j - \\bar{y})^2 . \\tag{2.1} \\end{equation}\\] Note that the division in (2.1) is done by \\(n-1\\), and not by \\(n\\). This is done in order to correct the value for the in-sample bias (we will discuss this in Subsection 4.3.1). The number itself does not tell us much about the variability, but having it allows calculating other more advanced measures. Typically the square root of variance is used in inferences. It is called standard deviation: \\[\\begin{equation} \\hat{\\sigma} = \\sqrt{\\mathrm{V}(y)} , \\tag{2.2} \\end{equation}\\] it has the same scale as the variable \\(y_j\\). In our example, both can be obtained via: var(mtcarsData$mpg) ## [1] 36.3241 sd(mtcarsData$mpg) ## [1] 6.026948 Coming back to the point about the asymmetry of the distribution of a variable in our example, we can investigate it further using skewness and kurtosis from timeDate package: timeDate::skewness(mtcarsData$mpg) ## [1] 0.610655 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; timeDate::kurtosis(mtcarsData$mpg) ## [1] -0.372766 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; Skewness shows the asymmetry of distribution. If it is greater than zero, then the distribution has the long right tail. If it is equal to zero, then it is symmetric. It is calculated as: \\[\\begin{equation} \\mathrm{skew}(y)= \\frac{1}{n}\\sum_{j=1}^n \\frac{(y_j - \\bar{y})^3}{\\hat{\\sigma}^3} . \\tag{2.3} \\end{equation}\\] Kurtosis shows the excess of distribution (fatness of tails) in comparison with the normal distribution. If it is equal to zero, then it is the same as for the normal distribution. Here how it is calculated: \\[\\begin{equation} \\mathrm{kurt}(y)= \\frac{1}{n}\\sum_{j=1}^n \\frac{(y_j - \\bar{y})^4}{\\hat{\\sigma}^4} - 3 . \\tag{2.4} \\end{equation}\\] Note that there is \\(3\\) in the formula. This is because the excess (which is the value without 3) of Normal distribution is equal to 3. Instead of dividing the value by 3 (which would make kurtosis easier to interpret), Karl Pearson has decided to use subtraction. The same formula (2.4) can be rewritten as: \\[\\begin{equation} \\mathrm{kurt}(y)= \\frac{1}{n}\\sum_{j=1}^n \\left(\\frac{y_j - \\bar{y}}{\\hat{\\sigma}}\\right)^4 - 3 . \\tag{2.5} \\end{equation}\\] Analysing the formula (2.5), we see that the impact of the deviations lying inside one \\(\\hat{\\sigma}\\) bounds are reduced, while those that lie outside, are increased. e.g. if the value \\(y_j - \\bar{y} &gt; \\hat{\\sigma}\\), then the value in the ratio will be greater than one (e.g. 2), thus impacting the final value (e.g. \\(2^4=16\\)), while in the opposite case of \\(y_j - \\bar{y} &lt; \\hat{\\sigma}\\), the ratio will be diminished (e.g. \\(0.5^4 = 0.0625\\)). After summing up all \\(n\\) values in (2.5), the values outside one \\(\\hat{\\sigma}\\) will have a bigger impact on the resulting kurtosis than those lying inside. So, kurtosis will be higher for the distributions with longer tails and in the cases, when there are outliers in the data. Based on all of this, we can conclude that the distribution of mpg is skewed and has the longer right tail. This is expected for such variable, because the cars that have higher mileage are not common in this dataset. All the conventional statistics discussed above can be produced using the following summary for all variables in the dataset: summary(mtcarsData) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs am ## Min. :2.760 Min. :1.513 Min. :14.50 V-shaped:18 automatic:19 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 Straight:14 manual :13 ## Median :3.695 Median :3.325 Median :17.71 ## Mean :3.597 Mean :3.217 Mean :17.85 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 ## Max. :4.930 Max. :5.424 Max. :22.90 ## gear carb ## Min. :3.000 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:2.000 ## Median :4.000 Median :2.000 ## Mean :3.688 Mean :2.812 ## 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :5.000 Max. :8.000 "],["dataAnalysisGraphical.html", "2.2 Graphical analysis", " 2.2 Graphical analysis Continuing our example with mtcars dataset, we now investigate what plots can be used for different types of data. As discussed earlier, we have two categorical variables: vs and am - and they need to be treated differently than the numerical ones. We can start by producing their barplots: barplotVS &lt;- barplot(table(mtcarsData$vs), xlab=&quot;Type of engine&quot;) text(barplotVS,table(mtcarsData$vs)/2,table(mtcarsData$vs),cex=1.25) Figure 2.1: Barplot for the engine type. This is just a graphical presentation of the contingency table we have already discussed earlier. Note that histograms do not make sense in case of categorical variables, because they assume that variables are numerical and continuous. Barplots are useful when you deal with either categorical variables or integer numerical ones. Here is what we can produce in case of the integer variable cyl: barplotCYL &lt;- barplot(table(mtcarsData$cyl), xlab=&quot;Number of cylinders&quot;) text(barplotCYL,table(mtcarsData$cyl)/2,table(mtcarsData$cyl),cex=1.25) Figure 2.2: Barplot for the number of cylinders. Figure 2.2 shows that there are three types of cars in the data: with 4, 6 and 8 cylinders. The most frequently met is the car with 8 cylinders. Judging by the plot, half of cars have not more than 6 cylinders (median is equal to 6). All of this can be deducted from the barplot. And here how the histogram would look like for cylinders: hist(mtcarsData$cyl) Figure 2.3: Histogram for the number of cylinders. Do not do this! Figure 2.3 is difficult to read, because on histogram, the bars show frequency at which continuous variable appears in pre-specified bins. In our case we would conclude that the most frequently cars in the dataset are those that have 7.5 - 8 cylinders, which is wrong and misleading. In addition, this basic plot does not have a readable label for x-axis and a meaningful title (in fact, we do not need one, given that we have caption). So, always label your axis and make sure that the text on plots is easy to understand for those people who do not work with the data. Coming back to categorical variables, we can construct two-dimensional plots to investigate potential relations between variables. We will first try the same barplot as above, but with vs and am variables: barplot(table(mtcarsData$vs,mtcarsData$am), xlab=&quot;Type of transmission&quot;, legend.text=levels(mtcarsData$vs)) Figure 2.4: Barplot for the type of engine and transmission. Figure 2.4 provides some information about the distribution of type of engine and transmission. For example, we can say that the most often met car in the dataset is the one with automatic transmission and V-shaped engine. However, it is not possible to say much about the relation between the two variables based on this plot. So, there is an alternative presentation, which uses the heat map (tableplot() from greybox): tableplot(mtcarsData$vs,mtcarsData$am, xlab=&quot;Type of engine&quot;, ylab=&quot;Type of transmission&quot;) Figure 2.5: Heat map for the type of engine and transmission. The idea of this plot is that the darkness of areas shows the frequency of occurrence of each specific value. The numbers inside the box show the proportions for each answer. So, we can conclude (again), that automatic transmission with V-shaped engine is met in 37.5% of cases. On the other hand, the least frequent type of car is the one with V-shaped engine and manual transmission. There might be some tendency in the dataset: the engine and transmission might be related (v-shaped with automatic vs Straigh with manual) - but it is not very well pronounced. The same table plot can be used for the analysis of relations between integer variables (and categorical). Here, for example, the plot between the number of cylinders and the type of engine: tableplot(mtcarsData$cyl,mtcarsData$vs, xlab=&quot;Number of cylinders&quot;, ylab=&quot;Type of engine&quot;) Figure 2.6: Heat map for the number of cylinders and the type of engine. Figure 2.6 allows making more solid conclusions about the relation between the two variables: we see that with the increase of the number of cylinders, the cars tend to switch from Straight to the V-shaped engines. This has an explanation: the engines with more cylinders need to have a different geometry to fit them all, and the V shape is more suitable for them. The table plot shows clearly this relation between the two variables. Next, we can analyse the numerical variables. We can start with the basic histogram: hist(mtcarsData$wt, xlab=&quot;Weight&quot;, main=&quot;&quot;, probability=TRUE) lines(density(mtcarsData$wt), col=&quot;red&quot;) Figure 2.7: Distribution of the weights of cars. The histogram 2.7 shows that there is a slight skewness in the data: the cars with weight from 3 to 4 thousands pounds are met more often than the cars with more than 5. The left tail of this distribution is slightly longer than the right one. Note that I have produced the probabilities on the y-axis of the plot in order to add the density curve, which smooths out the frequencies and shows how the distribution looks like. An alternative presentation of the histogram is the boxplot, which graphically presents quantiles of distribution: boxplot(mtcarsData$wt, ylab=&quot;Weight&quot;) points(mean(mtcarsData$wt),col=&quot;red&quot;, pch=16) Figure 2.8: Boxplot of the variable weight. This plot has the box in the middle, the whiskers on the sides, points at the top and the red point at the centre. The box shows 1st, 2nd and 3rd quartiles of distribution, thus the black line in the middle is the median. The distance between the 1st and the 3rd quartiles is called “Interquartile range” (IQR) and is used for the calculation of the interval (1st / 3rd quartile \\(\\pm 1.5 \\times\\)IQR), which corresponds roughly to the 99.3% interval (read more about this in Section ??) from Normal distribution and is graphically drawn as the furthest observation in the interval. So, the lower whisker on our plot corresponds to the minimum value in the data, which is still in the interval, while the upper whisker corresponds to the bound of the interval. All the observations that lie beyond the interval are marked as potential outliers. Note that this does not mean that the values are indeed outliers, they just lie outside the 99.3% interval of Normal distribution. Finally, the red dot was added by me to show where the mean is. It is lower than median, this implies that there is a slight skewness in the distribution of weight. There is also a way for producing the plots that would combine elements of histogram, density curve and boxplot. There is a plot called “violin plot.” We will use vioplot() function from vioplot package in order to produce them: vioplot(mtcarsData$wt, ylab=&quot;Weight&quot;) points(mean(mtcarsData$wt),col=&quot;red&quot;, pch=16) Figure 2.9: Violin plot together with boxplot of the variable weight. Figure 2.9 unites the boxplot and the density curve from the plots above, providing not only information about the quantiles, but also about the shape of the distribution. Finally, if we want to compare the distribution of a variable with a known theoretical distribution, we can produce the QQ-plot. Here how it looks for Normal distribution: qqnorm(mtcarsData$wt) qqline(mtcarsData$wt) Figure 2.10: QQ plot of Normal distribution for variable weight. The idea of the plot on Figure 2.10 is to compare theoretical quantiles with the empirical ones. If the variable would follow the specific distribution, then all the points would lie on the solid line. In our case, they do not: there are points in the right tail that are very far from the line - so we would conclude that the distribution of weight does not look Normal. So far, we have discussed the univariate analysis of numerical variables, but we can also produce plots showing potential relations between them. We start with the classical scatterplot: plot(mtcarsData$wt, mtcarsData$mpg, xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;) lines(lowess(mtcarsData$wt, mtcarsData$mpg), col=&quot;red&quot;) Figure 2.11: Scatterplot diagram between weight and mileage. The plot on Figure 2.11 shows the observations that have specific weight and mileage. Based on this, we can see if there is a relation between variables or not and what sort of relation this is. In order to simplify analysis, I have added the lowess line to the plot. It smooths the relation between variables, drawing the smooth line through the points and helps in understanding the existing relations in the data. Judging by Figure 2.11, there is a negative, slightly non-linear relation between the variables: the mileage decreases with reduced speed, when weight of a car increases. This relation makes sense, because heavier cars will consume more fuel and thus drive less on a gallon of petrol. We could construct similar plots for all the other numerical variables, but not all plots would be helpful. For example, a plot of mileage versus number of forward gears would be very difficult to read (see Figure 2.12). plot(mtcarsData$gear, mtcarsData$mpg, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) Figure 2.12: Scatterplot diagram between weight and mileage. This is because one of the variables is integer and takes only a handful of values. In this case, a boxplot or a violin plot would be more useful: boxplot(mpg~gear, mtcarsData, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=&quot;red&quot;, pch=16) Figure 2.13: Boxplot of mileage vs number of gears. The plot on Figure 2.13 is more informative than the one on Figure 2.12: it shows how the distribution of mileage changes with the increase of the numeric variable number of gears. We can also see that the mean value first increases and then goes down slightly. I do not have any good explanation of this phenomenon, but it might be related with how efficient the cars become with the increase fo the number of gears, or this could happen due to some latent, unobserved factors. So, the data tells us that there is a non-linear relation between number of gears and mileage. Similarly, we can produce violin plots for the same data using the following code: vioplot(mpg~gear, mtcarsData, xlab=&quot;Number of gears&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$gear, mean), col=&quot;red&quot;, pch=16) Finally, using exactly the same idea with boxplots / violin plots, we can analyse relations between categorical and numerical variables. Figure 2.14 shows the relation between transmission type and mileage. We can conclude that the cars with manual transmission tend to have a higher mileage than the ones with the automatic one in our dataset. vioplot(mpg~am, mtcarsData, xlab=&quot;Transmission type&quot;, ylab=&quot;Mileage&quot;) points(tapply(mtcarsData$mpg, mtcarsData$am, mean), col=&quot;red&quot;, pch=16) Figure 2.14: Violin plot of mileage vs transmission type. Finally, producing plots one by one might be a tedious and challenging task, so it is good to have some instruments for producing several of them together. The plot() method will produce scatterplot matrix for numerical variables, but does not deal well with integer and categorical variables: plot(mtcars) Figure 2.15: Scatterplot matrix for the mtcars dataset. Figure 2.15 is informative for the variables mpg, cyl, disp, hp, drat, qsec and carb, but is difficult to read for the others. In order to address this issue, we can use the spread() function from greybox, which will detect types of variables and produce the necessary plots automatically: spread(mtcarsData, lowess=TRUE) Figure 2.16: Spread plot for the mtcars dataset. The plot on Figure 2.16 is the collection of the plots discussed above, so I will not stop on explaining what it shows. As a final word for this section, when analysing data, it is critically important not to just describe what we see, but also explain why a result or a relationship is meaningful, otherwise this becomes an exercise of stating the obvious which does not have any value. So, for example, concluding based on Figure 2.16 that the mileage has a negative relation with displacement is not enough. If you want to analyse the data properly, you need to explain that this relation is meaningful, because with the increase of the size of engine, the fuel consumption will increase as well, and as a result the mileage will go down. Furthermore, the relation is non-linear because the change in decrease will slow down with cars with bigger engines. Inevitably, the car with a gigantic engine will be able to travel a short distance on a gallon of fuel - the mileage will not become negative, so the non-linearity is not an artefact of the data, but an existing phenomenon. "],["distributions.html", "Chapter 3 Continuous distributions", " Chapter 3 Continuous distributions There are several probability distributions that will be helpful in the further chapters of this textbook. They becomes useful in a variety of contexts, and it is important to be at least aware of them. In this chapter we discuss them in more detail, introducing mathematical notations and providing examples of what these distributions arise from. "],["distributionsNormal.html", "3.1 Normal distribution", " 3.1 Normal distribution Every statistical textbook has Normal distribution. It is that one famous bell-curved distribution that every statistician likes because it is easy to work with and because it is an asymptotic distribution for many other well-behaved distributions in some conditions (see discussion of “Central Limit Theorem” in Section 4.2). Here is the probability density function (PDF) of this distribution: \\[\\begin{equation} f(y_t) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} \\right) , \\tag{3.1} \\end{equation}\\] where \\(y_t\\) is the value of the response variable, \\(\\mu_{y,t}=\\mu_{y,t|t-1}\\) is the one step ahead conditional expectation on observation \\(t\\), given the information on \\(t-1\\) and \\(\\sigma^2\\) is the variance of the error term. The maximum likelihood estimate of \\(\\sigma^2\\) is: \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T \\left(y_t - \\hat{\\mu}_{y,t} \\right)^2 , \\tag{3.2} \\end{equation}\\] where \\(T\\) is the sample size and \\(\\hat{\\mu}_{y,t}\\) is the estimate of the conditional expecatation \\(\\mu_{y,t}\\). (3.2) coincides with Mean Squared Error (MSE), discussed in the section 1. And here how this distribution looks (Figure 3.1). Figure 3.1: Probability Density Function of Normal distribution What we typically assume in the basic time series models is that a variable is random and follows Normal distribution, meaning that there is a central tendency (in our case - the mean \\(mu\\)), around which the density of values is the highest and there are other potential cases, but their probability of appearance reduces proportionally to the distance from the centre. The Normal distribution has skewness of zero and kurtosis of 3 (and excess kurtosis, being kurtosis minus three, of 0). Additionally, if Normal distribution is used for the maximum likelihood estimation of a model, it gives the same parameters as the minimisation of MSE would give. The log-likelihood based on the Normal distribution is derived by taking the sum of logarithms of the PDF of Normal distribution (3.1): \\[\\begin{equation} \\ell(\\mathbf{Y}| \\theta, \\sigma^2) = - \\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) - \\sum_{t=1}^T \\frac{\\left(y_t - \\mu_{y,t} \\right)^2}{2 \\sigma^2} , \\tag{3.3} \\end{equation}\\] where \\(\\theta\\) is the vector of all the estimated parameters in the model, and \\(\\log\\) is the natural logarithm. If one takes the derivative of (3.3) with respect to \\(\\sigma^2\\), then the formula (3.2) is obtained. Another useful thing to note is the concentrated log-likelihood, which is obtained by inserting the estimated variance (3.2) in (3.3): \\[\\begin{equation} \\ell(\\mathbf{Y}| \\theta, \\hat{\\sigma}^2) = - \\frac{T}{2} \\log \\left( 2 \\pi e \\hat{\\sigma}^2 \\right) , \\tag{3.4} \\end{equation}\\] where \\(e\\) is the Euler’s constant. The concentrated log-likelihood is handy, when estimating the model and calculating information criteria. Sometimes, statisticians drop the \\(2 \\pi e\\) part from the (3.4), because it does not affect any inferences, as long as one works only with Normal distribution (for example, this is what is done in ets() function from forecast package in R). However, it is not recommended to do (Burnham and Anderson, 2004), because this makes the comparison with other distributions impossible. Normal distribution is available in stats package with dnorm(), qnorm(), pnorm() and rnorm() functions. References "],["distributionsLaplace.html", "3.2 Laplace distribution", " 3.2 Laplace distribution A more exotic distribution is Laplace, which has some similarities with Normal, but has higher excess. It has the following PDF: \\[\\begin{equation} f(y_t) = \\frac{1}{2 s} \\exp \\left( -\\frac{\\left| y_t - \\mu_{y,t} \\right|}{s} \\right) , \\tag{3.5} \\end{equation}\\] where \\(s\\) is the scale parameter, which, when estimated using likelihood, is equal to the Mean Absolute Error (MAE): \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left| y_t - \\hat{\\mu}_{y,t} \\right| . \\tag{3.6} \\end{equation}\\] It has the shape shown on Figure 3.2. Figure 3.2: Probability Density Function of Laplace distribution Similar to the Normal distribution, the skewness of Laplace is equal to zero. However, it has fatter tails - its kurtosis is equal to 6 instead of 3. The variance of the random variable following Laplace distribution is equal to: \\[\\begin{equation} \\sigma^2 = 2 s^2. \\tag{3.7} \\end{equation}\\] The dlaplace, qlaplace, plaplace and rlaplace functions from greybox package implement different sides of Laplace distribution in R. "],["s-distribution.html", "3.3 S distribution", " 3.3 S distribution This is something relatively new, but not ground braking. I have derived S distribution few years ago, but have never written a paper on that. It has the following density function (it is as a special case of Generalised Normal distribution, when \\(\\beta=0.5\\)): \\[\\begin{equation} f(y_t) = \\frac{1}{4 s^2} \\exp \\left( -\\frac{\\sqrt{|y_t - \\mu_{y,t}|}}{s} \\right) , \\tag{3.8} \\end{equation}\\] where \\(s\\) is the scale parameter. If estimated via maximum likelihood, the scale parameter is equal to: \\[\\begin{equation} \\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{\\left| y_t - \\hat{\\mu}_{y,t} \\right|} , \\tag{3.9} \\end{equation}\\] which corresponds to the minimisation of a half of “Mean Root Absolute Error” or “Half Absolute Moment” (HAM). This is a more exotic type of scale, but the main benefit of this distribution is sever heavy tails - it has kurtosis of 25.2. It might be useful in cases of randomly occurring incidents and extreme values (Black Swans?). Figure 3.3: Probability Density Function of S distribution The variance of the random variable following S distribution is equal to: \\[\\begin{equation} \\sigma^2 = 120 s^4. \\tag{3.10} \\end{equation}\\] The ds, qs, ps and rs from greybox package implement the density, quantile, cumulative and random generation functions. "],["distributionsGeneralisedNormal.html", "3.4 Generalised Normal distribution", " 3.4 Generalised Normal distribution Generalised Normal (\\(\\mathcal{GN}\\)) distribution (as the name says) is a generalisation for Normal distribution, which also includes Laplace and S as special cases (Nadarajah, 2005). There are two versions of this distribution: one with a shape and another with a skewness parameter. We are mainly interested in the first one, which has the following PDF: \\[\\begin{equation} f(y_t) = \\frac{\\beta}{2 s \\Gamma(\\beta^{-1})} \\exp \\left( -\\left(\\frac{|y_t - \\mu_{y,t}|}{s}\\right)^{\\beta} \\right), \\tag{3.11} \\end{equation}\\] where \\(\\beta\\) is the shape parameter, and \\(s\\) is the scale of the distribution, which, when estimated via MLE, is equal to: \\[\\begin{equation} \\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| y_t - \\hat{\\mu}_{y,t} \\right|^{\\beta}}, \\tag{3.12} \\end{equation}\\] which has MSE, MAE and HAM as special cases, when \\(\\beta\\) is equal to 2, 1 and 0.5 respectively. The parameter \\(\\beta\\) influences the kurtosis directly, it can be calculated for each special case as \\(\\frac{\\Gamma(5/\\beta)\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)^2}\\). The higher \\(\\beta\\) is, the lower the kurtosis is. The advantage of \\(\\mathcal{GN}\\) distribution is its flexibility. In theory, it is possible to model extremely rare events with this distribution, if the shape parameter \\(\\beta\\) is fractional and close to zero. Alternatively, when \\(\\beta \\rightarrow \\infty\\), the distribution converges point-wise to the uniform distribution on \\((\\mu_{y,t} - s, \\mu_{y,t} + s)\\). Note that the estimation of \\(\\beta\\) is a difficult task, especially, when it is less than 2 - the MLE of it looses properties of consistency and asymptotic normality. Depending on the value of \\(\\beta\\), the distribution can have different shapes shown in Figure 3.4 Figure 3.4: Probability Density Functions of Generalised Normal distribution Typically, estimating \\(\\beta\\) consistently is a tricky thing to do, especially if it is less than one. Still, it is possible to do that by maximising the likelihood function (3.11). The variance of the random variable following Generalised Normal distribution is equal to: \\[\\begin{equation} \\sigma^2 = s^2\\frac{\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)}. \\tag{3.13} \\end{equation}\\] The working functions for the Generalised Normal distribution are implemented in the greybox package for R. References "],["distributionsALaplace.html", "3.5 Asymmetric Laplace distribution", " 3.5 Asymmetric Laplace distribution Asymmetric Laplace distribution (\\(\\mathcal{AL}\\)) can be considered as a two Laplace distributions with different parameters \\(s\\) for left and right sides from the location \\(\\mu_{y,t}\\). There are several ways to summarise the probability density function, the neater one relies on the asymmetry parameter \\(\\alpha\\) (Yu and Zhang, 2005): \\[\\begin{equation} f(y_t) = \\frac{\\alpha (1- \\alpha)}{s} \\exp \\left( -\\frac{y_t - \\mu_{y,t}}{s} (\\alpha - I(y_t \\leq \\mu_{y,t})) \\right) , \\tag{3.14} \\end{equation}\\] where \\(s\\) is the scale parameter, \\(\\alpha\\) is the skewness parameter and \\(I(y_t \\leq \\mu_{y,t})\\) is the indicator function, which is equal to one, when the condition is satisfied and to zero otherwise. The scale parameter \\(s\\) estimated using likelihood is equal to the quantile loss: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\left(y_t - \\hat{\\mu}_{y,t} \\right)(\\alpha - I(y_t \\leq \\hat{\\mu}_{y,t})) . \\tag{3.15} \\end{equation}\\] Thus maximising the likelihood (3.14) is equivalent to estimating the model via the minimisation of \\(\\alpha\\) quantile, making this equivalent to quantile regression approach. So quantile regression models assume indirectly that the error term in the model is \\(\\epsilon_t \\sim \\mathcal{AL}(0, s, \\alpha)\\) (Geraci and Bottai, 2007). Depending on the value of \\(\\alpha\\), the distribution can have different shapes, shown in Figure 3.5. Figure 3.5: Probability Density Functions of Asymmetric Laplace distribution Similarly to \\(\\mathcal{GN}\\) distribution, the parameter \\(\\alpha\\) can be estimated during the maximisation of the likelihood, although it makes more sense to set it to some specific values in order to obtain the desired quantile of distribution. The variance of the random variable following Asymmetric Laplace distribution is equal to: \\[\\begin{equation} \\sigma^2 = s^2\\frac{(1-\\alpha)^2+\\alpha^2}{\\alpha^2(1-\\alpha)^2}. \\tag{3.16} \\end{equation}\\] Functions dalaplace, qalaplace, palaplace and ralaplace from greybox package implement the Asymmetric Laplace distribution. References "],["log-normal-log-laplace-log-s-and-log-gn-distributions.html", "3.6 Log Normal, Log Laplace, Log S and Log GN distributions", " 3.6 Log Normal, Log Laplace, Log S and Log GN distributions In addition, it is possible to derive the log-versions of the Normal, \\(\\mathcal{Laplace}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{GN}\\) distributions. The main differences between the original and the log-versions of density functions for these distributions can be summarised as follows: \\[\\begin{equation} f_{log}(\\log(y_t)) = \\frac{1}{y_t} f(\\log y_t). \\tag{3.17} \\end{equation}\\] They are defined for positive values only and will have different right tail, depending on the location, scale and shape parameters. \\(\\exp(\\mu_{\\log y,t})\\) in this case represents the geometric mean (and median) of distribution rather than the arithmetic one. The conditional expectation in these distributions is typically higher than \\(\\exp(\\mu_{\\log y,t})\\) and depends on the value of the scale parameter. It is known for log\\(\\mathcal{N}\\) and is equal to: \\[\\begin{equation} \\mathrm{E}(y_t) = \\mathrm{exp}\\left(\\mu_{\\log y,t} + \\frac{\\sigma^2}{2} \\right). \\tag{3.18} \\end{equation}\\] However, it does not have a simple form for the other distributions. "],["IGDistribution.html", "3.7 Inverse Gaussian distribution", " 3.7 Inverse Gaussian distribution An exotic distribution that will be useful for what comes in this textbook is the Inverse Gaussian (\\(\\mathcal{IG}\\)), which is parameterised using mean value \\(\\mu_{y,t}\\) and either the dispersion parameter \\(s\\) or the scale \\(\\lambda\\) and is defined for positive values only. This distribution is useful because it is scalable and has some similarities with the Normal one. In our case, the important property is the following: \\[\\begin{equation} \\text{if } (1+\\epsilon_t) \\sim \\mathcal{IG}(1, s) \\text{, then } y_t = \\mu_{y,t} \\times (1+\\epsilon_t) \\sim \\mathcal{IG}\\left(\\mu_{y,t}, \\frac{s}{\\mu_{y,t}} \\right), \\tag{3.19} \\end{equation}\\] implying that the dispersion of the model changes together with the expectation. The PDF of the distribution of \\(1+\\epsilon_t\\) is: \\[\\begin{equation} f(1+\\epsilon_t) = \\frac{1}{\\sqrt{2 \\pi s (1+\\epsilon_t)^3}} \\exp \\left( -\\frac{\\epsilon_t^2}{2 s (1+\\epsilon_t)} \\right) , \\tag{3.20} \\end{equation}\\] where the dispersion parameter can be estimated via maximising the likelihood and is calculated using: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T \\frac{e_t^2}{1+e_t} , \\tag{3.21} \\end{equation}\\] where \\(e_t\\) is the estimate of \\(\\epsilon_t\\). This distribution becomes very useful for multiplicative models, where it is expected that the data can only be positive. Figure 3.6 shows how the PDF of \\(\\mathcal{IG}(1,s)\\) looks for different values of the dispersion \\(s\\) Figure 3.6: Probability Density Functions of Inverse Gaussian distribution statmod package implements density, quantile, cumulative and random number generator functions for the \\(\\mathcal{IG}\\). "],["GammaDistribution.html", "3.8 Gamma distribution", " 3.8 Gamma distribution Finally, another distribution that will be useful for ETS and ARIMA is Gamma (\\(\\mathcal{\\Gamma}\\)), which is parameterised using shape \\(\\xi\\) and scale \\(s\\), and is defined for positive values only. This distribution is useful because it is scalable and is as flexible as (\\(\\mathcal{IG}\\)) in terms of possible shapes. It also has an important scalability property (simila to \\(\\mathcal{IG}\\)), but the shape needs to be restricted in order to make sense in ETS model: \\[\\begin{equation} \\text{if } (1+\\epsilon_t) \\sim \\mathcal{\\Gamma}(s^{-1}, s) \\text{, then } y_t = \\mu_{y,t} \\times (1+\\epsilon_t) \\sim \\mathcal{\\Gamma}\\left(s^{-1}, s \\mu_{y,t} \\right), \\tag{3.22} \\end{equation}\\] implying that the scale of the model changes together with the expectation. The restriction on the shape parameters is needed in order to make the expectation of \\((1+\\epsilon_t)\\) equal to one. The PDF of the distribution of \\(1+\\epsilon_t\\) is: \\[\\begin{equation} f(1+\\epsilon_t) = \\frac{1}{\\Gamma(s^{-1}) (s)^{s^{-1}}} (1+\\epsilon_t)^{s^{-1}-1}\\exp \\left(-\\frac{1+\\epsilon_t}{s}\\right) . \\tag{3.23} \\end{equation}\\] However, the scale \\(s\\) cannot be estimated via the maximisation of likelihood analytically due to the restriction (3.22). Luckliy, the method of moments can be used instead, where based on the expectation and variance we get: \\[\\begin{equation} \\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t^2 , \\tag{3.24} \\end{equation}\\] where \\(e_t\\) is the estimate of \\(\\epsilon_t\\). So, imposing the restrictions (3.22) implies that the scale of \\(\\mathcal{\\Gamma}\\) is equal to the variance of the error term. Figure 3.7 demonstrates how the PDF of \\(\\mathcal{\\Gamma}(s^{-1},s)\\) looks for different values of \\(s\\): Figure 3.7: Probability Density Functions of Gamma distribution With the increase of the shape \\(\\xi=s^{-1}\\) (in our case this implies the decrease of variance \\(s\\)), \\(\\mathcal{\\Gamma}\\) distribution converges to the normal one with \\(\\mu=\\xi s=1\\) and variance \\(\\sigma^2=s\\). This demonstrates indirectly that the estimate of the scale (3.24) maximises the likelihood of the function (3.23), although I do not have any proper proof of this. "],["PopulationSampling.html", "Chapter 4 Population and sampling", " Chapter 4 Population and sampling Consider a case, when you want to understand what is the average height of teenagers living in your town. It is very expensive and time consuming to go from one house to another and ask every single teenager (if you find one), what their hight is. If we could do that, we would get the true mean, true average height of teenagers living in the town. But in reality, it is more practical to ask a sample of teenagers and make conclusions about the “population” (all teenagers in the town) based on this sample. Indeed, you will spend much less time collecting the information about the height of 100 people rather than 100,000. However, when we take a sample of something, the statistics we work with will always differ from the truth: sample mean will never be equal to the true mean, but it can be shown mathematically that it will converge to the truth, when some specific conditions are met and when the sample size increases. If we set up the experiment correctly, then we can expect our statistics to follow some laws. In this chapter, we discuss these laws, how they work and what they imply. "],["LLN.html", "4.1 Law of Large Numbers", " 4.1 Law of Large Numbers The first law is called the Law of Large Numbers (LLN). It is the theorem saying that (under wide conditions) the average of a variable obtained over the large number of trials will be close to its expected value and will get closer to it with the increase of the sample size. This can be demonstrated with the following example: obs &lt;- 10000 # Generate data from normal distribution y &lt;- rnorm(obs,100,100) # Create sub-samples of 50 and 100 observations y30 &lt;- sample(y, 30) y1000 &lt;- sample(y, 1000) par(mfcol=c(1,2)) hist(y30, xlab=&quot;y&quot;) abline(v=mean(y30), col=&quot;red&quot;) hist(y1000, xlab=&quot;y&quot;) abline(v=mean(y1000), col=&quot;red&quot;) Figure 4.1: Histograms of samples of data from variable y. What we will typically see on the plots above is that the mean (red line) on the left plot will be further away from the true mean of 100 than in the case of the right plot. Given that this is randomly generated, the situation might differ, but the idea would be that with the increase of the sample size the estimated sample mean will converge to the true one. We can even produce a plot showing how this happens: yMean &lt;- vector(&quot;numeric&quot;,obs) for(i in 1:obs){ yMean[i] &lt;- mean(sample(y,i)) } plot(yMean, type=&quot;l&quot;, xlab=&quot;Sample size&quot;, ylab=&quot;Sample mean&quot;) Figure 4.2: Demonstration of Law of Large Numbers. We can see from the plot above that with the increase of the sample size the sample mean reaches the true value of 100. This is a graphical demonstration of the Law of Large Numbers: it only tells us about what will happen when the sample size increases. But it is still useful, because it used for many statistical inferences and if it does not work, then the estimate of mean would be incorrect, meaning that we cannot make conclusions about the behaviour in population. In order for LLN to work, the distribution of variable needs to have finite mean and variance. This is discussed in some detail in the next subsection. In summary, what LLN tells us is that if we average things out over a large number of observations, then that average starts looking very similar to the population value. However, this does not say anything about the performance of estimators on small samples. "],["CLT.html", "4.2 Central Limit Theorem", " 4.2 Central Limit Theorem As we have already seen on Figure 4.2, the sample mean is not exactly equal to the population mean even when the sample size is very large (thousands of observations). There is always some sort of variability around the population mean. In order to understand how this variability looks like, we could conduct a simple experiment. We could take a random sample of, for instance, 1000 observations several times and record each of the obtained means. We then can see how the variable will be distributed to see if there are any patterns in the behaviour of the estimator: nIterations &lt;- 1000 yMean &lt;- vector(&quot;numeric&quot;,nIterations) for(i in 1:nIterations){ yMean[i] &lt;- mean(sample(y,1000)) } hist(yMean, xlab=&quot;Sample mean&quot;, main=&quot;&quot;) Figure 4.3: Histogram of the mean of the variable y. There is a theorem that says that the distribution of mean in the experiment above will follow normal distribution under several conditions (discussed later in this section). It is called Central Limit Theorem (CLT) and very roughly it says that when independent random variables are added, their normalised sum will asymptotically follow normal distribution, even if the original variables do not follow it. Note that this is the theorem about what happens with the estimate (sum in this case), not with individual observations. This means that the error term might follow, for example, Inverse Gaussian distribution, but the estimate of its mean (under some conditions) will follow normal distribution. There are different versions of this theorem, built with different assumptions with respect to the random variable and the estimation procedure, but we do not dicuss these details in this textbook. In order for CLT to hold, the following important assumptions need to be satisfied: The true value of parameter is not near the bound. e.g. if the variable follows uniform distribution on (0, \\(a\\)) and we want to estimate \\(a\\), then its distribution will not be Normal (because in this case the true value is always approached from below). This assumption is important in our context, because ETS and ARIMA typically have restrictions on their parameters. The random variables are identically independent distributed (i.i.d.). If they are not, then their average might not follow normal distribution (in some conditions it still might). The mean and variance of the distribution are finite. This might seem as a weird issue, but some distributions do not have finite moments, so the CLT will not hold if a variable follows them, just because the sample mean will be all over the plane due to randomness and will not converge to the “true” value. Cauchy distribution is one of such examples. If these assumptions hold, then CLT will work for the estimate of a parameter, no matter what the distribution of the random variable is. This becomes especially useful, when we want to test a hypothesis or construct a confidence interval for an estimate of a parameter. "],["estimatesProperties.html", "4.3 Properties of estimators", " 4.3 Properties of estimators Before we move further, we need to agree what the term “estimator” means, which will be used several times further in this textbook: Estimate of a parameter is an in sample result of application of a statistical procedure to the data for obtaining some coefficients of a model. The value calculated using the arithmetic mean would be an estimate of the population mean; Estimator is the rule for calculating estimates of parameters based on a sample of data. For example, arithmetic mean is an estimator of the population mean. Another example would be method of Ordinary Least Squares, which is a rule for producing estimates of parameters of a regression model and thus an estimator. In this section, we discuss such terms as bias, efficiency and consistency of estimates of parameters, which are directly related to LLN and CLT. Although there are strict statistical definitions of the aforementioned terms (you can easily find them in Wikipedia or anywhere else), I do not want to copy-paste them here, because there are only a couple of important points worth mentioning in our context. Note that all the discussions in this chapter relate to the estimates of parameters, not to the distribution of a random variable itself. A common mistake that students make when studying statistics, is that they think that the properties apply to the variable \\(y_j\\) instead of the estimate of its parameters (e.g. mean of \\(y_j\\)). 4.3.1 Bias Bias refers to the expected difference between the estimated value of parameter (on a specific sample) and the “true” one (in the true model). Having unbiased estimates of parameters is important because they should lead to more accurate forecasts (at least in theory). For example, if the estimated parameter is equal to zero, while in fact it should be 0.5, then the model would not take the provided information into account correctly and as a result will produce less accurate point forecasts and incorrect prediction intervals. In inventory context this may mean that we constantly order 100 units less than needed only because the parameter is lower than it should be. The classical example of bias in statistics is the estimation of variance in sample. The following formula gives biased estimate of variance in sample: \\[\\begin{equation} \\mathrm{V}(y) = \\frac{1}{T} \\sum_{j=1}^n \\left( y_j - \\bar{y} \\right)^2, \\tag{4.1} \\end{equation}\\] where \\(T\\) is the sample size and \\(\\bar{y} = \\frac{1}{T} \\sum_{j=1}^n y_j\\) is the mean of the data. There is a lot of proofs in the literature of this issue (even Wikipedia (2020a) has one), we will not spend time on that. Instead, we will see this effect in the following simple simulation experiment: mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 1000 # Generate data from normal distribution, 10,000 observations y &lt;- rnorm(10000,mu,sigma) # This is the function, which will calculate the two variances varFunction &lt;- function(y){ return(c(var(y), mean((y-mean(y))^2))) } # Calculate biased and unbiased variances for the sample of 30 observations, # repeat nIterations times varValues &lt;- replicate(nIterations, varFunction(sample(y,30))) This way we have generated 1000 samples with 30 observations and calculated variances using the formulae (4.1) and the corrected one for each step. Now we can plot it in order to see how it worked out: par(mfcol=c(1,2)) # Histogram of the biased estimate hist(varValues[2,], xlab=&quot;V(y)&quot;, ylab=&quot;y&quot;, main=&quot;Biased estimate of V(y)&quot;) abline(v=mean(varValues[2,]), col=&quot;red&quot;) legend(&quot;topright&quot;,legend=TeX(paste0(&quot;E$\\\\left(V(y)\\\\right)$=&quot;,round(mean(varValues[2,]),2))),lwd=1,col=&quot;red&quot;) # Histogram of unbiased estimate hist(varValues[1,], xlab=&quot;V(y)&quot;, ylab=&quot;y&quot;, main=&quot;Unbiased estimate of V(y)&quot;) abline(v=mean(varValues[1,]), col=&quot;red&quot;) legend(&quot;topright&quot;,legend=TeX(paste0(&quot;E$\\\\left(V(y)\\\\right)$=&quot;,round(mean(varValues[1,]),2))),lwd=1,col=&quot;red&quot;) Figure 4.4: Histograms for biased and unbiased estimates of variance. Every run of this experiment will produce different plots, but typically what we will see is that, the biased estimate of variance (the histogram on the right hand side of the plot) will have lower mean than the unbiased one. This is the graphical example of the effect of not taking the number of estimated parameters into account. The correct formula for the unbiased estimate of variance is: \\[\\begin{equation} s^2 = \\frac{1}{T-k} \\sum_{j=1}^n \\left( y_j - \\bar{y} \\right)^2, \\tag{4.2} \\end{equation}\\] where \\(k\\) is the number of all independent estimated parameters. In this simple example \\(k=1\\), because we only estimate mean (the variance is based on it). Analysing the formulae (4.1) and (4.2), we can say that with the increase of the sample size, the bias will disappear and the two formulae will give almost the same results: when the sample size \\(T\\) becomes big enough, the difference between the two becomes negligible. This is the graphical presentation of the bias in the estimator. 4.3.2 Efficiency Efficiency means, if the sample size increases, then the estimated parameters will not change substantially, they will vary in a narrow range (variance of estimates will be small). In the case with inefficient estimates the increase of sample size from 50 to 51 observations may lead to the change of a parameter from 0.1 to, let’s say, 10. This is bad because the values of parameters usually influence both point forecasts and prediction intervals. As a result the inventory decision may differ radically from day to day. For example, we may decide that we urgently need 1000 units of product on Monday, and order it just to realise on Tuesday that we only need 100. Obviously this is an exaggeration, but no one wants to deal with such an erratically behaving model, so we need to have efficient estimates of parameters. Another classical example of not efficient estimator is the median, when used on the data that follows Normal distribution. Here is a simple experiment demonstrating the idea: mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 500 obs &lt;- 100 varMeanValues &lt;- vector(&quot;numeric&quot;,obs) varMedianValues &lt;- vector(&quot;numeric&quot;,obs) y &lt;- rnorm(100000,mu,sigma) for(i in 1:obs){ ySample &lt;- replicate(nIterations,sample(y,i*100)) varMeanValues[i] &lt;- var(apply(ySample,2,mean)) varMedianValues[i] &lt;- var(apply(ySample,2,median)) } In order to establish the efficiency of the estimators, we will take their variances and look at the ratio of mean over median. If both are equally efficient, then this ratio will be equal to one. If the mean is more efficient than the median, then the ratio will be less than one: options(scipen=6) plot(1:100*100,varMeanValues/varMedianValues, type=&quot;l&quot;, xlab=&quot;Sample size&quot;,ylab=&quot;Relative efficiency&quot;) abline(h=1, col=&quot;red&quot;) Figure 4.5: An example of a relatively inefficient estimator. What we should typically see on this graph, is that the black line should be below the red one, indicating that the variance of mean is lower than the variance of the median. This means that mean is more efficient estimator of the true location of the distribution \\(\\mu\\) than the median. In fact, it is easy to proove that asymptotically the mean will be 1.57 times more efficient than median (Wikipedia, 2020b) (so, the line should converge approximately to the value of 0.64). 4.3.3 Consistency Consistency means that our estimates of parameters will get closer to the stable values (true value in the population) with the increase of the sample size. This follows directly from LLN and is important because in the opposite case estimates of parameters will diverge and become less and less realistic. This once again influences both point forecasts and prediction intervals, which will be less meaningful than they should have been. In a way consistency means that with the increase of the sample size the parameters will become more efficient and less biased. This in turn means that the more observations we have, the better. An example of inconsistent estimator is Chebyshev (or max norm) metric. It is formulated the following way: \\[\\begin{equation} \\mathrm{LMax} = \\max \\left(|y_1-\\hat{y}|, |y_2-\\hat{y}|, \\dots, |y_n-\\hat{y}| \\right). \\tag{4.3} \\end{equation}\\] Minimising this norm, we can get an estimate \\(\\hat{y}\\) of the location parameter \\(\\mu\\). The simulation experiment becomes a bit more tricky in this situation, but here is the code to generate the estimates of the location parameter: LMax &lt;- function(y){ estimator &lt;- function(par){ return(max(abs(y-par))); } return(optim(mean(y), fn=estimator, method=&quot;Brent&quot;, lower=min(y), upper=max(y))); } mu &lt;- 100 sigma &lt;- 10 nIterations &lt;- 1000 y &lt;- rnorm(10000, mu, sigma) LMaxEstimates &lt;- vector(&quot;numeric&quot;, nIterations) for(i in 1:nIterations){ LMaxEstimates[i] &lt;- LMax(y[1:(i*10)])$par; } And here how the estimate looks with the increase of sample size: plot(1:nIterations*10, LMaxEstimates, type=&quot;l&quot;, xlab=&quot;Sample size&quot;,ylab=TeX(&quot;Estimate of $\\\\mu$&quot;)) abline(h=mu, col=&quot;red&quot;) Figure 4.6: An example of inconsistent estimator. While in the example with bias we could see that the lines converge to the red line (the true value) with the increase of the sample size, the Chebyshev metric example shows that the line does not approach the true one, even when the sample size is 10000 observations. The conclusion is that when Chebyshev metric is used, it produces inconsistent estimates of parameters. Remark. There is a prejudice in the world of practitioners that the situation in the market changes so fast that the old observations become useless very fast. As a result many companies just through away the old data. Although, in general the statement about the market changes is true, the forecasters tend to work with the models that take this into account (e.g. Exponential smoothing, ARIMA, discussed in this book). These models adapt to the potential changes. So, we may benefit from the old data because it allows us getting more consistent estimates of parameters. Just keep in mind, that you can always remove the annoying bits of data but you can never un-throw away the data. 4.3.4 Asymptotic normality Finally, asymptotic normality is not critical, but in many cases is a desired, useful property of estimates. What it tells us is that the distribution of the estimate of parameter will be well behaved with a specific mean (typically equal to \\(\\mu\\)) and a fixed variance. This follows directly from CLT. Some of the statistical tests and mathematical derivations rely on this assumption. For example, when one conducts a significance test for parameters of model, this assumption is implied in the process. If the distribution is not Normal, then the confidence intervals constructed for the parameters will be wrong together with the respective t- and p- values. Another important aspect to cover is what the term asymptotic, which we have already used, means in our context. Here and after in this book, when this word is used, we refer to an unrealistic hypothetical situation of having all the data in the multiverse, where the time index \\(t \\rightarrow \\infty\\). While this is impossible in practice, the idea is useful, because asymptotic behaviour of estimators and models is helpful on large samples of data. Besides, even if we deal with small samples, it is good to know what to expect to happen if the sample size increases. References "],["uncertaintyData.html", "Chapter 5 Sample related uncertainty", " Chapter 5 Sample related uncertainty As mentioned in Section 1.3, we always work with samples and inevitably we deal with randomness just because of that even, when there are no other sources of uncertainty in the data. For example, if we want to estimate the mean of a variable based on the observed data, the value we get will differ from one sample to another. This should have become apparent from the examples we discussed earlier. And, if the LLN and CLT hold, then we know that the estimate of our parameter will have its own distribution and will converge to the population value with the increase of the sample size. This is the basis for the confidence and prediction interval construction, discussed in this section. Depending on our needs, we can focus on the uncertainty of either the estimate of a parameter, or the random variable \\(y\\) itself. When dealing with the former, we typically work with the confidence interval - the interval constructed for the estimate of a parameter, while in the latter case we are interested in the prediction interval - the interval constructed for the random variable \\(y\\). In order to simplify further discussion in this section, we will take the population mean and its in-sample estimate as an example. In this case we have: A random variable \\(y\\), which is assumed to follow some distribution with finite mean \\(\\mu\\) and variance \\(\\sigma^2\\); A sample of size \\(n\\) from the population of \\(y\\); Estimates of mean \\(\\hat{\\mu}=\\bar{y}\\) and variance \\(\\hat{\\sigma}^2 = s^2\\), obtained based on the sample of size \\(n\\). "],["confidenceInterval.html", "5.1 Confidence interval", " 5.1 Confidence interval What we want to get by doing this is an idea about the population mean \\(\\mu\\). The value \\(\\bar{y}\\) does not tell us much on its own due to randomness and if we do not capture its uncertainty, we will not know, where the true value \\(\\mu\\) can be. But using LLN and CLT, we know that the sample mean should converge to the true one and should follow normal distribution. So, the distribution of the sample mean would look like this (Figure 5.1). Figure 5.1: Distribution of the sample mean. On its own, this distribution just tells us that the variable is random around the true mean \\(\\mu\\) and that its density function has a bell-like shape. In order to make this more useful, we can construct the confidence interval for it, which would tell us where the true parameter is most likely to lie. We can cut the tails of this distribution to determine the width of the interval, expecting it to cover \\((1-\\alpha)\\times 100\\)% of cases. In the ideal world, asymptotically, the confidence interval will be constructed based on the true value, like this: Figure 5.2: Distribution of the sample mean and the confidence interval based on the population data. Figure 5.2 shows the classical normal distribution curve around the population mean \\(\\mu\\), confidence interval of the level \\(1-\\alpha\\) and the cut off tails, the overall surface of which corresponds to \\(\\alpha\\). The value \\(1-\\alpha\\) is called confidence level, while \\(\\alpha\\) is the significance level. By constructing the interval this way, we expect that in the \\((1-\\alpha)\\times 100\\)% of cases the value will be inside the bounds, and in \\(\\alpha\\times 100\\)% it will not. In reality we do not know the true mean \\(\\mu\\), so we do a slightly different thing: we construct a confidence interval based on the sample mean \\(\\bar{y}\\) and sample variance \\(s^2\\), hoping that due to LLN they will converge to the true values. We use Normal distribution, because we expect CLT to work. This process looks something like in Figure 5.3, with the bell curve in the background representing the true distribution for the sample mean and the curve on the foreground representing the assumed distribution based on our sample: Figure 5.3: Distribution of the sample mean and the confidence interval based on a sample. So, what the confidence interval does in reality is tries to cover the unknown population mean, based on the sample values of \\(\\bar{y}\\) and \\(s^2\\). If we construct the confidence interval of the width \\(1-\\alpha\\) (e.g. 0.95) for thousands of random samples (thousands of trials), then in \\((1-\\alpha)\\times 100\\)% of cases (e.g. 95%) the true mean will be covered by the interval, while in \\(\\alpha \\times 100\\)% cases it will not be. The interval itself is random, and we rely on LLN and CLT, when constructing it, expecting for it to work asymptotically, with the increase of the number of trials. Mathematically the red bounds in Figure 5.3 are represented using the following well-known formula for the confidence interval: \\[\\begin{equation} \\mu \\in (\\bar{y} + t_{\\alpha/2}(df) s_{\\bar{y}}, \\bar{y} + t_{1-\\alpha/2}(df) s_{\\bar{y}}), \\tag{5.1} \\end{equation}\\] where \\(t_{\\alpha/2}(df)\\) is Student’s t-statistics for \\(df=n-k\\) degrees of freedom (\\(n\\) is the sample size and \\(k\\) is the number of estimated parameters, e.g. \\(k=1\\) in our case) and level \\(\\alpha/2\\), and \\(s_{\\bar{y}}=\\frac{1}{\\sqrt{n}}s\\) is the estimate of the standard deviation of the sample mean. If we knew for some reason the true variance \\(\\sigma^2\\), then we could use z-statistics instead of t, but we typically do not, so we need to take the uncertainty about the variance into account as well, thus the use of t-statistics. Note, that in order to construct confidence interval, we do not care what distribution \\(y\\) follows, as long as LLN and CLT hold. "],["confidenceIntervalsPrediction.html", "5.2 Prediction interval", " 5.2 Prediction interval If we are interested in capturing the uncertainty about the random variable \\(y\\), then we should refer to prediction interval. In this case, we typically rely on LLN and the assumed distribution for the random variable \\(y\\). For example, if we know that \\(y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then based on our sample we can construct a prediction interval of the width \\(1-\\alpha\\): \\[\\begin{equation} y \\in (\\bar{y} + z_{\\alpha/2} s, \\bar{y} + z_{1-\\alpha/2} s), \\tag{5.2} \\end{equation}\\] where \\(z_{\\alpha/2}\\) is the z-statistics (quantile of standard normal distribution) for the level \\(\\alpha/2\\) and \\(\\bar{y}\\) is the sample estimate of \\(\\mu\\) and \\(s\\) is the sample estimate of \\(\\sigma\\). If we assume some other distributions for the random variable, the formula would change. In a way, the prediction interval just comes to getting the quantiles of the assumed distribution based on estimated parameters. In some cases, when some of the assumptions do not hold, we might switch to more advanced methods for prediction interval construction. "],["hypothesisTesting.html", "5.3 Hypothesis testing", " 5.3 Hypothesis testing While we will not need hypothesis testing for ADAM itself, we might face it in some parts of the textbook, so it is worth discussing it briefly. Hypothesis testing arises naturally from the idea of confidence intervals: instead of constructing the interval and getting the idea about the uncertainty of the parameter, we could check, whether the sample agrees with our expectations or not. For example, we could test, whether the population mean is equal to zero based on our sample. We could either construct a confidence interval for the sample mean and see if zero is included in it (in which case it might indicate that zero is one of the possible values of the population mean), or we could reformulate the problem and compare some calculated value with the theoretical threshold. The latter approach is in the nutshell what hypothesis testing does. Fundamentally, the hypothesis testing process relies on the ideas of induction and dichotomy: we have a null (H\\(_0\\)) and alternative (H\\(_1\\)) hypotheses about the process or a property in the population, and we want to find some evidence to reject the H\\(_0\\). Rejecting a hypothesis is actually more useful than not rejecting it, because in the former case we know what not to expect from the data, while in the latter we just might not have enough evidence to make any solid conclusion. For example, we could formulate H\\(_0\\) that all cats are white. Failing to reject this hypothesis based on the data that we have (e.g. a dataset of white cats) does not mean that they are all (in the universe) indeed white, it just means that we have not observed the non-white ones. If we collect enough evidence to reject H\\(_0\\) (i.e. encountered a black cat), then we can conclude that not all cats are white. This is a more solid conclusion than the one in the previous case. So, if you are interested in a specific outcome, then it makes sense to put this in the alternative hypothesis and see if the data allows to reject the null. For example, if we want to see if the average salary of professors in the UK is higher than £100k per year we would formulate the hypotheses in the following way: \\[\\begin{equation*} \\mathrm{H}_0: \\mu \\leq 100, \\mathrm{H}_1: \\mu &gt; 100. \\end{equation*}\\] Having formulated hypotheses, we can check them, but in order to do that, we need to follow a proper procedure, which can be summarised in the six steps: Formulate null and alternative hypotheses (H\\(_0\\) and H\\(_1\\)) based on your understanding of the problem; Select the significance level \\(\\alpha\\) on which the hypothesis will be tested; Select the test appropriate for the formulated hypotheses (1); Conduct the test (3) and get the calculated value; Compare the value in (4) with the threshold one; Make a conclusion based on (5) on the selected level (2). Note that the order of some elements might change depending on the circumstances, but (2) should always happen before (4), otherwise we might be dealing with so called “p-hacking,” trying to make results look nicer than they really are. Consider an example, where we want to check, whether the population mean \\(\\mu\\) is equal to zero, based on a sample of 36 observations, where \\(\\bar{y}=-0.5\\) and \\(s^2=1\\). In this case, we formulate the null and alternative hypotheses: \\[\\begin{equation*} \\mathrm{H}_0: \\mu=0, \\mathrm{H}_1: \\mu \\neq 0. \\end{equation*}\\] We then select the significance level \\(\\alpha=0.05\\) (just as an example) and select the test. Based on the description of the task, this can be either a t-test, or a z-test, depending on whether the variance of the variable is known or not. Usually it is not, so we tend to use t-test. We then conduct the test using the formula: \\[\\begin{equation} t = \\frac{\\bar{y} - \\mu}{s_{\\bar{y}}} = \\frac{-0.5 - 0}{\\frac{1}{\\sqrt{36}}} = -3 . \\tag{5.3} \\end{equation}\\] After that we get the critical value of t with \\(df=36-1=35\\) degrees of freedom and significance level \\(\\alpha/2=0.025\\), which is approximately equal to -2.03. We compare this value with the (5.3) by absolute and reject H\\(_0\\) if the calculated value is higher than the critical one. In our case it is, so it appears that we have enough evidence to say that the population mean is not equal to 0, on the 5% significance level. Visually, the whole process of hypothesis testing explained above can be represented in the following way: Figure 5.4: The process of hypothesis testing with t value. If the blue line on Figure 5.4 would lie inside the red bounds (i.e. the calculated value is less than the critical value by absolute), then we would fail to reject H\\(_0\\). But in our example it is outside the bounds, so we have enough evidence to conclude that the population mean is not equal to zero on 5% significance level. Notice, how similar the mechanisms of confidence interval construction and hypothesis testing are. This is because they are one and the same thing, presented differently. In fact, we could test the same hypothesis by constructing the 95% confidence interval using (5.1) and checking, whether the interval covers the \\(\\mu=0\\): \\[\\begin{equation*} \\begin{aligned} &amp; \\mu \\in \\left(-0.50 -2.03 \\frac{1}{\\sqrt{36}}, -0.50 + 2.03 \\frac{1}{\\sqrt{36}} \\right), \\\\ &amp; \\mu \\in (-0.84, -0.16). \\end{aligned} \\end{equation*}\\] In our case it does not, so we conclude that we reject H\\(_0\\) on 5% significance level. This can be roughly represented by the graph on Figure 5.5: Figure 5.5: Confidence interval for for the population mean example. Note that the positioning of the blue line has changed in the case of confidence interval, which happens because of the transition from (5.3) to (5.1). The idea and the message, however, stay the same: if the value is not inside the light grey area, then we reject H\\(_0\\) on the selected significance level. Also note that we never say that we accept H\\(_0\\), because this is not what we do in hypothesis testing: if the value would lie inside the interval, then this would only mean that our sample shows that the tested value is covered by the region - the true value can be any of the numbers between the bounds. Finally, there is a third way to test the hypothesis. We could calculate how much surface is left in the tails with the cut off of the assumed distribution by the blue line on Figure 5.4 (calculated value). In R this can be done using the pt() function: pt(-3, 36-1) ## [1] 0.002474416 Given that we had the inequality in the alternative hypothesis, we need to consider both tails, multiplying the value by 2 to get approximately 0.0049. This is the significance level, for which the switch from “reject” to “do not reject” happens. We could compare this value with the pre-selected significance level directly, rejecting H\\(_0\\) if it is lower than \\(\\alpha\\). This value is called “p-value” and simplifies the hypothesis testing, because we do not need to look at critical values or construct the confidence interval. There are different definitions of what it is, I personally find the following easier to comprehend: p-value is the smallest significance level at which a null hypothesis can be rejected. Despite this simplification, we still need to follow the procedure and select \\(\\alpha\\) before conducting the test! We should not change the significance level after observing the p-values, otherwise we might end up bending reality for our needs. Also note that if in one case p-value is 0.2, while in the other it is 0.3, it does not mean that the the first case is more significant than the second! P-values are not comparable with each other and they do not tell you about the size of significance. This is still a binary process: we either reject, or fail to reject H\\(_0\\), depending on whether p-value is smaller or greater than the selected significance level. While p-value is a comfortable instrument, I personally prefer using confidence intervals, because they show the uncertainty clearer and are less confusing. Consider the following cases to see what I mean: We reject H\\(_0\\) because t-value is -3, which is smaller than the critical value of -2.03 (or equivalently the absolute of t-value is 3, while the critical is 2.03); We reject H\\(_0\\) because p-value is 0.0049, which is smaller than the significance level \\(\\alpha=0.05\\); The confidence interval for the mean is \\(\\mu \\in (-0.84, -0.16)\\). It does not include zero, so we reject \\(\\mathrm{H}_0\\). In case of (3), we not only get the same message as in (1) and (2), but we also see how far the bound is from the tested value. In addition, in the situation, when we fail to reject H\\(_0\\), the approach (3) gives more appropriate information. Consider the case, when we test, whether \\(\\mu=-0.6\\) in the example above. We then have the following three approaches to the problem: We fail to reject H\\(_0\\) because t-value is 0.245, which is smaller than the critical value of 2.03; We fail to reject H\\(_0\\) because p-value is 0.808, which is greater than the significance level \\(\\alpha=0.05\\); The confidence interval for the mean is \\(\\mu \\in (-0.84, -0.16)\\). It includes -0.6, so we fail to reject H\\(_0\\). This does not mean that the true mean is indeed equal to -0.6, but it means that the region will cover this number in 95% of cases if we do resampling many times. In my opinion, the third approach is more informative and saves from making wrong conclusions about the tested hypothesis, making you work a bit more (you cannot change the confidence level on the fly, you would need to reconstruct the interval). Having said that, either of the three is fine, as long as you understand what they really imply. Furthermore, if you do hypothesis testing and use p-values, it is worth mentioning the statement of American Statistical Association about p-values (Wasserstein and Lazar, 2016). Among the different aspects discussed in this statement, there is a list of principles related to p-values, which I cite below: P-values can indicate how incompatible the data are with a specified statistical model; P-values do not measure: the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone; Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold; Proper inference requires full reporting and transparency; A p-value, or statistical significance, does not measure: the size of an effect or the importance of a result; By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. The statement provides more details about that, but summarising, whatever hypothesis you test and however you test it, you should have apriori understanding of the problem. Diving in the data and trying to see what floats (i.e. which of the p-values is higher than \\(\\alpha\\)) is not a good idea (Wasserstein and Lazar, 2016). Follow the proper procedure if you want to test the hypothesis. Finally, the hypothesis testing mechanism has been criticised by many scientists over the years. For example, Cohen (1994) discussed issues with the procedure, making several important points, some of which are outlined above. He also points out at the fundamental problem with hypothesis testing, which is typically neglected by proponents of the procedure: in practice, null hypothesis is always wrong. In reality, it is not possible for a value to be equal, for example, to zero. Even an unimportant effect of one variable on another would be close to zero, but not equal to it. This means that with the increase of the sample size, H\\(_0\\) will inevitably be rejected. Furthermore, our mind operates with binary constructs: true / not true - while the hypothesis testing works in the dichotomy “I know / I don’t know,” with the latter appearing when there is not enough evidence to reject H\\(_0\\). Summarising all of this, in my opinion, it makes sense to move away from hypothesis testing if possible and switch to other instruments for uncertainty measurement, such as confidence intervals. 5.3.1 Common mistakes related to hypothesis testing Over the years of teaching statistics, I have seen many different mistakes, related to hypothesis testing. No wonder, this is a difficult topic to grasp. Here, I have decided to summarise several typical erroneous statements, providing explanations why they are wrong. They partially duplicate the 6 principles from ASA discussed above, but they are formulated slightly differently. “Calculated value is lower than the critical one, so the null hypothesis is true.” This is wrong on so many level, that I do not even know where to start. We can never know if the hypothesis is true or wrong. All the evidence might point towards the H\\(_0\\) being correct, but it still can be wrong and at some point in future one observation might reject it. The classical example is the “Black swan in Australia.” Up until the discover of Australia, the Europeans thought that there only exist white swans. This was supported by all the observations they had. Wise people would say that “We fail to reject H\\(_0\\) that all swans are white.” Uneducated people would be tempted to say that \" H\\(_0\\): All swans are white\" is true. After discovering Australia in 1606, Europeans have collected evidence of existence of black swans, thus rejecting H\\(_0\\) and showing that “not all swans are white,” which implies that actually the alternative hypothesis is true. This is the essence of scientific method: we always try rejecting H\\(_0\\), collecting some evidence. If we fail to reject it, it might just mean that we have not collected enough evidence or have not modelled it correctly. “Calculated value is lower than the critical one, so we accept the null hypothesis.” We never accept null hypothesis. Even if your house is on fire or there is a tsunami coming, you should not “accept H\\(_0\\).” This is a fundamental statistical principle. We collect evidence to reject the null hypothesis. If we do not have enough evidence, then we just fail to reject it, but we can never accept it, because failing to reject just means that we need to collect more data. As mentioned earlier, we focus on rejecting hypothesis, because this at least tells us, what the phenomenon is not (e.g. that not all swans are white). “The parameter in the model is significant, so we can conclude that…” You cannot conclude if something is significant or not without specifying the significance level. Things are only significant if they pass specific test on a specified level \\(\\alpha\\). The correct sentence would be “The parameter in the model is significant on 3%, so we can conclude that…,” where 3% is the selected significance level \\(\\alpha\\). “The parameter in the model is significant because p-value&lt;0.0000” Indeed, some statistical software will tell you that p-value&lt;0.0000, but this just says that the value is very small and cannot be printed. Even if it is that small, you need to state your significance level and compare it with the p-value. You might wonder, “why bother if it is that low?” Well, if you change the sample size or change model specification, your p-value will change as well, and in some cases it might all of a sudden become higher than your significance level. So, you always need to keep it in mind and make conclusions based on the significance level, not just based on what software tells you. “The parameter is not significant, so we remove the variable from the model.” This is one of the worst motivations for removing variables that there is (statistical blasphemy!). There are thousands of reasons, why you might get p-value greater than your significance level (assumptions do not hold, sample is too small, the test is too weak, the true value is small etc) and only one of them is that the explanatory variable does not impact the response variable and thus you fail to reject H\\(_0\\). Are you sure that you face exactly this one special case? If yes, then you already have some other (better) reasons to remove the variable. This means that you should not make decisions just based on the results of a statistical test. You always need to have some fundamental reason to include or remove variables in the model. Hypothesis testing just gives you additional information that can be helpful for decision making. “The parameter in the new model is more significant than in the old one.” There is no such thing as “more significant” or “less significant.” Significance is binary and depends on the selected level. The only thing you can conclude is whether the parameter is significant on the chosen level \\(\\alpha\\) or not. “The p-value of one variables is higher than the p-value of another, so….” p-values are not comparable between variables. They only show on what level the hypothesis is rejected and only work together with the chosen significance level. (6) is similar to this mistake. Remember that the p-value itself is random and will change if you change the sample size or the model specification. Always keep this in mind, when conducting statistical tests. All these mistakes typically arise because of the misuse of p-values and hypothesis testing mechanism. This is one of the reasons, why I prefer confidence intervals, when possible (as discussed above). Finally, a related question to all of this, is how to select the significance level. Dave Worthington, a colleague of mine and a Statistics mentor at Lancaster University, has proposed an interesting motivation for that. If you do not have a level, driven by the problem (e.g. we need to satisfy 99% of demand, thus the significance level is 1%), then select the one for your life time. In how many cases in your life would you be ready to make a mistake? Would it be 5%? 3%? 1%? Select something and stick with it. Then over the years you will know that you have made the selected proportion of mistakes, when conducting different statistical test in variaous circumstances. References "],["correlations.html", "Chapter 6 Measuring relations between variables", " Chapter 6 Measuring relations between variables Now that we have discussed confidence intervals and hypothesis testing, we can move towards the analysis of relations between variables, in a way continuing the preliminary data analysis that we finished in Section 2. We continue using the same dataset mtcarsData with the two categorical variables, am and vs. "],["nominal-scale-1.html", "6.1 Nominal scale", " 6.1 Nominal scale As discussed in Section 1.2, not all scales support the more advanced operations (such as taking mean in ordinal scale). This means that if we want to analyse relations between variables, we need to use appropriate instrument. The coefficients that show relations between variables are called “measures of association.” We start their discussions with the simplest scale - nominal. There are several measures of association for the variables in nominal scale. They are all based on calculating the number of specific values of variables, but use different formulae. The first one is called contingency coefficient \\(\\phi\\) and can only be calculated between variables that have only two values. As the name says, this measure is based on the contingency table. Here is an example: table(mtcarsData$vs,mtcarsData$am) ## ## automatic manual ## V-shaped 12 6 ## Straight 7 7 The \\(\\phi\\) coefficient is calculated as: \\[\\begin{equation} \\phi = \\frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{\\sqrt{n_{1,\\cdot}\\times n_{2,\\cdot}\\times n_{\\cdot,1}\\times n_{\\cdot,2}}} , \\tag{6.1} \\end{equation}\\] where \\(n_{i,j}\\) is the element of the table on row \\(i\\) and column \\(j\\), \\(n_{i,\\cdot}=\\sum_{j}n_{i,j}\\) - is the sum in row \\(i\\) and \\(n_{\\cdot,j}=\\sum_{i} n_{i,j}\\) - is the sum in column \\(j\\). This coefficient lies between -1 and 1 and has a simple interpretation: if will be close to 1, when the elements on diagonal are greater than the off-diagonal ones, implying that there is a relation between variables. The value of -1 can only be obtained, when off-diagonal elements are non-zero, while the diagonal ones are zero. Finally, if the values in the contingency table are distributed evenly, the coefficient will be equal to zero. In our case the value of \\(\\phi\\) is: (12*7 - 6*7)/sqrt(19*13*14*18) ## [1] 0.1683451 This is a very low value, so even if the two variables are related, the relation is not well pronounced. In order to see, whether this value is statistically significantly different from zero, we could test a statistical hypothesis (hypothesis testing was discussed in Section 5.3): \\(H_0\\): there is no relation between variables \\(H_1\\): there is some relation between variables This can be done using \\(\\chi^2\\) test, the statistics for which is calculated via: \\[\\begin{equation} \\chi^2 = \\sum_{i,j} \\frac{n \\times n_{i,j} - n_{i,\\cdot} \\times n_{\\cdot,j}}{n \\times n_{i,\\cdot} \\times n_{\\cdot,j}} , \\tag{6.2} \\end{equation}\\] where \\(n\\) is the sum of elements in the contingency table. The value calculated based on (6.2) will follow \\(\\chi^2\\) distribution with \\((r-1)(c-1)\\) degrees of freedom, where \\(r\\) is the number of rows and \\(c\\) is the number of columns in contingency table. This is a proper statistical test, so it should be treated as one. We select my favourite significance level, 1% and can now conduct the test: chisq.test(table(mtcarsData$vs,mtcarsData$am)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(mtcarsData$vs, mtcarsData$am) ## X-squared = 0.34754, df = 1, p-value = 0.5555 Given that p-value is greater than 1%, we fail to reject the null hypothesis and can conclude that the relation does not seem to be different from zero - we do not find a relation between the variables in our data. The main limitation of the coefficient \\(\\phi\\) is that it only works for the \\(2\\times 2\\) tables. In reality we can have variables in nominal scale that take several values and it might be useful to know relations between them. For example, we can have a variable colour, which takes values red, green and blue and we would want to know if it is related to the transmission type. We do not have this variable in the data, so just for this example, we will create one (using multinomial distribution): colour &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.4,0.5,0.6)) colour &lt;- factor(colour, levels=c(1:3), labels=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) barplot(table(colour), xlab=&quot;Colour&quot;) In order to measure relation between the new variable and the am, we can use Cramer’s V coefficient, which relies on the formula of \\(\\chi^2\\) test (6.2): \\[\\begin{equation} V = \\sqrt{\\frac{\\chi^2}{n\\times \\min(r-1, c-1)}} . \\tag{6.3} \\end{equation}\\] Cramer’s V always lies between 0 and 1, becoming close to one only if there is some relation between the two categorical variables. greybox package implements this coefficient in cramer() function: cramer(mtcarsData$am,colour) ## Cramer&#39;s V: 0.1003 ## Chi^2 statistics = 0.3222, df: 2, p-value: 0.8512 The output above shows that the value of the coefficient is approximately 0.1, which is low, implying that the relation between the two variables is very weak. In addition, the p-value tells us that we fail to reject the null hypothesis on 1% level in the \\(\\chi^2\\) test (6.2), and the relation does not look statistically significant. So we can conclude that according to our data, the two variables are not related (no wonder, we have generated one of them). The main limitation of Cramer’s V is that it is difficult to interpret beyond “there is a relation.” Imagine a situation, where the colour would be related to the variable “class” of a car, that can take 5 values. What could we say more than to state the fact that the two are related? After all, in that case you end up with a contingency table of \\(3\\times 5\\), and it might not be possible to say how specifically one variable changes with the change of another one. Still, Cramer’s V at least provides some information about the relation of two categorical variables. "],["ordinal-scale-1.html", "6.2 Ordinal scale", " 6.2 Ordinal scale As discussed in Section 1.2, ordinal scale has more flexibility than the nominal one - its values have natural ordering, which can be used, when we want to measure relations between several variables in ordinal scale. Yes, we can use Cramer’s V and \\(\\chi^2\\) test, but this way we would not be using all advantages of the scale. So, what can we use in this case? There are three popular measures of association for variables in ordinal scale: Goodman-Kruskal’s \\(\\gamma\\), Yule’s Q, Kendall’s \\(\\tau\\). Given that the ordinal scale does not have distances, the only thing we can do is to compare values of variables between different observations and say, whether one is greater than, less than or equal to another. What can be done with two variables in ordinal scale is the comparison of the values of those variables for a couple respondents. Based on that the pairs of the observations can be called: Concordant if both \\(x_1 &lt; x_2\\) and \\(y_1 &lt; y_2\\) or \\(x_1 &gt; x_2\\) and \\(y_1 &gt; y_2\\) - implying that there is an agreement in order between the two variables (e.g. with a switch from a younger age group to the older one, the size of the T-shirt will switch from S to M); Discordant if for \\(x_1 &lt; x_2\\) and \\(y_1 &gt; y_2\\) or for \\(x_1 &gt; x_2\\) and \\(y_1 &lt; y_2\\) - implying that there is a disagreement in the order of the two variables (e.g. with a switch from a younger age group to the older one, the satisfaction from drinking Coca-Cola will switch to the lower level); Ties if both \\(x_1 = x_2\\) and \\(y_1 = y_2\\); Neither otherwise (e.g. when \\(x_1 = x_2\\) and \\(y_1 &lt; y_2\\)). All the measures of association for the variables in ordinal scale rely on the number of concordant, discordant variables and number of ties. All of these measures lie in the region of [-1, 1]. Goodman-Kruskal’s \\(\\gamma\\) is calculated using the following formula: \\[\\begin{equation} \\gamma = \\frac{n_c - n_d}{n_c + n_d}, \\tag{6.4} \\end{equation}\\] where \\(n_c\\) is the number of concordant pairs, \\(n_d\\) is the number of discordant pairs. This is a very simple measure of association, but it only works with scales of the same size (e.g. 5 options in one variable and 5 options in the other one) and ignores the ties. In order to demonstrate this measure in action, we will create two artificial variables in ordinal scale: Age of a person: young, adult and elderly; Size of t-shirt they wear: S, M or L. Here how we can do that in R: age &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.4,0.5,0.6)) age &lt;- factor(age, levels=c(1:3), labels=c(&quot;young&quot;,&quot;adult&quot;,&quot;elderly&quot;)) size &lt;- c(1:3) %*% rmultinom(nrow(mtcars), 1, c(0.3,0.5,0.7)) size &lt;- factor(size, levels=c(1:3), labels=c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;)) And here is how the relation between these two artificial variables looks: tableplot(age,size,xlab=&quot;Age&quot;,ylab=&quot;T-shirt size&quot;) Figure 6.1: Heat map for age of a respondent and the size of their t-shirt. The graphical analysis based on Figure 6.1 does not provide a clear information about the relation between the two variables. But this is where the Goodman-Kruskal’s \\(\\gamma\\) becomes useful. We will use GoodmanKruskalGamma() function from DescTools package for R for this: DescTools::GoodmanKruskalGamma(age,size,conf.level=0.95) ## gamma lwr.ci upr.ci ## -0.03846154 -0.51302449 0.43610141 This function returns three values: the \\(\\gamma\\), which is close to zero in our case, implying that there is no relation between the variables, lower and upper bounds of the 95% confidence interval. Note that the interval shows us how big the uncertainty about the parameter is: the true value in the population can be anywhere between -0.51 and 0.44. But based on all these values we can conclude that we do not see any relation between the variables in our sample. The next measure is called Yule’s Q and is considered as a special case of Goodman-Kruskal’s \\(\\gamma\\) for the variables that only have 2 options. It is calculated based on the resulting contingency \\(2\\times 2\\) table and has some similarities with the contingency coefficient \\(\\phi\\): \\[\\begin{equation} \\mathrm{Q} = \\frac{n_{1,1} n_{2,2} - n_{1,2} n_{2,1}}{n_{1,1} n_{2,2} + n_{1,2} n_{2,1}} . \\tag{6.5} \\end{equation}\\] The main difference from the contingency coefficient is that it assumes that the data has ordering, it implicitly relies on the number of concordant (on the diagonal) and discordant (on the off diagonal) pairs. In our case we could calculate it if we had two simplified variables based on age and size (in real life we would need to recode them to “young,” “older” and “S,” “Bigger than S” respectively): table(age,size)[1:2,1:2] ## size ## age S M ## young 2 4 ## adult 2 2 (2*2-4*2)/(2*2+4*2) ## [1] -0.3333333 In our toy example, the measure shows that there is a weak negative relation between the trimmed age and size variables. We do not make any conclusions based on this, because this is not meaningful and is shown here just for purposes of demonstration. Finally, there is Kendall’s \\(\\tau\\). In fact, there are three different coefficients, which have the same name, so in the literature they are known as \\(\\tau_a\\), \\(\\tau_b\\) and \\(\\tau_c\\). \\(\\tau_a\\) coefficient is calculated using the formula: \\[\\begin{equation} \\tau_a = \\frac{n_c - n_d}{\\frac{T (T-1)}{2}}, \\tag{6.6} \\end{equation}\\] where \\(T\\) is the number of observations, and thus in the denominator, we have the number of all the pairs in the data. In theory this coefficient should lie between -1 and 1, but it does not solve the problem with ties, so typically it will not reach the boundary values and will say that the relation is weaker than it really is. Similar to Goodman-Kruskal’s \\(\\gamma\\), it can only be applied to the variables that have the same number of levels (same sizes of scales). In order to resolve some of these issues, \\(\\tau_b\\) was developed: \\[\\begin{equation} \\tau_b = \\frac{n_c - n_d}{\\sqrt{\\left(\\frac{T (T-1)}{2} - n_x\\right)\\left(\\frac{T (T-1)}{2} - n_y\\right)}}, \\tag{6.7} \\end{equation}\\] where \\(n_x\\) and \\(n_y\\) are the number of ties calculated for both variables. This coefficient resolves the problem with ties and can now reach the boundary values in practice. However, this coefficient does not resolve the issue with different scale sizes. And in order to address this problem, we have \\(\\tau_c\\) (Stuart-Kendall’s \\(\\tau_c\\)): \\[\\begin{equation} \\tau_c = \\frac{n_c - n_d}{\\frac{n^2}{2}\\frac{\\min(r, c)-1}{\\min(r, c)}}, \\tag{6.7} \\end{equation}\\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns. This coefficient works for variables with different lengths of scales (e.g. age with 5 options and t-shirt size with 7 options). But now we are back to the problem with the ties… In R, the cor() function implements Kendall’s \\(\\tau_a\\) and \\(\\tau_b\\) (the function will select automatically based on the presence of ties). There are also functions KendallTauA(), KendallTauB() and StuartTauC() in DescTools package that implement the three respective measures of association. The main limitation of cor() function is that it only works with numerical variables, so we would need to transform variables before applying the function. The functions from DescTools package, on the other hand, work with factors. Here are the values of the three coefficients for our case: DescTools::KendallTauA(age,size,conf.level=0.95) ## tau_a lwr.ci upr.ci ## -0.01612903 -0.15347726 0.12121920 DescTools::KendallTauB(age,size,conf.level=0.95) ## tau_b lwr.ci upr.ci ## -0.02469136 -0.32938991 0.28000720 DescTools::StuartTauC(age,size,conf.level=0.95) ## tauc lwr.ci upr.ci ## -0.0234375 -0.3126014 0.2657264 Given that both variables have the same scale sizes, we should use either \\(\\tau_a\\) or \\(\\tau_b\\) for the analysis. However, we do not know if there are any ties in the data, so the safer option would be to use \\(\\tau_b\\) coefficient. The value of the coefficient and its confidence interval tell us that there is no obvious association between the two variables in our sample. This is expected, because the two variables were generated independently of each other. "],["correlationCoefficient.html", "6.3 Numerical scale", " 6.3 Numerical scale Finally we come to the discussion of relations between variables measured in numerical scales. The most famous measure in this category is the Pearson’s correlation coefficient, which population value is: \\[\\begin{equation} \\rho_{x,y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\sigma_y}, \\tag{6.8} \\end{equation}\\] where \\(\\sigma_{x,y}\\) is the covariance between variables \\(x\\) and \\(y\\), while \\(\\sigma_x\\) and \\(\\sigma_y\\) are standard deviations of these variables. Typically, we do not know the population values, so this coefficient can be estimated in sample via: \\[\\begin{equation} r_{x,y} = \\frac{\\mathrm{cov}(x,y)}{\\sqrt{V(x)V(y)}}, \\tag{6.9} \\end{equation}\\] where all the values from (6.8) are substituted by their in-sample estimates. This coefficient measures the strength of linear relation between variables and lies between -1 and 1, where the boundary values correspond to perfect linear relation and 0 implies that there is no linear relation between the variables. In some textbooks the authors claim that this coefficient relies on Normal distribution of variables, but nothing in the formula assumes that. It was originally derived based on the simple linear regression (see Section 7) and its rough idea is to get information about the angle of the straight line drawn on the scatterplot. It might be easier to explain this on an example: plot(mtcarsData$disp,mtcarsData$mpg, xlab=&quot;Displacement&quot;,ylab=&quot;Mileage&quot;) abline(lm(mpg~disp,mtcarsData),col=&quot;red&quot;) Figure 6.2: Scatterplot for dispalcement vs mileage variables in mtcars dataset Figure 6.2 shows the scatterplot between the two variables and also has the straight line, going through the cloud of points. The closer the points are to the line, the stronger the linear relation between the two variables is. The line corresponds to the formula \\(\\hat{y}=a_0+a_1 x\\), where \\(x\\) is the displacement and \\(\\hat{y}\\) is the line value for the Mileage. The same relation can be presented if we swap the axes and draw the line \\(\\hat{x}=b_0+b_1 y\\): plot(mtcarsData$mpg,mtcarsData$disp, xlab=&quot;Displacement&quot;,ylab=&quot;Mileage&quot;) abline(lm(disp~mpg,mtcarsData),col=&quot;red&quot;) Figure 6.3: Scatterplot for mileage vs dispalcement The slopes for the two lines will in general differ, and will only coincide if the two variables have functional relations (all the point lie on the line). Based on this property, the correlation coefficient was originally constructed, as a geometric mean of the two parameters of slopes: \\(r_{x,y}=\\sqrt{a_1 b_1}\\). We will come back to this specific formula later in Section 7. But this idea provides an explanation why the correlation coefficient measures the strength of linear relation. For the two variables of interest it will be: cor(mtcarsData$mpg,mtcarsData$disp) ## [1] -0.8475514 Which shows strong negative linear relation between the displacement and mileage. This makes sense, because in general the cars with bigger engines will have bigger consumption and thus will make less miles per gallon of fuel. The more detailed information about the correlation is provided by the cor.test() function: cor.test(mtcarsData$mpg,mtcarsData$disp) ## ## Pearson&#39;s product-moment correlation ## ## data: mtcarsData$mpg and mtcarsData$disp ## t = -8.7472, df = 30, p-value = 9.38e-10 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9233594 -0.7081376 ## sample estimates: ## cor ## -0.8475514 In addition to the value, we now have results of the hypothesis testing (where null hypothesis is \\(\\rho_{x,y}=0\\)) and the confidence interval for the parameter. Given that the value of the parameter is close to its bound, we could conclude that the linear relation between the two variables is strong and statistically significant on 1% level. Note that the value of correlation coefficient only depends on the distance of points from the straight line, it does not depend on the slope (excluding case, when slope is equal to zero and thus the coefficient is equal to zero as well). So the following two cases will have exactly the same correlation coefficients: error &lt;- rnorm(100,0,10) x &lt;- c(1:100) y1 &lt;- 10+0.5*x+0.5*error y2 &lt;- 2+1.5*x+1.5*error # Produce the plots par(mfcol=c(1,2)) plot(x,y1,ylim=c(0,200)) abline(lm(y1~x),col=&quot;red&quot;) text(30,150,paste0(&quot;r=&quot;,round(cor(x,y1),5))) plot(x,y2,ylim=c(0,200)) abline(lm(y2~x),col=&quot;red&quot;) text(30,150,paste0(&quot;r=&quot;,round(cor(x,y2),5))) Figure 6.4: Example of relations with exactly the same correlations, but different slopes. There are other examples of cases, when correlation coefficient would be misleading or not provide the necessary information. One of the canonical examples is the Anscombe’s quartet (Wikipedia, 2021), which shows very different types of relations, for which the Pearson’s correlation coefficient would be exactly the same. An important lesson from this is to always do graphical analysis (see Section 2.2) of your data, when possible - this way misleading situations can be avoided. Coming back to the scatterplot in Figure 6.2, it demonstrates some non-linearity in the relation between the two variables. So, it would make sense to have a different measure that could take it into account. This is where Spearman’s correlation coefficient becomes useful. It is calculated using exactly the same formula (6.9), but applied to the data in ranks. By using ranks, we loose information about the natural zero and distances between values of the variable, but at the same time we linearise possible non-linear relations. So, Spearman’s coefficient shows the strength of monotonic relation between the two variables: cor.test(mtcarsData$mpg,mtcarsData$disp, method=&quot;spearman&quot;) ## Warning in cor.test.default(mtcarsData$mpg, mtcarsData$disp, method = ## &quot;spearman&quot;): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: mtcarsData$mpg and mtcarsData$disp ## S = 10415, p-value = 6.37e-13 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.9088824 We can notice that the value of the Spearman’s coefficient in our case is higher than the value of the Pearson’s correlation, which implies that there is indeed non-linear relation between variables. The two variables have a strong monotonic relation, which makes sense for the reasons discussed earlier. The non-linearity makes sense as well because the car with super powerful engines would still be able to do several miles on a gallon of fuel, no matter what. The relation will never be zero or even negative. Note that while Spearman’s correlation will tell you something about monotonic relations, it will fail to capture all other non-linear relations between variables. For example, in the following case the true relation is trigonometric: x &lt;- c(1:100) y &lt;- sin(x) plot(x,y,type=&quot;l&quot;) But neither Pearson’s nor Spearman’s coefficients will be able to capture it: cor(x,y) ## [1] -0.04806497 cor(x,y,method=&quot;spearman&quot;) ## [1] -0.04649265 In order to correctly diagnose such non-linear relation, either one or both variables need to be transformed to linearise the relation. In our case this implies measuring the relation between \\(y\\) and \\(\\sin(x)\\) instead of \\(y\\) and \\(x\\): cor(sin(x),y) ## [1] 1 References "],["correlationsMixed.html", "6.4 Mixed scales", " 6.4 Mixed scales Finally, when we have two variables measured in different scales, the general recommendation is to use the measure of association for the lower scale. For example, if we have the nominal variable colour and the ordinal variable size (both related to T-shirts people prefer), we should use Cramer’s V in order to measure the relation between them: cramer(colour,size) ## Cramer&#39;s V: 0.2991 ## Chi^2 statistics = 5.7241, df: 4, p-value: 0.2207 Similarly, if we have a numerical and ordinal variables, we should use one of the measures for ordinal scales. However, in some cases we might be able to use a different measure of association. One of those is called multiple correlation coefficient and can be calculated for variables in numerical vs categorical scales. This coefficient can be calculated using different principles, the simplest of which is constructing a regression model (discussed later in Section 8) of numerical variable from the dummy variables (see Section 10) created from the categorical one and then extracting the square root of coefficient of determination (discussed in Section ??). The resulting coefficient lies between 0 and 1, where 1 implies perfect linear relation between the two variables and 0 implies no linear relation between them. mcor() function from greybox implements this: mcor(mtcars$am, mtcars$mpg) ## Multiple correlations value: 0.5998 ## F-statistics = 16.8603, df: 1, df resid: 30, p-value: 3e-04 Based on the value above, we can conclude that the type of transmission has a linear relation with the mileage. This aligns with what we have already discovered earlier, in preliminary analysis section (Section 2.2) in Figure 2.14. Finally, there is a function assoc() (aka association()) in greybox package, which will automatically select the necessary measure of association based on the type of a variable and produce three matrices: 1. measures of association, 2. p-values for testing H\\(_0\\) that there measure is equal to zero, 3. names of functions used for each pair. Here how it works for the mtcarsData example: assoc(mtcarsData) ## Associations: ## values: ## mpg cyl disp hp drat wt qsec vs am ## mpg 1.0000 0.8558 -0.8476 -0.7762 0.6812 -0.8677 0.4187 0.6640 0.5998 ## cyl 0.8558 1.0000 0.9152 0.8449 0.7018 0.7826 0.5913 0.8166 0.5226 ## disp -0.8476 0.9152 1.0000 0.7909 -0.7102 0.8880 -0.4337 0.7104 0.5912 ## hp -0.7762 0.8449 0.7909 1.0000 -0.4488 0.6587 -0.7082 0.7231 0.2432 ## drat 0.6812 0.7018 -0.7102 -0.4488 1.0000 -0.7124 0.0912 0.4403 0.7127 ## wt -0.8677 0.7826 0.8880 0.6587 -0.7124 1.0000 -0.1747 0.5549 0.6925 ## qsec 0.4187 0.5913 -0.4337 -0.7082 0.0912 -0.1747 1.0000 0.7445 0.2299 ## vs 0.6640 0.8166 0.7104 0.7231 0.4403 0.5549 0.7445 1.0000 0.1042 ## am 0.5998 0.5226 0.5912 0.2432 0.7127 0.6925 0.2299 0.1042 1.0000 ## gear 0.6551 0.5309 0.7671 0.6638 0.8319 0.6587 0.6334 0.6181 0.8090 ## carb 0.6667 0.6173 0.5605 0.7873 0.3344 0.6129 0.6695 0.6924 0.4415 ## gear carb ## mpg 0.6551 0.6667 ## cyl 0.5309 0.6173 ## disp 0.7671 0.5605 ## hp 0.6638 0.7873 ## drat 0.8319 0.3344 ## wt 0.6587 0.6129 ## qsec 0.6334 0.6695 ## vs 0.6181 0.6924 ## am 0.8090 0.4415 ## gear 1.0000 0.5080 ## carb 0.5080 1.0000 ## ## p-values: ## mpg cyl disp hp drat wt qsec vs am gear ## mpg 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0003 ## cyl 0.0000 0.0000 0.0000 0.0000 0.0001 0.0000 0.0020 0.0000 0.0126 0.0012 ## disp 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0000 ## hp 0.0000 0.0000 0.0000 0.0000 0.0100 0.0000 0.0000 0.0000 0.1798 0.0002 ## drat 0.0000 0.0001 0.0000 0.0100 0.0000 0.0000 0.6196 0.0117 0.0000 0.0000 ## wt 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.3389 0.0010 0.0000 0.0003 ## qsec 0.0171 0.0020 0.0131 0.0000 0.6196 0.3389 0.0000 0.0000 0.2057 0.0006 ## vs 0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000 0.0000 0.5555 0.0022 ## am 0.0003 0.0126 0.0004 0.1798 0.0000 0.0000 0.2057 0.5555 0.0000 0.0000 ## gear 0.0003 0.0012 0.0000 0.0002 0.0000 0.0003 0.0006 0.0022 0.0000 0.0000 ## carb 0.0065 0.0066 0.0662 0.0001 0.6607 0.0242 0.0061 0.0090 0.2838 0.0857 ## carb ## mpg 0.0065 ## cyl 0.0066 ## disp 0.0662 ## hp 0.0001 ## drat 0.6607 ## wt 0.0242 ## qsec 0.0061 ## vs 0.0090 ## am 0.2838 ## gear 0.0857 ## carb 0.0000 ## ## types: ## mpg cyl disp hp drat wt qsec ## mpg &quot;none&quot; &quot;mcor&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; ## cyl &quot;mcor&quot; &quot;none&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## disp &quot;pearson&quot; &quot;mcor&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; ## hp &quot;pearson&quot; &quot;mcor&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; ## drat &quot;pearson&quot; &quot;mcor&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; &quot;pearson&quot; ## wt &quot;pearson&quot; &quot;mcor&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; &quot;pearson&quot; ## qsec &quot;pearson&quot; &quot;mcor&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;pearson&quot; &quot;none&quot; ## vs &quot;mcor&quot; &quot;cramer&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## am &quot;mcor&quot; &quot;cramer&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## gear &quot;mcor&quot; &quot;cramer&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## carb &quot;mcor&quot; &quot;cramer&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## vs am gear carb ## mpg &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## cyl &quot;cramer&quot; &quot;cramer&quot; &quot;cramer&quot; &quot;cramer&quot; ## disp &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## hp &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## drat &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## wt &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## qsec &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; &quot;mcor&quot; ## vs &quot;none&quot; &quot;cramer&quot; &quot;cramer&quot; &quot;cramer&quot; ## am &quot;cramer&quot; &quot;none&quot; &quot;cramer&quot; &quot;cramer&quot; ## gear &quot;cramer&quot; &quot;cramer&quot; &quot;none&quot; &quot;cramer&quot; ## carb &quot;cramer&quot; &quot;cramer&quot; &quot;cramer&quot; &quot;none&quot; "],["simpleLinearRegression.html", "Chapter 7 Simple Linear Regression", " Chapter 7 Simple Linear Regression When we want to analyse some relations between variables, we can do graphical and correlations analysis. But this will not provide us sufficient information about what happens with the response variable with the change of explanatory variable. So it makes sense to consider the possible relations between variables, and the basis for this is Simple Linear Regression, which can be represented in the form: \\[\\begin{equation} y_j = \\beta_0 + \\beta_1 x_j + \\epsilon_j , \\tag{7.1} \\end{equation}\\] where \\(\\beta_0\\) is the intercept (constant term), \\(\\beta_1\\) is the coefficient for the slope parameter and \\(\\epsilon_j\\) is the error term. The regression model is a basic statistical model that captures the relation between an explanatory variable \\(x_j\\) and the response variable \\(y_j\\). The parameters of the models are typically denoted as \\(\\beta_0\\) and \\(\\beta_1\\) in econometrics literature, but we use \\(\\beta_0\\) and \\(\\beta_1\\) because we will use \\(\\beta\\) for other purposes later in this textbook. In order to better understand what simple linear regression implies, consider the scatterplot (we discussed it earlier in Section 2.2) shown in Figure 7.1. slmMPGWt &lt;- lm(mpg~wt,mtcarsData) plot(mtcarsData$wt, mtcarsData$mpg, xlab=&quot;Weight&quot;, ylab=&quot;Mileage&quot;, xlim=c(0,6), ylim=c(0,40)) abline(h=0, col=&quot;grey&quot;) abline(v=0, col=&quot;grey&quot;) abline(slmMPGWt,col=&quot;red&quot;) text(4,35,paste0(c(&quot;mpg=&quot;,round(coef(slmMPGWt),2),&quot;wt+et&quot;),collapse=&quot;&quot;)) Figure 7.1: Scatterplot diagram between weight and mileage. The line drawn on the plot is the regression line, parameters of which were estimated based on the available data. In this case the intercept \\({b}_0\\)=37.29, meaning that this is where the red line crosses the y-axis, while the parameter of slope \\({b}_1\\)=-5.34 shows how fast the values change (how steep the line is). I’ve added hat symbols on the parameters to point out that they were estimated based on a sample of data. If we had all the data in the universe (population) and estimated a correct model on it, we would not need the hats. In simple linear regression, the re line will always go through the cloud of points, showing the averaged out tendencies. The one that we observe above can be summarise as “with the increase of weight, on average the mileage of cars goes down.” Note that we might find some specific points, where the increase of weight would not decrease mileage (e.g. the two furthest left points show this), but this can be considered as a random fluctuation, so overall, the average tendency is as described above. "],["OLS.html", "7.1 Ordinary Least Squares (OLS)", " 7.1 Ordinary Least Squares (OLS) For obvious reasons, we do not have the values of parameters from the population. This means that we will never know what the true intercept and slope are. Luckily, we can estimate them based on the sample of data. There are different ways of doing that, and the most popular one is called “Ordinary Least Squares” method. This is the method that was used in the estimation of the model in Figure 7.1. So, how does it work? Figure 7.2: Scatterplot diagram between weight and mileage. When we estimate the simple linear regression model, the model (7.1) transforms into: \\[\\begin{equation} y_j = {b}_0 + {b}_1 x_j + e_j , \\tag{7.2} \\end{equation}\\] where \\(b_0\\) is the estimate of \\(\\beta_0\\), \\(b_1\\) is the estimate of \\(\\beta_1\\) and \\(e_j\\) is the estimate of \\(\\epsilon_j\\). This is because we do not know the true values of parameters and thus they are substituted by their estimates. This also applies to the error term for which in general \\(e_j \\neq \\epsilon_j\\) because of the sample estimation. Now consider the same situation with weight vs mileage in Figure 7.2 but with some arbitrary line with unknown parameters. Each point on the plot will typically lie above or below the line, and we would be able to calculate the distances from those points to the line. They would correspond to \\(e_j = y_j - \\hat{y}_j\\), where \\(\\hat{y}_j\\) is the value of the regression line (aka “fitted” value) for each specific value of explanatory variable. For example, for the weight of car of 1.835 tones, the actual mileage is 33.9, while the fitted value is 27.478. The resulting error (or residual of model) is 6.422. We could collect all these errors of the model for all available cars based on their weights and this would result in a vector of positive and negative values like this: ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## -2.2826106 -0.9197704 -2.0859521 1.2973499 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## -0.2001440 -0.6932545 -3.9053627 4.1637381 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 2.3499593 0.2998560 -1.1001440 0.8668731 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## -0.0502472 -1.8830236 1.1733496 2.1032876 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 5.9810744 6.8727113 1.7461954 6.4219792 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## -2.6110037 -2.9725862 -3.7268663 -3.4623553 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 2.4643670 0.3564263 0.1520430 1.2010593 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## -4.5431513 -2.7809399 -3.2053627 -1.0274952 This corresponds to the formula: \\[\\begin{equation} e_j = y_j - {b}_0 - {b}_1 x_j. \\tag{7.3} \\end{equation}\\] If we needed to estimate parameters \\({b}_0\\) and \\({b}_1\\) of the model, we would need to minimise those distances by changing the parameters of the model. The problem is that some errors are positive, while the others are negative. If we just sum them up, they will cancel each other out, and we would loose the information about the distance. The simplest way to get rid of sign and keep the distance is by taking squares of each error and calculating Sum of Squared Errors for the whole sample \\(T\\): \\[\\begin{equation} \\mathrm{SSE} = \\sum_{j=1}^n e_j^2 . \\tag{7.4} \\end{equation}\\] If we now minimise SSE by changing values of parameters \\({b}_0\\) and \\({b}_1\\), we will find those parameters that would guarantee that the line goes somehow through the cloud of points. Luckily, we do not need to use any fancy optimisers for this, as this has analytical solution (in order to get it, insert (7.3) in (7.4), take derivatives with respect to the parameters \\({b}_0\\) and \\({b}_1\\) and equate the resulting values to zero): \\[\\begin{equation} \\begin{aligned} {b}_1 = &amp; \\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(x)} \\\\ {b}_0 = &amp; \\bar{y} - {b}_1 \\bar{x} \\end{aligned} , \\tag{7.5} \\end{equation}\\] where \\(\\bar{x}\\) is the mean of the explanatory variable \\(x_j\\) and \\(\\bar{y}\\) is the mean of the response variables \\(y_j\\). Note that if for some reason \\({b}_1=0\\) (for example, because the covariance between \\(x\\) and \\(y\\) is zero, implying that they are not correlated), then the intercept \\({b}_0 = \\bar{y}\\), meaning that the global average of the data is the best predictor of the variable \\(y_j\\). This method of estimation of parameters based on the minimisation of SSE, is called “Ordinary Least Squares.” It is simple and does not require any specific assumptions: we just minimise the overall distance by changing the values of parameters. Another thing to note is the connection between the parameter \\({b}_1\\) and the correlation coefficient. We have already briefly discussed this in Section 6.3, we could estimate two models given the pair of variable \\(x\\) and \\(y\\): Model (7.2); The inverse model \\(x_j = \\hat{b}_0 + \\hat{b}_1 y_j + u_j\\). We could then extract the slope parameters of the two models via (7.5) and get the value of correlation coefficient as a geometric mean of the two: \\[\\begin{equation} r_{x,y} = \\mathrm{sign}(\\hat{b}_1) \\sqrt{{b}_1 \\hat{b}_1} = \\mathrm{sign}(\\mathrm{cov}(x,y)) \\sqrt{\\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(x)} \\frac{\\mathrm{cov}(x,y)}{\\mathrm{V}(y)}} = \\frac{\\mathrm{cov}(x,y)}{\\sqrt{V(x)V(y)}} , \\tag{7.6} \\end{equation}\\] which is the formula (6.9). This is how the correlation coefficient was originally derived. While we can do some inference based on simple linear regression, we know that the bivariate relations are not often met in practice: typically a variable is influenced by a set of variables, not just by one. This implies that the correct model would typically include many explanatory variables. This is why we will discuss inference in the next section. "],["linearRegressionSimpleQualityOfFit.html", "7.2 Quality of the fit", " 7.2 Quality of the fit In order to get a general impression about the performance of the estimated model, we can calculate several in-sample measures, which could provide us insights about the fit of the model. The fundamental measure that lies in the basis of many other ones is SSE, which is the value of the OLS criterion (7.4). It cannot be interpreted on its own and cannot be used for model comparison, but it shows the overall variability of the data around the regression line. In a more general case, it is written as: \\[\\begin{equation} \\mathrm{SSE} = \\sum_{j=1}^n (y_j - \\hat{y}_j)^2 . \\tag{7.7} \\end{equation}\\] This sum of squares is related to another two, the first being the Sum of Squares Total: \\[\\begin{equation} \\mathrm{SST}=\\sum_{j=1}^n (y_j - \\bar{y})^2, \\tag{7.8} \\end{equation}\\] where \\(\\bar{y}\\) is the in-sample mean. If we divide the value (7.8) by \\(n-1\\), then we will get the in-sample variance (introduced in Section 2.1): \\[\\begin{equation*} \\mathrm{V}(y)=\\frac{\\mathrm{SST}}{n-1}=\\frac{1}{n-1} \\sum_{j=1}^n (y_j - \\bar{y})^2 . \\end{equation*}\\] The last sum of squares is Sum of Squared of Regression: \\[\\begin{equation} \\mathrm{SSR} = \\sum_{j=1}^n (\\bar{y} - \\hat{y}_j)^2 , \\tag{7.9} \\end{equation}\\] which shows the variability of the regression line. It is possible to show that in the linear regression (this is important! This property might be violated in other models), the three sums are related to each other via the following equation: \\[\\begin{equation} \\mathrm{SST} = \\mathrm{SSE} + \\mathrm{SSR} . \\tag{7.10} \\end{equation}\\] Proof. This involves manipulations, some of which are not straightforward: \\[\\begin{equation} \\begin{aligned} \\mathrm{SST} &amp;= \\mathrm{SSR} + \\mathrm{SSE} = \\sum_{j=1}^n (\\hat{y}_j - \\bar{y})^2 + \\sum_{j=1}^n (y_j - \\hat{y}_j)^2 \\\\ &amp;= \\sum_{j=1}^n \\left( \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} + \\bar{y}^2 \\right) + \\sum_{j=1}^n \\left( y_j^2 - 2 y_j \\hat{y}_j + \\hat{y}_j^2 \\right) \\\\ &amp;= \\sum_{j=1}^n \\left( \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} + \\bar{y}^2 + y_j^2 - 2 y_j \\hat{y}_j + \\hat{y}_j^2 \\right) \\\\ &amp;= \\sum_{j=1}^n \\left(\\bar{y}^2 -2 \\bar{y} y_j + y_j^2 + 2 \\bar{y} y_j + \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} - 2 y_j \\hat{y}_j + \\hat{y}_j^2 \\right) \\\\ &amp;= \\sum_{j=1}^n \\left((\\bar{y} - y_j)^2 + 2 \\bar{y} y_j + 2 \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} - 2 y_j \\hat{y}_j \\right) \\end{aligned} . \\tag{7.11} \\end{equation}\\] We can then substitute \\(y_j=\\hat{y}_j+e_j\\) in the right hand side of (7.11) to get: \\[\\begin{equation} \\begin{aligned} \\mathrm{SST} &amp;= \\sum_{j=1}^n \\left((\\bar{y} - y_j)^2 + 2 \\bar{y} (\\hat{y}_j+e_j) + 2 \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} - 2 (\\hat{y}_j+e_j) \\hat{y}_j \\right) \\\\ &amp;= \\sum_{j=1}^n \\left((\\bar{y} - y_j)^2 + 2 \\bar{y} \\hat{y}_j + 2 \\bar{y} e_j + 2 \\hat{y}_j^2 - 2 \\hat{y}_j \\bar{y} - 2 \\hat{y}_j\\hat{y}_j -2 e_j \\hat{y}_j \\right) \\\\ &amp;= \\sum_{j=1}^n \\left((\\bar{y} - y_j)^2 + 2 \\bar{y} e_j + 2 \\hat{y}_j^2 - 2 \\hat{y}_j^2 -2 e_j \\hat{y}_j \\right) \\\\ &amp;= \\sum_{j=1}^n \\left((\\bar{y} - y_j)^2 + 2 \\bar{y} e_j - 2 e_j \\hat{y}_j \\right) \\end{aligned} . \\tag{7.12} \\end{equation}\\] Now if we split the sum into three elements, we will get: \\[\\begin{equation} \\begin{aligned} \\mathrm{SST} &amp;= \\sum_{j=1}^n (\\bar{y} - y_j)^2 + 2 \\sum_{j=1}^n \\left(\\bar{y} e_j\\right) - 2 \\sum_{j=1}^n \\left(e_j \\hat{y}_j \\right) \\\\ &amp;= \\sum_{j=1}^n (\\bar{y} - y_j)^2 + 2 \\bar{y} \\sum_{j=1}^n e_j - 2 \\sum_{j=1}^n \\left(e_j \\hat{y}_j \\right) \\end{aligned} . \\tag{7.13} \\end{equation}\\] The second sum in (7.13) is equal to zero, because OLS guarantees that the in-sample mean of error term is equal to zero. The second one can be expanded to: \\[\\begin{equation} \\begin{aligned} \\sum_{j=1}^n \\left(e_j \\hat{y}_j \\right) = \\sum_{j=1}^n \\left(e_j b_0 + b_1 e_j x_j \\right) \\end{aligned} . \\tag{7.14} \\end{equation}\\] We see the sum of errors in the first sum of (7.14) again, so the first elements is equal to zero again. The second term is equal to zero as well due to OLS estimation (this will be discussed in Chapter 12) because it guarantees that \\(\\sum_{j=1}^n e_j x_j = 0\\). This means that: \\[\\begin{equation} \\mathrm{SST} = \\sum_{j=1}^n (\\bar{y} - y_j)^2 , \\tag{7.15} \\end{equation}\\] which is the formula of SST (7.8). The relation between SSE, SSR and SST is shown in Figure 7.3. If we take any observation in that Figure, we will see how the deviations from the regression line and from the mean are related. Figure 7.3: Relation between different sums of squares. Building upon thata, there is a measure called “Coefficient of Determination,” which is calculated based on the sums of squares discussed above: \\[\\begin{equation} \\mathrm{R}^2 = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}} = \\frac{\\mathrm{SSR}}{\\mathrm{SST}} . \\tag{7.16} \\end{equation}\\] Given the meaning of the sums of squares, we can imagine the following situations to interpret the values of \\(\\mathrm{R}^2\\): The model fits the data in the same way as the mean line (grey line in Figure 7.3). In this case SSE would be equal to SST and SSR would be equal to zero (because \\(\\hat{y}_j=\\bar{y}\\)) and as a result the R\\(^2\\) would be equal to zero. The model fits the data perfectly, without any errors (all points lie on the black line in Figure 7.3). In this situation SSE would be equal to zero and SSR would be equal to SST, because the regression would go through all points (i.e. \\(\\hat{y}_j=y_j\\)). This would make R\\(^2\\) equal to one. In the linear regression model due to (7.10), the coefficient of determination would always lie between zero and one, where zero means that the model does not explain the data at all and one means that it overfits the data. The value itself is usually interpreted as a percentage of variability in data explained by the model. This definition above provides us an important point about the coefficient of determination: it should not be equal to one, and it is alarming if it is very close to one - because in this situation we are implying that there is no randomness in the data, but this contradicts our definition of the statistical model (see Section 1.1.1). The adequate statistical model should always have some randomness in it. The situation of \\(\\mathrm{R}^2=1\\) corresponds to: \\[\\begin{equation*} y_j = b_0 + b_1 x_j , \\end{equation*}\\] implying that all \\(e_j=0\\), which is unrealistic and is only possible if there is a functional relation between \\(y\\) and \\(x\\) (no need for statistical inference then). So, in practice we should not maximise R\\(^2\\) and should be careful with models that have very high values of it. At the same time, too low values of R\\(^2\\) are also alarming, as they tell us that the model becomes: \\[\\begin{equation*} y_j = b_0 + e_j, \\end{equation*}\\] meaning that it is not different from the global mean. So, coefficient of determination in general is not a very good measure for assessing performance of a model. It can be used for further inferences, and for a basic indication of whether the model overfits (R\\(^2\\) close to 1) or underfits (R\\(^2\\) close to 0) the data. But no serious conclusions should be made based on it. Here how this measure can be calculated in R based on the model earlier: n &lt;- nobs(slmMPGWt) R2 &lt;- 1 - sum(resid(slmMPGWt)^2) / (var(actuals(slmMPGWt))*(n-1)) R2 ## [1] 0.7528328 Note that in this formula we used the relation between SST and V\\((y)\\), multiplying the value by \\(n-1\\) to get rid of the denominator. The resulting value tells us that the model has explained 75.3% deviations in the data. Finally, based on coefficient of determination, we can also calculate the coefficient of multiple correlation, which we have already discussed in Section 6.4: \\[\\begin{equation} R = \\sqrt{R^2} = \\sqrt{\\frac{\\mathrm{SSR}}{\\mathrm{SST}}} . \\tag{7.17} \\end{equation}\\] It shows the closeness of relation between the response variable \\(y_j\\) and the explanatory variables to the linear one. The coefficient has a positive sign, no matter what the relation between the variables is. In case of the simple linear regression, it is equal to the correlation coefficient (from Section 6.3) with the sign equal to the sign of the coefficient of the slop \\(b_1\\): \\[\\begin{equation} r_{x,y} = \\mathrm{sign} (b_1) R . \\tag{7.18} \\end{equation}\\] Here is a demonstration of the formula above in R: sign(coef(slmMPGWt)[2]) * sqrt(R2) ## wt ## -0.8676594 cor(mtcars$mpg,mtcars$wt) ## [1] -0.8676594 "],["linearRegression.html", "Chapter 8 Multiple Linear Regression", " Chapter 8 Multiple Linear Regression While simple linear regression provides a basic understanding of the idea of capturing the relations between variables, it is obvious that in reality there are more than one external variable that would impact the response variable. This means that instead of (7.1) we should have: \\[\\begin{equation} y_j = \\beta_0 + \\beta_1 x_{1,j} + \\beta_2 x_{2,j} + \\dots + \\beta_{k-1} x_{k-1,j} + \\epsilon_j , \\tag{8.1} \\end{equation}\\] where \\(\\beta_i\\) is a \\(i\\)-th parameter for the respective \\(i\\)-th explanatory variable and there is \\(k-1\\) of them in the model, meaning that when we want to estimate this model, we will have \\(k\\) unknown parameters. The regression line of this model in population (aka expectation conditional on the values of explanatory variables) is: \\[\\begin{equation} \\mu_{y,j} = \\mathrm{E}(y_j | \\mathbf{x}_j) = \\beta_0 + \\beta_1 x_{1,j} + \\beta_2 x_{2,j} + \\dots + \\beta_{k-1} x_{k-1,j} , \\tag{8.2} \\end{equation}\\] while in case of a sample estimation of the model we will use: \\[\\begin{equation} \\hat{y}_j = b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \\dots + b_{k-1} x_{k-1,j} . \\tag{8.3} \\end{equation}\\] While the simple linear regression can be represented as a line on the plane with an explanatory variable and a response variable, the multiple linear regression cannot be easily represented in the same way. In case of two explanatory variables the plot becomes three dimensional and the regression line transforms into regression plane. Figure 8.1: 3D scatterplot of Mileage vs Weight of a car and its Engine Horsepower. Figure 8.1 demonstrates a three dimensional scatterplot with the regression plane, going through the points, similar to how the regression line went through the two dimensional scatterplot 7.1. These sorts of plots are already difficult to read, but the situation becomes even more challenging, when more than two explanatory variables are under consideration: plotting 4D, 5D etc is not a trivial task. Still, what can be said about the parameters of the model even if we cannot plot it in the same way, is that they represent slopes for each variable, in a similar manner as \\(\\beta_1\\) did in the simple linear regression. "],["ols-estimation.html", "8.1 OLS estimation", " 8.1 OLS estimation In order to show how the estimation of multiple linear regression is done, we need to present it in a more compact form. In order to do that we will introduce the following vectors: \\[\\begin{equation} \\mathbf{x}&#39;_j = \\begin{pmatrix}1 &amp; x_{1,j} &amp; \\dots &amp; x_{k-1,j} \\end{pmatrix}, \\boldsymbol{\\beta} = \\begin{pmatrix}\\beta_0 \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k-1} \\end{pmatrix} , \\tag{8.4} \\end{equation}\\] where \\(&#39;\\) symbol is the transposition. This can then be substituted in (8.1) to get: \\[\\begin{equation} y_j = \\mathbf{x}&#39;_j \\boldsymbol{\\beta} + \\epsilon_j . \\tag{8.5} \\end{equation}\\] But this is not over yet, we can make it even more compact, if we pack all those values with index \\(t\\) in vectors and matrices: \\[\\begin{equation} \\mathbf{X} = \\begin{pmatrix} \\mathbf{x}&#39;_1 \\\\ \\mathbf{x}&#39;_2 \\\\ \\vdots \\\\ \\mathbf{x}&#39;_n \\end{pmatrix}, \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} , \\tag{8.6} \\end{equation}\\] where \\(T\\) is the sample size. This leads to the following compact form of multiple linear regression: \\[\\begin{equation} \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} . \\tag{8.7} \\end{equation}\\] Now that we have this compact form of multiple linear regression, we can estimate it using linear algebra. Many statistical textbooks explain how the following result is obtained (this involves taking derivative of SSE (7.4) with respect to \\(\\boldsymbol{\\beta}\\) and equating it to zero): \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\mathbf{b} = \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1} \\mathbf{X}&#39; \\mathbf{y} . \\tag{8.8} \\end{equation}\\] The formula (8.8) is used in all the statistical software, including lm() function from stats package for R. Here is an example with the same mtcars dataset: mtcarsModel01 &lt;- lm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars) The simplest plot that we can produce from this model is fitted values vs actuals, plotting \\(\\hat{y}_j\\) on x-axis and \\(y_j\\) on the y-axis: plot(fitted(mtcarsModel01),actuals(mtcarsModel01)) The same plot is produced via plot() method if we use alm() function from greybox instead: mtcarsModel02 &lt;- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb, mtcars, loss=&quot;MSE&quot;) plot(mtcarsModel02,1) Figure 8.2: Actuals vs fitted values for multiple linear regression model on mtcars data. We use loss=\"MSE\" in this case, to make sure that the model is estimated via OLS. We will discuss the default estimation method in alm(), likelihood, in Section 13. The plot on Figure 8.2 can be used for diagnostic purposes and in ideal situation the red line (LOWESS line) should coincide with the grey one, which would mean that we have correctly capture the tendencies in the data, so that all the regression assumptions are satisfied (see Section 12). "],["linearRegressionMultipleQualityOfFit.html", "8.2 Quality of the fit", " 8.2 Quality of the fit Building upon the discussion of the quality of the fit in Section 7.2, we can introduce a measure, based on the OLS criterion, (7.4), which is called either “Root Mean Squared Error” (RMSE) or a “standard error” or a “standard deviation of error” of the regression: \\[\\begin{equation} \\mathrm{RMSE} = \\sqrt{\\frac{1}{n-k} \\sum_{j=1}^n e_j^2 }. \\tag{8.9} \\end{equation}\\] Note that it is divided by the number of degrees of freedom in the model, \\(n-k\\), not on the number of observations, so technically speaking this is not a “mean” any more. This is needed to correct the in-sample bias of the measure. RMSE does not tell us about the in-sample performance but can be used to compare several models with the same response variable between each other: the lower RMSE is, the better the model fits the data. Note that this measure is not aware of the randomness in the true model and thus will be equal to zero in a model that fits the data perfectly (thus ignoring the existence of error term). This is a potential issue, as we might end up with a poor model that would seem like the best one. Here is how this can be calculated for our model, estimated using alm() function: sigma(mtcarsModel02) ## [1] 2.622011 The value of RMSE does not provide any important insights on its own, but it can be compared to the RMSE of another model to decide, which one of the two fits the data better. Similarly to the simple linear regression, we can calculate the R\\(^2\\). The problem is that the value of coefficient of determination would always increase with the increase of number of variables included in the model. This is because every variable will explain some proportion of the data due to randomness. So, if we add redundant variables, the fit will improve, but the quality of model will deteriorate. Here is an example: n &lt;- nobs(mtcarsModel02) mtcarsData$noise &lt;- rnorm(n,0,10) mtcarsModel02WithNoise &lt;- alm(mpg~cyl+disp+hp++drat+wt+qsec+gear+carb+noise, mtcarsData, loss=&quot;MSE&quot;) And here is the value of determination coefficient of the new model: 1 - sum(resid(mtcarsModel02WithNoise)^2) / (var(actuals(mtcarsModel02WithNoise))*(n-1)) ## [1] 0.8626051 and the previous one: 1 - sum(resid(mtcarsModel02)^2) / (var(actuals(mtcarsModel02))*(n-1)) ## [1] 0.8595764 The value in the new model will always be higher than in the previous one (or equal to it in some very special cases), no matter how we generate the random fluctuations. This means that some sort of penalisation of the number of variables in the model is required in order to make the measure more reasonable. This is what adjusted coefficient of determination does: \\[\\begin{equation} R^2_{adj} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{V}(y)} = 1 - \\frac{(n-1)\\mathrm{SSE}}{(n-k)\\mathrm{SST}}, \\tag{8.10} \\end{equation}\\] where MSE is the Mean Squared Error (square of RMSE (8.9)). So, instead of dividing sums of squares, in the adjusted R\\(^2\\) we divide the entities that are based on degrees of freedom. Given the presence of \\(k\\) in the formula (8.10), the coefficient will not necessarily increase with the addition of variables - when the variable does not contribute in the reduction of SSE of model substantially, R\\(^2\\) will not go up. Furthermore, if one model has higher RMSE than the other one, then the R\\(^2_{adj}\\) of that model will be lower, which becomes apparent, given that we have \\(-MSE\\) in the formula (8.10). Here how the adjusted R\\(^2\\) can be calculated for a model in R: setNames(c(1 - sigma(mtcarsModel02)^2 / var(actuals(mtcarsModel02)), 1 - sigma(mtcarsModel02WithNoise)^2 / var(actuals(mtcarsModel02WithNoise))), c(&quot;R^2-adj&quot;,&quot;R^2-adj, Noise&quot;)) ## R^2-adj R^2-adj, Noise ## 0.8107333 0.8063981 What we will typically see in the output above is that the model with the noise will have a lower value of adjusted R\\(^2\\) than the model without it. However, given that we deal with randomness, if you reproduce this example many times, you will see different situation, including those, where introducing noise still increases the value of the parameter. So, you should not fully trust R\\(^2_{adj}\\) either. When constructing a model or deciding what to include in it, you should always use your judgement - make sure that the variables included in the model are meaningful. Otherwise you can easily overfit the data, which would lead to inaccurate forecasts and inefficient estimates of parameters (see Section 12 for details). "],["interpretation-of-parameters.html", "8.3 Interpretation of parameters", " 8.3 Interpretation of parameters Finally, we come to the discussion of parameters of a model. As mentioned earlier, each one of them represents the slope of the model. But there is more to the meaning of parameters of the model. Consider the coefficients of the previously estimated model: coef(mtcarsModel02) ## (Intercept) cyl disp hp drat wt ## 17.88963741 -0.41459575 0.01293240 -0.02084886 1.10109551 -3.92064847 ## qsec gear carb ## 0.54145693 1.23321026 -0.25509911 Each of the parameters of this model shows an average effect of each variable on the mileage. They have a simple interpretation and show how the response variable will change on average with the increase of a variable by 1 unit, keeping all the other variables constant. For example, the parameter for wt (weight) shows that with the increase of weight of a car by 1000 pounds, the mileage would decrease on average by 3.921 miles per gallon, if all the other variables do not change. I have made the word “average” boldface three times in this paragraph for a reason. This is a very important point to keep in mind - the parameters will not tell you how variable will change for any specific observation. They do not show how it will change for each point. The regression model capture average tendencies and thus the word “average” is very important in the interpretation. In each specific case, the increase of weight by 1 will lead to different decreases (and even increases in some cases). But if we take the arithmetic mean of those individual effects, it will be close to the value of the parameter in the model. This however is only possible if all the assumptions of regression hold (see Section 12). "],["uncertaintyParameters.html", "Chapter 9 Uncertainty in regression", " Chapter 9 Uncertainty in regression Coming back to the example of mileage vs weight of cars, the estimated simple linear regression on the data was mpg=37.29-5.34wt+et. But what would happen if we estimate the same model on a different sample of data (e.g. 15 first observations instead of 32)? Figure 9.1: Weight vs mileage and two regression lines. Figure 9.1 shows the two lines: the red one corresponds to the larger sample, while the blue one corresponds to the small one. We can see that these lines have different intercepts and slope parameters. So, which one of them is correct? An amateur analyst would say that the one that has more observations is the correct model. But a more experienced statistician would tell you that none of the two is correct. They are both estimated on a sample of data and they both inevitably inherit the uncertainty of the data, making them both incorrect if we compare them to the hypothetical true model. This means that whatever regression model we estimate on a sample of data, it will be incorrect as well. This uncertainty about the regression line actually comes to the uncertainty of estimates of parameters of the model. In order to see it more clearly, consider the example with Speed and Stopping Distances of Cars dataset from datasets package (?cars): Figure 9.2: Speed vs stopping distance of cars While the linear relation between these variables might be not the the most appropriate, it suffices for demonstration purposes. What we will do for this example is fit the model and then use a simple bootstrap technique to get estimates of parameters of the model. We will do that using coefbootstrap() method from greybox package. The bootstrap technique implemented in the function applies the same model to subsamples of the original data and returns a matrix with parameters. This way we get an idea about the empirical uncertainty of parameters: slmSpeedDistanceBootstrap &lt;- coefbootstrap(slmSpeedDistance) Based on that we can plot the histograms of the estimates of parameters. par(mfcol=c(1,2)) hist(slmSpeedDistanceBootstrap$coefficients[,1], xlab=&quot;Intercept&quot;, main=&quot;&quot;) hist(slmSpeedDistanceBootstrap$coefficients[,2], xlab=&quot;Slope&quot;, main=&quot;&quot;) Figure 9.3: Distribution of bootstrapped parameters of a regression model Figure 9.3 shows the uncertainty around the estimates of parameters. These distributions look similar to the normal distribution. In fact, if we repeated this example thousands of times, the distribution of estimates of parameters would indeed follow the normal one due to CLT (if the assumptions hold, see Sections 4.2 and 12). As a result, when we work with regression we should take this uncertainty about the parameters into account. This applies to both parameters analysis and forecasting. "],["confidence-intervals.html", "9.1 Confidence intervals", " 9.1 Confidence intervals In order to take this uncertainty into account, we could construct confidence intervals for the estimates of parameters, using the principles discussed in Section 5.1. This way we would hopefully have some idea about the uncertainty of the parameters, and not just rely on average values. If we assume that CLT holds, we could use the t statistics for the calculation of the quantiles of distribution (we need to use t because we do not know the variance of estimates of parameters). But in order to do that, we need to have variances of estimates of parameters. One of possible ways of getting them would be the bootstrap used in the example above. However, this is a computationally expensive operation, and there is a more efficient procedure, which however only works with linear regression models either estimated using OLS or via Maximum Likelihood Estimation assuming Normal distribution (see Section 13). In these conditions the covariance matrix of parameters can be calculated using the following formula: \\[\\begin{equation} \\mathrm{V}(\\hat{\\boldsymbol{\\beta}}) = \\frac{1}{n-k} \\sum_{j=1}^n e_j^2 \\times \\left(\\mathbf{X}&#39; \\mathbf{X}\\right)^{-1}. \\tag{9.1} \\end{equation}\\] This matrix will contain variances of parameters on the diagonal and covariances between the parameters on off-diagonals. In this specific case, we only need the diagonal elements. We can take square root of them to obtain standard errors of parameters, which can then be used to construct confidence intervals for each parameter \\(i\\) via: \\[\\begin{equation} \\beta_i \\in (b_i + t_{\\alpha/2}(n-k) s_{b_i}, b_i + t_{1-\\alpha/2}(n-k) s_{b_i}), \\tag{9.1} \\end{equation}\\] where \\(s_{b_i}\\) is the standard error of the parameter \\(b_i\\). All modern software does all these calculations automatically, so we do not need to do them manually. Here is an example: vcov(slmSpeedDistance) ## (Intercept) speed ## (Intercept) 45.676514 -2.6588234 ## speed -2.658823 0.1726509 This is the covariance matrix of parameters, the diagonal elements of which are then used in the confint() method: confint(slmSpeedDistance) ## S.E. 2.5% 97.5% ## (Intercept) 6.7584402 -31.167850 -3.990340 ## speed 0.4155128 3.096964 4.767853 The confidence interval for speed above shows, for example, that if we repeat the construction of interval many times, the true value of parameter speed will lie in 95% of cases between 3.08 and 4.78. This gives an idea about the real effect in the population. We can also present all of this in the following summary (this is based on the alm() model, the other functions will produce different summaries): summary(slmSpeedDistance) ## Response variable: dist ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) -17.5791 6.7584 -31.1678 -3.9903 * ## speed 3.9324 0.4155 3.0970 4.7679 * ## ## Error standard deviation: 15.3796 ## Sample size: 50 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 48 ## Information criteria: ## AIC AICc BIC BICc ## 417.1569 417.4122 420.9809 421.4803 This summary provide all the necessary information about the estimates of parameters: their mean values in the column “Estimate,” their standard errors in “Std. Error,” the bounds of confidence interval and finally a star if the interval does not contain zero. This typically indicates that we are certain on the selected confidence level (95% in our example) about the sign of the parameter and that the effect really exists. "],["hypothesis-testing.html", "9.2 Hypothesis testing", " 9.2 Hypothesis testing Another way to look at the uncertainty of parameters is to test a statistical hypothesis. As it was discussed in Section 5.3, I personally think that hypothesis testing is a less useful instrument for these purposes than the confidence interval and that it might be misleading in some circumstances. Nonetheless, it has its merits and can be helpful if an analyst knows what they are doing. In order to test the hypothesis, we need to follow the procedure, described in Section 5.3. 9.2.1 Regression parameters The classical hypotheses for the parameters are formulated in the following way: \\[\\begin{equation} \\begin{aligned} \\mathrm{H}_0: \\beta_i = 0 \\\\ \\mathrm{H}_1: \\beta_i \\neq 0 \\end{aligned} . \\tag{9.2} \\end{equation}\\] This formulation of hypotheses comes from the idea that we want to check if the effect estimated by the regression is indeed there (i.e. statistically significantly different from zero). Note however, that as in any other hypothesis testing, if you fail to reject the null hypothesis, this only means that you do not know, we do not have enough evidence to conclude anything. This does not mean that there is no effect and that the respective variable can be removed from the model. In case of simple linear regression, the null and alternative hypothesis can be represented graphically as shown in Figure 9.4. Figure 9.4: Graphical presentation of null and alternative hypothesis in regression context The graph on the left in Figure 9.4 demonstrates how the true model could look if the null hypothesis was true - it would be just a straight line, parallel to x-axis. The graph on the right demonstrates the alternative situation, when the parameter is not equal to zero. We do not know the true model, and hypothesis testing does not tell us, whether the hypothesis is true or false, but if we have enough evidence to reject H\\(_0\\), then we might conclude that we see an effect of one variable on another in the data. Note, as discussed in Section 5.3, the null hypothesis is always wrong, and it will inevitably be rejected with the increase of sample size. Given the discussion in the previous subsection, we know that the parameters of regression model will follow normal distribution, as long as all assumptions are satisfied (including those for CLT). We also know that because the standard errors of parameters are estimated, we need to use Student’s distribution, which takes the uncertainty about the variance into account. Based on this, we can say that the following statistics will follow t with \\(n-k\\) degrees of freedom: \\[\\begin{equation} \\frac{b_i - 0}{s_{b_i}} \\sim t(n-k) . \\tag{9.3} \\end{equation}\\] After calculating the value and comparing it with the critical t-value on the selected significance level or directly comparing p-value based on (9.3) with the significance level, we can make conclusions about the hypothesis. The context of regression provides a great example, why we never accept hypothesis and why in the case of “Fail to reject H\\(_0\\),” we should not remove a variable (unless we have more fundamental reasons for doing that). Consider an example, where the estimated parameter \\(b_1=0.5\\), and its standard error is \\(s_{b_1}=1\\), we estimated a simple linear regression on a sample of 30 observations, and we want to test, whether the parameter in the population is zero (i.e. hypothesis (9.2)) on 1% significance level. Inserting the values in formula (9.3), we get: \\[\\begin{equation*} \\frac{|0.5 - 0|}{1} = 0.5, \\end{equation*}\\] with the critical value for two-tailed test of \\(t_{0.01}(30-2)\\approx 2.76\\). Comparing t-value with the critical one, we would conclude that we fail to reject H\\(_0\\) and thus the parameter is not statistically different from zero. But what would happen if we check another hypothesis: \\[\\begin{equation*} \\begin{aligned} \\mathrm{H}_0: \\beta_1 = 1 \\\\ \\mathrm{H}_1: \\beta_1 \\neq 1 \\end{aligned} . \\end{equation*}\\] The procedure is the same, the calculated t-value is: \\[\\begin{equation*} \\frac{|0.5 - 1|}{1} = 0.5, \\end{equation*}\\] which leads to exactly the same conclusion as before: on 1% significance level, we fail to reject the new H\\(_0\\), so the value is not distinguishable from 1. So, which of the two is correct? The correct answer is “we do not know.” The non-rejection region just tells us that uncertainty about the parameter is so high that it also include the value of interest (0 in case of the classical regression analysis). If we constructed the confidence interval for this problem, we would not have such confusion, as we would conclude that on 1% significance level the true parameter lies in the region \\((-2.26, 3.26)\\) and can be any of these numbers. In R, if you want to test the hypothesis for parameters, I would recommend using lm() function for regression: lmSpeedDistance &lt;- lm(dist~speed,cars) summary(lmSpeedDistance) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 This output tells us that when we consider the parameter for the variable speed, we reject the standard H\\(_0\\) on the pre-selected 1% significance level (comparing the level with p-value in the last column of the output). Note that we should first select the significance level and only then conduct the test, otherwise we would be bending reality for our needs. 9.2.2 Regression line Finally, in regression context, we can test another hypothesis, which becomes useful, when a lot of parameters of the model are very close to zero and seem to be insignificant on the selected level: \\[\\begin{equation} \\begin{aligned} \\mathrm{H}_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{k-1} = 0 \\\\ \\mathrm{H}_1: \\beta_1 \\neq 0 \\vee \\beta_2 \\neq 0 \\vee \\dots \\vee \\beta_{k-1} \\neq 0 \\end{aligned} , \\tag{9.4} \\end{equation}\\] which translates into normal language as “H\\(_0\\): all parameters (except for intercept) are equal to zero; H\\(_1\\): at least one parameter is not equal to zero.” This hypothesis is only needed, when you have a model with many statistically insignificant variables and want to see if the model explains anything. This is done using F-test, which can be calculated based on sums of squares: \\[\\begin{equation*} F = \\frac{ SSR / (k-1)}{SSE / (n-k)} \\sim F(k-1, n-k) , \\end{equation*}\\] where the sums of squares are divided by their degrees of freedom. The test is conducted in the similar manner as any other test (see Section 5.3): after choosing the significance level, we can either calculate the critical value of F for the specified degrees of freedom, or compare it with the p-value from the test to make a conclusion about the null hypothesis. This hypothesis is not very useful, when the parameter are significant and coefficient of determination is high. It only becomes useful in difficult situations of poor fit. The test on its own does not tell if the model is adequate or not. And the F value and related p-value is not comparable with respective values of other models. Graphically, this test checks, whether in the true model the slope of the straight line on the plot of actuals vs fitted is different from zero. An example with the same stopping distance model is provided in Figure 9.5. Figure 9.5: Graphical presentation of F test for regression model. What the test is tries to get insight about, is whether in the true model the blue line coincides with the red line (i.e. the slope is equal to zero, which is only possible, when all parameters are zero). If we have enough evidence to reject the null hypothesis, then this means that the slopes are different on the selected significance level. Here is an example with the speed model discussed above with the significance level of 1%: lmSpeedDistanceF &lt;- summary(lmSpeedDistance)$fstatistic # F value lmSpeedDistanceF[1] ## value ## 89.56711 # F critical qf(0.99,lmSpeedDistanceF[2],lmSpeedDistanceF[3]) ## [1] 7.194218 # p-value from the test 1-pf(lmSpeedDistanceF[1],lmSpeedDistanceF[2],lmSpeedDistanceF[3]) ## value ## 1.489919e-12 In the output above, the critical value is lower than the calculated, so we can reject the H\\(_0\\), which means that there is something in the model that explains the variability in the variable dist. Alternatively, we could focus on p-value. We see that the it is lower than the significance level of 1%, so we reject the H\\(_0\\) and come to the same conclusion as above. "],["regression-line-uncertainty.html", "9.3 Regression line uncertainty", " 9.3 Regression line uncertainty Given the uncertainty of estimates of parameters, the regression line itself and the points around it will be uncertain. This means that in some cases we should not just consider the predicted values of the regression \\(\\hat{y}_j\\), but also the uncertainty around them. The uncertainty of the regression line builds upon the uncertainty of parameters and can be measured via the conditional variance in the following way: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_j| \\mathbf{x}_j) = \\mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \\dots + b_{k-1} x_{k-1,j}) , \\tag{9.5} \\end{equation}\\] which after some simplifications leads to: \\[\\begin{equation} \\mathrm{V}(\\hat{y}_j| \\mathbf{x}_j) = \\sum_{l=0}^{k-1} \\mathrm{V}(b_j) x^2_{l,j} + 2 \\sum_{l=1}^{k-1} \\sum_{i=0}^{l-1} \\mathrm{cov}(b_i,b_l) x_{i,j} x_{l,j} , \\tag{9.6} \\end{equation}\\] where \\(x_{0,j}=1\\). As we see, the variance of the regression line involves variances and covariances of parameters. This variance can then be used in the construction of the confidence interval for the regression line. Given that each estimate of parameter \\(b_i\\) will follow normal distribution with a fixed mean and variance due to CLT, the predicted value \\(\\hat{y}_j\\) will follow normal distribution as well. This can be used in the construction of the confidence interval, in a manner similar to the one discussed in Section 5.1: \\[\\begin{equation} \\mu_j \\in (\\hat{y}_j + t_{\\alpha/2}(n-k) s_{\\hat{y}_j}, \\hat{y}_j + t_{1-\\alpha/2}(n-k) s_{\\hat{y}_j}), \\tag{9.7} \\end{equation}\\] where \\(s_{\\hat{y}_j}=\\sqrt{\\mathrm{V}(\\hat{y}_j| \\mathbf{x}_j)}\\). In R, this interval can be constructed via the function predict() with interval=\"confidence\". It is based on the covariance matrix of parameters, extracted via vcov() method in R (it was discussed in a previous subsection). Note that the interval can be produced not only for the in-sample value, but for the holdout as well. Here is an example with alm() function: slmSpeedDistanceCI &lt;- predict(slmSpeedDistance,interval=&quot;confidence&quot;) plot(slmSpeedDistanceCI, main=&quot;&quot;) Figure 9.6: Fitted values and confidence interval for the stopping distance model. The same fitted values and interval can be presented differently on the actuals vs fitted plot: plot(fitted(slmSpeedDistance),actuals(slmSpeedDistance), xlab=&quot;Fitted&quot;,ylab=&quot;Actuals&quot;) abline(a=0,b=1,col=&quot;blue&quot;,lwd=2) lines(sort(fitted(slmSpeedDistance)), slmSpeedDistanceCI$lower[order(fitted(slmSpeedDistance))], col=&quot;red&quot;) lines(sort(fitted(slmSpeedDistance)), slmSpeedDistanceCI$upper[order(fitted(slmSpeedDistance))], col=&quot;red&quot;) Figure 9.7: Actuals vs Fitted and confidence interval for the stopping distance model. Figure 9.7 demonstrates the actuals vs fitted plot, together with the 95% confidence interval around the line, demonstrating where the line would be expected to be in 95% of the cases if we re-estimate the model many times. We also see that the uncertainty of the regression line is lower in the middle of the data, but expands in the tails. Conceptually, this happens because the regression line, estimated via OLS, always passes through the average point of the data \\((\\bar{x},\\bar{y})\\) and the variability in this point is lower than the variability in the tails. If we are not interested in the uncertainty of the regression line, but rather in the uncertainty of the observations, we can refer to prediction interval. The variance in this case is: \\[\\begin{equation} \\mathrm{V}(y_j| \\mathbf{x}_j) = \\mathrm{V}(b_0 + b_1 x_{1,j} + b_2 x_{2,j} + \\dots + b_{k-1} x_{k-1,j} + e_j) , \\tag{9.8} \\end{equation}\\] which can be simplified to (if assumptions of regression model hold, see Section 12): \\[\\begin{equation} \\mathrm{V}(y_j| \\mathbf{x}_j) = \\mathrm{V}(\\hat{y}_j| \\mathbf{x}_j) + \\hat{\\sigma}^2, \\tag{9.9} \\end{equation}\\] where \\(\\hat{\\sigma}^2\\) is the variance of the residuals \\(e_j\\). As we see from the formula (9.9), the variance in this case is larger than (9.6), which will result in wider interval than the confidence one. We can use normal distribution for the construction of the interval in this case (using formula similar to (9.7)), as long as we can assume that \\(\\epsilon_j \\sim \\mathcal{N}(0,\\sigma^2)\\). In R, this can be done via the very same predict() function with interval=\"prediction\": slmSpeedDistancePI &lt;- predict(slmSpeedDistance,interval=&quot;prediction&quot;) Based on this, we can construct graphs similar to 9.6 and 9.7. Figure 9.8: Fitted values and prediction interval for the stopping distance model. Figure 9.8 shows the prediction interval for values over observations and for actuals vs fitted. As we see, the interval is wider in this case, covering only 95% of observations (there are 2 observations outside it). In forecasting, prediction interval has a bigger importance than the confidence interval. This is because we are typically interested in capturing the uncertainty about the observations, not about the estimate of a line. Typically, the prediction interval would be constructed for some holdout data, which we did not have at the model estimation phase. In the example with stopping distance, we could see what would happen if the speed of a car was, for example, 30mph: slmSpeedDistanceForecast &lt;- predict(slmSpeedDistance,newdata=data.frame(speed=30), interval=&quot;prediction&quot;) plot(slmSpeedDistanceForecast) Figure 9.9: Forecast of the stopping distance for the speed of 30mph. Figure 9.9 shows the point forecast (the expected stopping distance if the speed of car was 30mph) and the 95% prediction interval (we expect that in 95% of the cases, the cars will have the stopping distance between 66.865 and 133.921 feet. "],["dummyVariables.html", "Chapter 10 Regression with categorical variables", " Chapter 10 Regression with categorical variables So far we assumed that the explanatory variables in the model are numerical. But is it possible to include somehow in regression model variables in categorical scales, for example, colour and size of t-shirts? Yes, it is. This is done using so called “dummy variables.” "],["dummy-variables-for-the-intercept.html", "10.1 Dummy variables for the intercept", " 10.1 Dummy variables for the intercept As we remember from Section 1.2, the variables in categorical scale do not have distance or natural zero. This means that if we encode the values in numbers (e.g. “red” - “1,” “green” - “2,” “blue” - “3”), then these numbers will not have any proper mathematical meaning - they will only represent specific values (and order in case of ordinal scale), but we would be limited in operations with these values. In order to overcome this limitation, we could create a set of dummy variables, each of which would be equal to one if the value of the original variable is equal to a specific value and zero otherwise. Consider the example with colours, where we have three types of t-shirts to sell: Red, Green, Blue. Every t-shirt in our dataset would have one of these colours, and based on this we could create three dummy variables: colourRed, which would be equal to one if the t-shirt is Red and zero otherwise, colourGreen: 1 if the t-shirt is Green and 0 otherwise, colourBlue: 1 if the t-shirt is Blue and 0 otherwise. These dummy variables can then be added to a model instead of the original variable colour, resulting, for example, in the model: \\[\\begin{equation} sales_j = \\beta_0 + \\beta_1 price_j + \\beta_2 colourRed_j + \\beta_3 colourGreen_j + \\epsilon_j . \\tag{10.1} \\end{equation}\\] Notice that I have only included two dummy variables out of the three. This is because we do not need to have all of them to be able to say what colour of t-shirt we have: if it is not Red and not Green, then it must be Blue. Furthermore, while some models and estimation methods could handle all the dummy variables in the model, the linear regression cannot be estimated via the conventional methods if they are all in. This is exactly because of this situation with “not Red, not Green.” If we introduce all three, the model will have so called “dummy variables trap,” implying perfect multicollinearity (see Subsection 12.3), because of the functional relation between variables: \\[\\begin{equation} colourBlue_j = 1 - colourRed_j - colourGreen_j \\text{ for all } j=1, \\dots, n . \\tag{10.2} \\end{equation}\\] This is a general rule: if you have created a set of dummy variables from a categorical one, then one of them needs to be dropped, in order not to have the dummy variables trap. So, what does the inclusion of dummy variables in the regression model means? We can see that on the following example of artificial data: tShirts &lt;- cbind(rnorm(150,20,2),0,0,0) tShirts[1:50,2] &lt;- 1 tShirts[1:50+50,3] &lt;- 1 tShirts[1:50+50*2,4] &lt;- 1 tShirts &lt;- cbind(1000 + tShirts %*% c(-2.5, 30, -20, 50) + rnorm(150,0,5), tShirts) colnames(tShirts) &lt;- c(&quot;sales&quot;,&quot;price&quot;,&quot;colourRed&quot;,&quot;colourGreen&quot;,&quot;colourBlue&quot;) We can produce spread plot to see how the data looks like: spread(tShirts) Figure 10.1: Spread plot of t-shirts data. Figure @red(fig:tShirtsSpread) demonstrates that the sales differ depending on the type of colour (the boxplots). The scatterplot between sales and price is not very clear, but there are actually three theoretical lines on that plot. We can enlarge the plot and draw them: plot(tShirts[,2:1]) abline(a=1000+30, b=-2.5, col=&quot;red&quot;) abline(a=1000-20, b=-2.5, col=&quot;green&quot;) abline(a=1000+50, b=-2.5, col=&quot;blue&quot;) Figure 10.2: Scatterplot of Sales vs Price of t-shirts of different colour. Now, if we want to construct the regression that would take these differences into account, we need to estimate the model (10.1): tShirtsALM &lt;- alm(sales~price+colourRed+colourGreen, tShirts, loss=&quot;MSE&quot;) summary(tShirtsALM) ## Response variable: sales ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) 1044.0344 4.5187 1035.1038 1052.9649 * ## price -2.2765 0.2209 -2.7131 -1.8399 * ## colourRed -18.3371 0.9574 -20.2292 -16.4449 * ## colourGreen -67.5644 0.9527 -69.4472 -65.6816 * ## ## Error standard deviation: 4.7627 ## Sample size: 150 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 146 ## Information criteria: ## AIC AICc BIC BICc ## 897.8702 898.1460 909.9127 910.6038 Notice that the intercept in this model is not 1000, as we used in the generation of the data, but is 1044. This is because it now also contains the effect of blue colour on sales in it. So, the sales of blue coloured t-shirt is now the baseline category, and each dummy variable now represents the shifts of sales, when we switch from one colour to another. For example, we can say that the sales of red colour t-shirt are on average lower than the sales of the blue one by approximately 18 units. What dummy variables do in the model is just shift the line from one level to another. This becomes clear if we consider special cases of models for the three t-shirts: For the blue t-shirt, our model is: sales=1044.03-2.28price+et. This is because both colourRed and colourGreen are zero in this case; For the red t-shirt the model is: sales=1044.03+-18.34-2.28price+et or sales=1025.69-2.28price+et; Finally, for the green one, the model is: sales=1044.03+-67.56-2.28price+et or sales=976.47-2.28price+et. In a way, we could have constructed three different regression models for the sub-samples of data, and in the ideal situation (all the data in the world) we would get the same set of estimates of parameters. However, this would be a costly procedure from the statistical perspective, because three separate models will have lower number of degrees of freedom, then the model with dummy variables. Thus, the estimates of parameters will be more uncertain in those three models than in one model tShirtsALM. One thing that we can remark is that the estimated parameters differ from the ones we used in the data generation. This is because the intercepts of the three models above intersect the y-axis in the points 1044.03, 1025.69 and 976.47 respectively. Furthermore, in general it is not possible to extract the specific effect of blue colour on sales based on the estimates of parameters, unless we impose some restrictions on parameters. The closest we can get to the true parameters is if we normalise them (assuming that there is some baseline and that the colours build upon it and add up to zero): colourParameters &lt;- c(coef(tShirtsALM)[3:4]+coef(tShirtsALM)[1],coef(tShirtsALM)[1]) names(colourParameters)[3] &lt;- &quot;colourBlue&quot;; colourParameters - mean(colourParameters) ## colourRed colourGreen colourBlue ## 10.29675 -38.93057 28.63382 The meaning of these effects is that on average they change the baseline sales of colourless t-shirts according to these values. For example, the specific increase of sales due to the red colour of t-shirt is 10 units. In general, it is not worth bothering with these specific effects, and we can just stick with parameters of model, keeping in mind that we only have effects comparative to the selected baseline category. In R, we can also work with factor variables, without a need to expand variables in a set of dummies - the program will do the expansion automatically and drop the first level of the variable. In order to see how it works, we create a data frame with the factor variable colour: tShirtsDataFrame &lt;- as.data.frame(tShirts[,1:2]) tShirtsDataFrame$colour &lt;- factor(c(&quot;Red&quot;,&quot;Green&quot;,&quot;Blue&quot;)[tShirts[,3:5] %*% c(1:3)]) spread(tShirtsDataFrame) Notice that the “Blue” was automatically set as the first level, because factor() function would sort labels alphabetically unless the levels are provided explicitly. The estimated model in this case will be exactly the same as the tShirts model above: tShirtsDataFrameALM &lt;- alm(sales~price+colour, tShirtsDataFrame, loss=&quot;MSE&quot;) summary(tShirtsDataFrameALM) ## Response variable: sales ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) 1044.0344 4.5187 1035.1038 1052.9649 * ## price -2.2765 0.2209 -2.7131 -1.8399 * ## colourGreen -67.5644 0.9527 -69.4472 -65.6816 * ## colourRed -18.3371 0.9574 -20.2292 -16.4449 * ## ## Error standard deviation: 4.7627 ## Sample size: 150 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 146 ## Information criteria: ## AIC AICc BIC BICc ## 897.8702 898.1460 909.9127 910.6038 Finally, it is recommended in general not to drop dummy variables one by one, if for some reason you decide that some of them are not helping. If, for example, we decide not to include colourRed and only have the model with colourGreen, then the meaning of the dummy variables will change - we will not be able to distinguish the Blue from Red. Furthermore, while some dummy variables might not seem important (or significant) in regression, their combination might be improving the model, and dropping some of them might be damaging for the model in terms of its predictive power. So, it is more common either to include all levels (but one) of categorical variable or not to include any of them. "],["categorical-variables-for-the-slope.html", "10.2 Categorical variables for the slope", " 10.2 Categorical variables for the slope In reality, we can have more complicated situations, when the change of price would lead to different changes in sales for different types of t-shirts. In this case, we are talking about an interaction effect between price and colour. The following artificial example demonstrates the situation: tShirtsInteraction &lt;- cbind(rnorm(150,20,2),0,0,0) tShirtsInteraction[1:50,2] &lt;- tShirtsInteraction[1:50,1] tShirtsInteraction[1:50+50,3] &lt;- tShirtsInteraction[1:50+50,1] tShirtsInteraction[1:50+50*2,4] &lt;- tShirtsInteraction[1:50+50*2,1] tShirtsInteraction &lt;- cbind(1000 + tShirtsInteraction %*% c(-2.5, -1.5, -0.5, -4) + rnorm(150,0,5), tShirtsInteraction) colnames(tShirtsInteraction) &lt;- c(&quot;sales&quot;,&quot;price&quot;,&quot;price:colourRed&quot;, &quot;price:colourGreen&quot;,&quot;price:colourBlue&quot;) This artificial data can be plotted in the following way to show the effect: plot(tShirtsInteraction[,2:1]) abline(a=1000, b=-2.5-1.5, col=&quot;red&quot;) abline(a=1000, b=-2.5-0.5, col=&quot;green&quot;) abline(a=1000, b=-2.5-4, col=&quot;blue&quot;) Figure 10.3: Scatterplot of Sales vs Price of t-shirts of different colour, interaction effect. The plot on Figure 10.3 shows that there are three categories of data and that for each of it, the price effect will be different: the increase in price by one unit leads to the faster reduction of sales for the blue t-shirts than for the others. Compare this with Figure 10.2, where we had the difference only in intercepts. This implies a different model: \\[\\begin{equation} sales_j = \\beta_0 + \\beta_1 price_j + \\beta_2 price_j \\times colourRed_j + \\beta_3 price_j \\times colourGreen_j + \\epsilon_j . \\tag{10.3} \\end{equation}\\] Notice that we still include only two dummy variables out of three in order to avoid the dummy variables trap. What is new in this case is the multiplication of price by the dummy variables. This trick allows changing the slope of price, depending on the colour of t-shirt. For example, here what the model (10.3) would look like for the three types of colours: Red colour: \\(sales_j = \\beta_0 + \\beta_1 price_j + \\beta_2 price_j + \\epsilon_j\\) or \\(sales_j = \\beta_0 + (\\beta_1 + \\beta_2) price_j + \\epsilon_j\\); Green colour: \\(sales_j = \\beta_0 + \\beta_1 price_j + \\beta_3 price_j + \\epsilon_j\\) or \\(sales_j = \\beta_0 + (\\beta_1 + \\beta_3) price_j + \\epsilon_j\\); Blue colour: \\(sales_j = \\beta_0 + \\beta_1 price_j + \\epsilon_j\\). In R, the interaction effect can be introduced explicitly in the formula via : symbol if you have a proper factor variable: tShirtsInteractionDataFrame &lt;- as.data.frame(tShirtsInteraction[,1:2]) tShirtsInteractionDataFrame$colour &lt;- tShirtsDataFrame$colour # Fit the model tShirtsInteractionDataFrameALM &lt;- alm(sales~price+price:colour, tShirtsInteractionDataFrame, loss=&quot;MSE&quot;) summary(tShirtsInteractionDataFrameALM) ## Response variable: sales ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) 1005.1326 3.7625 997.6966 1012.5687 * ## price -6.7291 0.1920 -7.1085 -6.3497 * ## price:colourGreen 3.4613 0.0453 3.3718 3.5508 * ## price:colourRed 2.3886 0.0452 2.2992 2.4780 * ## ## Error standard deviation: 4.5096 ## Sample size: 150 ## Number of estimated parameters: 4 ## Number of degrees of freedom: 146 ## Information criteria: ## AIC AICc BIC BICc ## 881.4926 881.7685 893.5352 894.2263 Note that the interpretation of parameters in such model will be different, because now the price shows the baseline effect for the blue t-shirts, while the interaction effects show how this effect will change for other colours. So, for example, in order to see what would be the effect of price change on sales of red t-shirts, we need to sum up the parameter for price and price:colourRed. We then can say that if price of red t-shirt increases by £1, the sales will decrease on average by 4.34 units. "],["variablesTransformations.html", "Chapter 11 Variables transformations", " Chapter 11 Variables transformations So far we have discussed linear regression models, where the response variable linearly depends on a set of explanatory variables. These models work well in many contexts, especially when the response variable is measured in high volumes (e.g. sales in thousands of units). However, in reality the relations between variables can be non-linear. In this chapter we consider an example of application to see how transformations can be motivated by a real life example and then discuss different types of transformations and what they imply for analytics and forecasting "],["example-of-application.html", "11.1 Example of application", " 11.1 Example of application Consider, for example, the stopping distance vs speed of the car, the case we have discussed in the previous sections. This sort of relation in reality is non-linear. We know from physics that the distance travelled by car is proportional to the mass of car, the squared speed and inversely proportional to the breaking force: \\[\\begin{equation} distance \\propto \\frac{mass}{2 breaking} \\times speed^2. \\tag{11.1} \\end{equation}\\] If we use the linear function instead, then we might fail in capturing the relation correctly. Here is how the linear regression looks like, when applied to the data (Figure 11.1). Figure 11.1: Speed vs stopping distance and a linear model The model on the plot in Figure 11.1 is misleading, because it predicts that the stopping distance of a car, travelling with speed less than 4mph will be negative. Furthermore, the modelunderestimates the real stopping distance for cars with higher speed. If a decision is made based on this model, then it will be inevitably wrong and might potentially lead to serious repercussions in terms of road safety. Given the relation (11.1), we should consider a non-linear model. In this specific case, we should consider the model of the type: \\[\\begin{equation} distance = \\beta_0 speed^{\\beta_1} \\times (1+\\epsilon). \\tag{11.2} \\end{equation}\\] The multiplication of speed by the error term is necessary, because the effect of randomness will have an increasing variability with the increase of speed: if the speed is low, then the random factors (such as road conditions, breaks condition etc) will not have a strong effect on distance, while in case of the high speed these random factors might lead either to the serious decrease or increase of distance (a car on a slippery road, stopping from 50mph will have much longer distance than the same car on a dry road). Note that I have left the parameter \\(\\beta_1\\) in (eq:speedDistanceModel) and did not set it equal to two. This is done for the case we want to estimate the parameter based on the data. The problem with the model (11.2) is that it is difficult to estimate due to the non-linearity. In order to resolve this problem, we can linearise it by taking logarithms of both sides, which will lead to: \\[\\begin{equation} \\log (distance) = \\log \\beta_0 + \\beta_1 \\log (speed) + \\log(1+\\epsilon). \\tag{11.3} \\end{equation}\\] If we substituted every element with \\(\\log\\) in (11.3) by other names (e.g. \\(\\log(\\beta_0)=\\beta^\\prime_0\\) and \\(\\log(speed)=x\\)), it would be easier to see that this is a linear model, which can be estimated via OLS. This type of model is called “log-log,” reflecting that it has logarithms on both sides. Even the data will be much better behaved if we use logarithms in this situation (see Figure 11.2). Figure 11.2: Speed vs stopping distance in logarithms What we want to see on Figure 11.2 is the linear relation between the variables with points having fixed variance. However, in our case we can notice that the variance of the stopping distances does not seem to be stable: the variability around 2.0 is higher than the variability around 3.0. This might cause issues in the model due to violation of assumptions (see Section 12). For now, we acknowledge the issue but do not aim to fix it. And here how the model (11.3) can be estimated using R: slmSpeedDistanceModel01 &lt;- alm(log(dist)~log(speed), cars, loss=&quot;MSE&quot;) The values of parameters of this model will have a different meaning than the parameters of the linear model. Consider the example with the model above: summary(slmSpeedDistanceModel01) ## Response variable: logdist ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## (Intercept) -0.7297 0.3758 -1.4854 0.026 ## log(speed) 1.6024 0.1395 1.3218 1.883 * ## ## Error standard deviation: 0.4053 ## Sample size: 50 ## Number of estimated parameters: 2 ## Number of degrees of freedom: 48 ## Information criteria: ## AIC AICc BIC BICc ## 53.5318 53.7872 57.3559 57.8553 The value of parameter for the variable log(speed) now does not represent the marginal effect of speed on distance, but rather shows the elasticity, i.e. if the speed of a car increases by 1%, the travel distance will increase on average by 1.6%. In order to analyse the fit of the model on the original data, we would need to produce fitted values and exponentiate them. Note that in this case they would correspond to geometric rather than arithmetic means: plot(cars, xlab=&quot;Speed&quot;, ylab=&quot;Stopping distance&quot;) lines(cars$speed,exp(fitted(slmSpeedDistanceModel01)),col=&quot;red&quot;) Figure 11.3: Speed vs stopping distance and the log-log model fit. The resulting model in Figure 11.3 is the power function, which exhibits the increase in speed of change of one parameter with a linear change of another one. Note that technically speaking, the log-log model only makes sense, when the data is strictly positive. If it also contains zeroes (the speed is zero, thus the stopping distance is zero), then some other transformations might be in order. For example, we could square the speed in the model and try constructing the linear model, aligning it better with the physical model (11.1): \\[\\begin{equation} distance = \\beta_0 + \\beta_1 speed^2 + \\epsilon . \\tag{11.4} \\end{equation}\\] The issue of this model would be that the error term is additive and thus the model would assume that the variability of the error does not change with the speed, which is not realistic. Figure 11.4: Speed squared vs stopping distance. Figure 11.4 demonstrates the scatterplot for squared speed vs stopping distances. While we see that the relation between variables is closer to linear, the problem with variance is not resolved. If we want to estimate this model, we can use the following command in R: slmSpeedDistanceModel02 &lt;- alm(dist~I(speed^2), cars, loss=&quot;MSE&quot;) Note that we use I() in the formula to tell R to square the variable - it will not do the necessary transformation otherwise. Also note that in our specific case we did not include the non-transformed speed variable, because we know that the lowest distance should be, when speed is zero. But this might not be the case in other cases, so in general instead of the formula used above we should use: y~x+I(x^2). Furthermore, if we know for sure that the intercept is not needed (i.e. we know that the distance will be zero, when speed is zero), then we can remove it and estimate the model: slmSpeedDistanceModel03 &lt;- alm(dist~I(speed^2)-1, cars, loss=&quot;MSE&quot;) ## Warning: You have asked not to include intercept in the model. We will try to ## fit the model, but this is a very naughty thing to do, and we cannot guarantee ## that it will work... alm() function will complain about the exclusion of the intercept, but it should estimate the model nonetheless. The fit of the model to the data would be similar in its shape to the one from the log-log model (see Figure 11.5). Figure 11.5: Speed squared vs stopping distance with models with speed^2. The plot in Figure 11.5 demonstrates how the two models fit the data. The Model 2, as we see goes through the origin, which makes sense from the physical point of view. However, because of that it might fit the data worse than the Model 1 does. Still, it it better to have a more meaningful model than the one that potentially overfits the data. Another way to introduce the squares in the model is to take square root of distance. This would potentially align better with the physical model of stopping distance (11.1): \\[\\begin{equation} \\sqrt{distance} = \\beta_0 + \\beta_1 speed + \\epsilon , \\tag{11.5} \\end{equation}\\] which will be equivalent to: \\[\\begin{equation} distance = (\\beta_0 + \\beta_1 speed + \\epsilon)^2 . \\tag{11.6} \\end{equation}\\] The good news is, the error term in this model will change with the change of speed due to the interaction effect, cause by the square of the sum in (11.6). And, similar to the previous models, the parameter \\(\\beta_0\\) might not be needed. Graphically, this transformation is present on Figure 11.6. Figure 11.6: Speed vs square root of stopping distance. As the plot in Figure 11.6 demonstrates, the relation has become linear and the variance seems to be constant, no matter what the speed is. This means that the proposed model might be more appropriate to the data than the previous ones. This is how we can estimate this model: slmSpeedDistanceModel04 &lt;- alm(sqrt(dist)~speed, cars, loss=&quot;MSE&quot;) Similar to the Model 2 with squares, we will also consider the model without intercept on the grounds that if we capture the relation correctly, the zero speed should result in zero distance. slmSpeedDistanceModel05 &lt;- alm(sqrt(dist)~speed-1, cars, loss=&quot;MSE&quot;) ## Warning: You have asked not to include intercept in the model. We will try to ## fit the model, but this is a very naughty thing to do, and we cannot guarantee ## that it will work... Finally, we can see how both models will fit the original data (squaring the fitted values to get to the original scale): Figure 11.7: Speed squared vs stopping distance with Square Root models. Subjectively, I would say that Model 5 is the most appropriate from all the models under consideration: it corresponds to the physical model on one hand, and has constant variance on the other one. Here is its summary: summary(slmSpeedDistanceModel05) ## Response variable: sqrtdist ## Distribution used in the estimation: Normal ## Loss function used in estimation: MSE ## Coefficients: ## Estimate Std. Error Lower 2.5% Upper 97.5% ## speed 0.3967 0.0102 0.3764 0.4171 * ## ## Error standard deviation: 1.1674 ## Sample size: 50 ## Number of estimated parameters: 1 ## Number of degrees of freedom: 49 ## Information criteria: ## AIC AICc BIC BICc ## 158.3623 158.4456 160.2743 160.4373 Its parameter contains some average information about the mass of cars and their breaking forces (this is based on the formula (11.1)). The interpretation of the parameter in this model, however, is challenging. In order to get to some crude interpretation, we need to revert to maths. Model 5 can be written as: \\[\\begin{equation} distance = (\\beta_1 speed + \\epsilon)^2 . \\tag{11.7} \\end{equation}\\] If we take the first derivative of distance with respect to speed, we will get: \\[\\begin{equation} \\frac{\\mathrm{d}distance}{\\mathrm{d}speed} = 2 (\\beta_1 speed + \\epsilon) , \\tag{11.8} \\end{equation}\\] which is now closer to what we need. We can say that if speed increases by 1mph, the distance will change on average by \\(2 \\beta_1 speed\\). But this does not explain what the meaning of \\(\\beta_1\\) in the model is. So we take the second derivative with respect to speed: \\[\\begin{equation} \\frac{\\mathrm{d}^2 distance}{\\mathrm{d}^2 speed} = 2 \\beta_1 . \\tag{11.9} \\end{equation}\\] The meaning of the second derivative is that it shows the change of change of distance with a change of change of speed by 1. This implies a tricky interpretation of the parameter. Based on the summary above, the only thing we can conclude is that when the change of speed increases by 1mph, the change of distance will increase by 0.7934 feet. An alternative interpretation would be based on the model (11.5): with the increase of speed of car by 1mph, the square roo tof stopping distance would increase by 0.3967 square root feet. Neither of these two interpretations are very helpful, but this is the best we have for the parameter \\(\\beta_1\\) in the Model 5. "],["types-of-variables-transformations.html", "11.2 Types of variables transformations", " 11.2 Types of variables transformations Having considered this case study, we can summarise the possible types of transformations of variables in regression models and what they would mean. Here, we only discuss monotonic transformations, i.e. those that guarantee that if \\(x\\) was increasing before transformations, it would be increasing after transformations as well. Linear model: \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\). As discussed earlier, in this model, \\(\\beta_1\\) can be interpreted as a marginal effect of x on y. The typical interpretation is that with the increase of \\(x\\) by 1 unit, \\(y\\) will change on average by \\(\\beta_1\\) units. In case of dummy variables, their interpretation is that the specific category of product will have a different (higher or lower) impact on \\(y\\) by \\(\\beta_1\\) units. e.g. “sales of red mobile phones are on average higher than the sales of the blue ones by 100 units.” Log-Log model, or power model or a multiplicative model: \\(\\log y = \\beta_0 + \\beta_1 \\log x + \\log (1+\\epsilon)\\). It is equivalent to \\(y = \\beta_0 x^{\\beta_1} (1+\\epsilon)\\). The parameter \\(\\beta_1\\) is interpreted as elasticity: If \\(x\\) increases by 1%, the response variable \\(y\\) changes on average by \\(\\beta_1\\)%. Depending on the value of \\(\\beta_1\\), this model can capture non-linear relations with slowing down or accelerating changes. Figure 11.8 demonstrates several examples of artificial data with different values of \\(\\beta_1\\). Figure 11.8: Examples of log-log relations with different values of elasticity parameter. As discussed earlier, this model can only be applied to positive data. If there are zeroes in the data, then logarithm will be equal to \\(-\\infty\\) and it would not be possible to estimate the model correctly. Log-linear or exponential model: \\(\\log y = \\beta_0 + \\beta_1 x + \\log (1+\\epsilon)\\) is equivalent to \\(y = \\beta_0 \\exp(\\beta_1 x) (1+\\epsilon)\\). The parameter \\(\\beta_1\\) will control the change of speed of growth / decline in the model. If variable \\(x\\) increases by 1 unit, then the variable \\(y\\) will change on average by \\((\\exp(\\beta_1)-1)\\times 100\\)%. If the value of \\(\\beta_1\\) is small (roughly \\(\\beta_1 \\in (-0.2, 0.2)\\)), then due to one of the limits the interpretation can be simplified to: when \\(x\\) increases by 1 unit, the variable \\(y\\) will change on average by \\(\\beta_1\\times 100\\)%. The exponent is in general a dangerous function as it exhibits either explosive (when \\(\\beta_1 &gt; 0\\)) or implosive (when \\(\\beta_1 &lt; 0\\)) behaviour. This is shown in Figure 11.9, where the values of \\(\\beta_1\\) are -0.05 and 0.05, and we can see how fast the value of \\(y\\) changes with the increase of \\(x\\). Figure 11.9: Examples of log-linear relations with two values of slope parameter. If \\(x\\) is a dummy variable, then its interpretation is slightly different: the presence of the effect \\(x\\) leads on average to the change of variable \\(y\\) by \\(\\beta_1 \\times 100\\)%. e.g. “sales of red laptops are on average 15% higher than sales of blue laptops.” Linear-log: \\(y = \\beta_0 + \\beta_1 \\log x + \\epsilon\\). This is just a logarithmic transform of explanatory variable. The parameter \\(\\beta_1\\) in this case regulates the direction and speed of change. If \\(x\\) increases by 1%, then \\(y\\) will change on average by \\(\\frac{\\beta_1}{100}\\) units. Figure 11.10 shows two cases of relations with positive and negative slope parameters. Figure 11.10: Examples of linear-log relations with two values of slope parameter. The logarithmic model assumes that the increase in \\(x\\) always leads on average to the slow down of the value of \\(y\\). Square root: \\(y = \\beta_0 + \\beta_1 \\sqrt x + \\epsilon\\). The relation between \\(y\\) and \\(x\\) in this model looks similar to the on in linear-log model, but the with a lower speed of change: the square root represents the slow down in the change and might be suitable for cases of diminishing returns of scale in various real life problems. There is no specific interpretation for the parameter \\(\\beta_1\\) in this model - it will show how the response variable \\(y\\) will change on average wih increase of square root of \\(x\\) by one. Figure 11.11 demonstrates square root relations for two cases, with parameters \\(\\beta_1=1.5\\) and \\(\\beta_1=-1.5\\). Figure 11.11: Examples of linear - square root relations with two values of slope parameter. Quadratic equation: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\\). This relation demonstrates increase or decrease with an acceleration due to the present of squared \\(x\\). This model has an extremum (either a minimum or a maximum), when \\(x=\\frac{-\\beta_1}{2 \\beta_2}\\). This means that the growth in the data will be changed by decline or vice versa with the increase of \\(x\\). This makes the model potentially prone to overfitting, so it needs to be used with care. Note that in general the quadratic equation should include both \\(x\\) and \\(x^2\\), unless we know that the extremum should be at the point \\(x=0\\) (see the example with Model 5 in the previous section). Furthrmore, this model is close to the one with square root of \\(y\\): \\(\\sqrt y = \\beta_0 + \\beta_1 x + \\epsilon\\), with the main difference being that the latter formulation assumes that the variability of the error term will change together with the change of \\(x\\) (so called “heteroscedasticity” effect, see Section 12.2). This model was used in the examples with stopping distance above. Figure 11.12 shows to classical examples: with branches of the function going down and going up. Figure 11.12: Examples of linear-log relations with two values of slope parameter. Polynomial: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots \\ \\beta_k x^k + \\epsilon\\). This is a more general model than the quadratic one, introducing \\(k\\) polynomials. This is not used very often in analytics, because any data can be approximated by a high order polynomial, and because the branches of polynomial will inevitably lead to infinite increase / decrease, which is not a common tendency in practice. Box-Cox or power transform: \\(\\frac{y^\\lambda -1}{\\lambda} = \\beta_0 + \\beta_1 x + \\epsilon\\). This type of transform can be applied to either response variable or any of explanatory variables and can be considered as something more general than linear, log-linear, quadratic and square root models. This is because with different values of \\(\\lambda\\), the transformation would revert to one of the above. For example, with \\(\\lambda=1\\), we end up with a linear model, just with a different intercept. If \\(\\lambda=0.5\\), then we end up with square root, and when \\(\\lambda \\rightarrow 0\\), then the relation becomes equivalent to logarithmic. The choice of \\(\\lambda\\) might be a challenging task on its own, however it can be estimated via likelihood. If estimated and close to either 0, 0.5, 1 or 2, then typically a respective transformation should be applied instead of Box-Cox. For example, if \\(\\lambda=0.49\\), then taking square root might be a preferred option. In this subsection we discussed the basic types of variables transformations on examples with simple linear regression. The more complicated models with multiple explanatory variables and complex transformations can be considered as well. However, whatever transformation is considered, it needs to be meaningful and come from the theory, not from the data. Otherwise we may overfit the data, which will lead to a variety of issues, some of which are discussed in Section 12.1. "],["assumptions.html", "Chapter 12 Statistical models assumptions", " Chapter 12 Statistical models assumptions In order for a statistical model to work adequately and not to fail, when applied to a data, several assumptions about it should hold. If they do not, then the model might lead to biased or inefficient estimates of parameters and inaccurate forecasts. In this section we discuss the main assumptions, united in three big groups: Model is correctly specified; Residuals are independent and identically distributed (i.i.d.); The explanatory variables are not correlated with anything but the response variable. We do not aim to explain why the violation of assumptions would lead to the discussed problem, and refer a curious reader to econometrics textbooks (for example Hanck et al., 2020). In many cases, in our discussions in this textbook, we assume that all of these assumptions hold. In some of the cases, we will say explicitly, which are violated and what needs to be done in those situations. References "],["assumptionsCorrectModel.html", "12.1 Model is correctly specified", " 12.1 Model is correctly specified This is one of the fundamental group of assumptions, which can be summarised as “we have included everything necessary in the model in the correct form.” It implies that: We have not omitted important variables in the model (underfitting the data); We do not have redundant variables in the model (overfitting the data); The necessary transformations of the variables are applied; We do not have outliers in the model. 12.1.1 Omitted variables If there are some important variables that we did not include in the model, then the estimates of the parameters might be biased and in some cases quite seriously (e.g. positive sign instead of the negative one). A classical example of model with omitted important variables is simple linear regression, which by definition includes only one explanatory variable. Making decisions based on such model might not be wise, as it might mislead about the significance and sign of effects. Yes, we use simple linear regression for educational purposes, to understand how the model works and what it implies, but it is not sufficient on its own. Finally, when it comes to forecasting, omitting important variables is equivalent to underfitting the data, ignoring significant aspects of the model. This means that the point forecasts from the model might be biased (systematic under or over forecasting), the variance of the error term will be higher than needed, which will result in wider than necessary prediction interval. In some cases, it is possible to diagnose the violation of this assumption. In order to do that an analyst needs to analyse a variety of plots of residuals vs fitted, vs time (if we deal with time series), and vs omitted variables. Consider an example with mtcars data and a simple linear regression: mtcarsSLR &lt;- alm(mpg~wt, mtcars, loss=&quot;MSE&quot;) Based on the preliminary analysis that we have conducted in Sections 2 and 6, this model omits important variables. And there are several basic plots that might allow us diagnosing the violation of this assumption. par(mfcol=c(1,2)) plot(mtcarsSLR,c(1,2)) Figure 12.1: Diagnostics of omitted variables. Figure 12.1 demonstrates actuals vs fitted and fitted vs standardised residuals. The standardised residuals are the residuals from the model that are divided by their standard deviation, thus removing the scale. What we want to see on the first plot in Figure 12.1, is for all the point lie around the grey line and for the LOWESS line to coincide with the grey line. That would mean that the relations are captured correctly and all the observations are explained by the model. As for the second plot, we want to see the same, but it just presents that information in a different format, which is sometimes easier to analyse. In both plot of Figure 12.1, we can see that there are still some patterns left: the LOWESS line has a u-shaped form, which in general means that something is wrong with model specification. In order to investigate if there are any omitted variables, we construct a spread plot of residuals vs all the variables not included in the model (Figure 12.2). spread(data.frame(residuals=resid(mtcarsSLR), mtcars[,-c(1,6)])) Figure 12.2: Diagnostics of omitted variables. What we want to see in Figure 12.2 is the absence of any patterns in plots of residuals vs variables. However, we can see that there are still many relations. For example, with the increase of the number of cylinders, the mean of residuals decreases. This might indicate that the variable is needed in the model. And indeed, we can imagine a situation, where mileage of a car (the response variable in our model) would depend on the number of cylinders because the bigger engines will have more cylinders and consume more fuel, so it makes sense to include this variable in the model as well. Note that we do not suggest to start modelling from simple linear relation! You should construct a model that you think is suitable for the problem, and the example above is provided only for illustrative purposes. 12.1.2 Redundant variables If there are redundant variables that are not needed in the model, then the estimates of parameters and point forecasts might be unbiased, but inefficient. This implies that the variance of parameters can be lower than needed and thus the prediction intervals will be narrower than needed. There are no good instruments for diagnosing this issue, so judgment is needed, when deciding what to include in the model. 12.1.3 Transformations This assumption implies that we have taken all possible non-linearities into account. If, for example, instead of using a multiplicative model, we apply an additive one, the estimates of parameters and the point forecasts might be biased. This is because the model will produce linear trajectory of the forecast, when a non-linear one is needed. This was discussed in detail in Section 11. The diagnostics of this assumption is similar to the diagnostics shown above for the omitted variables: construct actuals vs fitted and residuals vs fitted in order to see if there are any patterns in the plots. Take the multiple regression model for mtcars, which includes several variables, but is additive in its form: mtcarsALM01 &lt;- alm(mpg~wt+qsec+am, mtcars, loss=&quot;MSE&quot;) Arguably, the model includes important variables (although there might be some others that could improve it), but the residuals will show some patterns, because the model should be multiplicative (see Figure 12.3), because mileage should not reduce linearly with increase of those variables. In order to understand that, ask yourself, whether the mileage can be negative and whether weight and other variables can be non-positive (a car with \\(wt=0\\) just does not exist). par(mfcol=c(1,2)) plot(mtcarsALM01,c(1,2)) Figure 12.3: Diagnostics of necessary transformations in linear model. Figure 12.3 demonstrates the u-shaped pattern in the residuals, which is one of the indicators of a wrong model specification, calling for a non-linear transformation. We can try a model in logarithms: mtcarsALM02 &lt;- alm(log(mpg)~log(wt)+log(qsec)+am, mtcars, loss=&quot;MSE&quot;) And see what would happen with the diagnostics of the model in logarithms: par(mfcol=c(1,2)) plot(mtcarsALM02,c(1,2)) Figure 12.4: Diagnostics of necessary transformations in log-log model. Figure 12.4 demonstrates that while the LOWESS lines do not coincide with the grey lines, the residuals do not have obvious patterns. The fact that the LOWESS line starts from below, when fitted values are low in our case only shows that we do not have enough observations with low actual values. As a result, LOWESS is impacted by 2 observations that lie below the grey line. This demonstrates that LOWESS lines should be taken with a pinch of salt and we should abstain from finding patterns in randomness, when possible. Overall, the log-log model is more appropriate to this data than the linear one. 12.1.4 Outliers In a way, this assumption is similar to the first one with omitted variables. The presence of outliers might mean that we have missed some important information, implying that the estimates of parameters and forecasts would be biased. There can be other reasons for outliers as well. For example, we might be using a wrong distributional assumption. If so, this would imply that the prediction interval from the model is narrower than necessary. The diagnostics of outliers comes to producing standardised residuals vs fitted, to studentised vs fitted and to Cook’s distance plot. While we are already familiar with the first one, the other two need to be explained in more detail. Studentised residuals are the residuals that are calculated in the same way as the standardised ones, but removing the value of each residual. For example, the studentised residual on observation 25 would be calculated as the raw residual divided by standard deviation of residuals, calculated without this 25th observation. This way we diminish the impact of potential serious outliers on the standard deviation, making it easier to spot the outliers. As for the Cook’s distance, its idea is to calculate measures for each observation showing how influential they are in terms of impact on the estimates of parameters of the model. If there is an influential outlier, then it would distort the values of parameters, causing bias. par(mfcol=c(1,2)) plot(mtcarsALM02,c(2,3)) Figure 12.5: Diagnostics of outliers. Figure 12.5 demonstrates standardised and studentised residuals vs fitted values for the log-log model on mtcars data. We can see that the plots are very similar, which already indicates that there are no strong outliers in the residuals. The bounds produced on the plots correspond to the 95% prediction interval, so by definition it should contain \\(0.95\\times 32 \\approx 30\\) observations. Indeed, there are only two observations: 15 and 25 - that lie outside the bounds. Technically, we would suspect that they are outliers, but they do not lie far away from the bounds and their number meets our expectations, so we can conclude that there are no outliers in the data. plot(mtcarsALM02,12) Figure 12.6: Cook’s distance plot. Finally, we produce Cook’s distance over observations in Figure 12.6. The x-axis says “Time,” because alm() function is tailored for time series data, but this can be renamed into “observations.” The plot shows how influential the outliers are. If there were some significantly influential outliers in the data, then the plot would draw red lines, corresponding to 0.5, 0.75 and 0.95 quantiles of Fisher’s distribution, and the line of those outliers would be above the red lines. Consider the following example for demonstration purposes: mtcarsData[28,6] &lt;- 4 mtcarsALM03 &lt;- alm(log(mpg)~log(wt)+log(qsec)+am, mtcarsData, loss=&quot;MSE&quot;) This way, we intentionally create an influential outlier (the car should have the minimum weight in the dataset, and now it has a very high one). plot(mtcarsALM03, 12, ylim=c(0,1.5), xlab=&quot;Observations&quot;, main=&quot;&quot;) Figure 12.7: Cook’s distance plot for the data with influential outlier. Figure 12.7 shows how Cook’s distance will look in this case - it detects that there is an influential outlier, which is above the norm. We can compare the parameters of the new and the old models to see how the introduction of one outlier leads to bias in the estimates of parameters: rbind(coef(mtcarsALM02), coef(mtcarsALM03)) ## (Intercept) log(wt) log(qsec) am ## [1,] 1.2095788 -0.7325269 0.8857779 0.05205307 ## [2,] 0.1382442 -0.4852647 1.1439862 0.21406331 "],["assumptionsResidualsAreIID.html", "12.2 Residuals are i.i.d.", " 12.2 Residuals are i.i.d. There are five assumptions in this group: There is no autocorrelation in the residuals; The residuals are homoscedastic; The expectation of residuals is zero, no matter what; The variable follows the assumed distribution; More generally speaking, distribution of residuals does not change over time. 12.2.1 No autocorrelations This assumption only applies to time series data, and in a way comes to capturing correctly the dynamic relations between variables. The term “autocorrelation” refers to the situation, when variable is correlated with itself from the past. If the residuals are autocorrelated, then something is neglected by the applied model. Typically, this leads to inefficient estimates of parameters, which in some cases might also become biased. The model with autocorrelated residuals might produce inaccurate point forecasts and prediction intervals of a wrong width (wider or narrower than needed). There are several ways of diagnosing the problem, including visual analysis and statistical tests. In order to show some of them, we consider the Seatbelts data from datasets package for R. We fit a basic model, predicting monthly totals of car drivers in the Great Britain killed or seriously injured in car accidents: SeatbeltsALM01 &lt;- alm(drivers~PetrolPrice+kms+front+rear+law, Seatbelts) In order to do graphical diagnose, we can produce plots of standardised / studentised residuals over time: plot(SeatbeltsALM01,8,main=&quot;&quot;) Figure 12.8: Standardised residuals over time. If the assumption is not violated, then the plot in Figure 12.8 would not contain any patterns. However, we can see that, first, there is a seasonality in the residuals and second, the expectation (captured by the red LOWESS line) changes over time. This indicates that there might be some autocorrelation in residuals caused by omitted components. We do not aim to resolve the issue now, it is discussed in more detail in Section 14.5 of Svetunkov (2021b). The other instrument for diagnostics is ACF / PACF plots, which are produced in alm() via the following command: par(mfcol=c(1,2)) plot(SeatbeltsALM01,c(10,11),main=&quot;&quot;) Figure 12.9: ACF and PACF of the residuals of a model. These are discussed in more detail in Sections ?? and ??. 12.2.2 Homoscedastic residuals In general, we assume that the variance of residuals is constant. If this is violated, then we say that there is a heteroscedasticity in the model. This means that with a change of a variable, the variance of the residuals will change as well. If the model neglects this, then typically the estimates of parameters become inefficient and prediction intervals are wrong: they are wider than needed in some cases (e.g.m when the volume of data is low) and narrower than needed in the other ones (e.g. on high volume data). Typically, this assumption will be violated if the model is not specified correctly. The classical example is the income versus expenditure on meals for different families. If the income is low, then there is not many options what to buy and the variability of expenditures would be low. However, with the increase of the income, the mean expenditures and their variability would increase as well, because there are more options of what to buy, including both cheap and expensive products. If we constructed a basic linear model on such data, then it would violate the assumption of homoscedasticity and as a result will have the issues discussed above. But arguably this would typically appear because of the misspecification of the model. For example, taking logarithms might resolve the issue in many cases, implying that the effect of one variable on the other should be multiplicative rather than the additive. Alternatively, dividing variables by some other variable (e.g. working with expenses per family member, not per family) might resolve the problem as well. Unfortunately, the transformations are not the panacea, so in some cases analyst would need to construct a model, taking the changing variance into account (e.g. GARCH or GAMLSS models). This is discussed in Section ??. While in forecasting we are more interested in the holdout performance of models, in econometrics, the parameters of models are typically of the main interest. And, as we discussed earlier, in case of correctly specified model with heteroscedastic residuals, the estimates of parameters will be unbiased, but inefficient. So, econometricians would use different approaches to diminish the heteroscedasticity effect on parameters: either a different estimator for a model (such as Weighted Least Squares), or a different method for calculation of standard errors of parameters (e.g. Heteroskedasticity-Consistent Standard Errors). This does not resolve the problem, but rather corrects the parameters of the model (i.e. does not heal the illness, but treats the symptoms). Although these approaches typically suffice for the analytical purposes, they do not fix the issues in forecasting. The diagnostics of heteroscedasticity can be done via plotting absolute and / or squared residuals against the fitted values. par(mfcol=c(1,2)) plot(mtcarsALM01,4:5) Figure 12.10: Detecting heteroscedasticity. Model 1. If your model assumes that residuals follow a distribution related to the Normal one, then you should focus on the plot of squared residuals vs fitted, as this would be closer related to the variance of the distribution. In the example of mtcars model in Figure 12.10 we see that the variance of residuals increases with the increase of Fitted values (the LOWESS line increases and the overall variability around 1200 is lower than the one around 2000). This indicates that the residuals are heteroscedastic. One of the possible solutions of the problem is taking the logarithms, as we have done in the model mtcarsALM02: par(mfcol=c(1,2)) plot(mtcarsALM02,4:5) Figure 12.11: Detecting heteroscedasticity. Model 2. While the LOWESS lines on plots in Figure 12.11 demonstrate some dynamics, the variability of residuals does not change significantly with the increase of fitted value, so non-linear transformation seems to fix the issue in our example. If it would not, then we would need to consider either some other transformations or finding out, which of the variables causes heteroscedasticity and then modelling it explicitly via the scale model (Section ??). 12.2.3 Mean of residuals While in sample, this holds automatically in many cases (e.g. when using Least Squares method for regression model estimation), this assumption might be violated in the holdout sample. In this case the point forecasts would be biased, because they typically do not take the non-zero mean of forecast error into account, and the prediction interval might be off as well, because of the wrong estimation of the scale of distribution (e.g. variance is higher than needed). This assumption also implies that the expectation of residuals is zero even conditional on the explanatory variables in the model. If it is not, then this might mean that there is still some important information omitted in the applied model. This implies that the following holds for all \\(x_i\\): \\[\\begin{equation*} \\mathrm{cov}(x_i, e) = 0 , \\end{equation*}\\] which in the case of \\(\\mathrm{E}(e)=0\\) is equivalent to: \\[\\begin{equation*} \\mathrm{E}(x_i e) = 0 . \\end{equation*}\\] If OLS is used in linear model estimation, then this condition is satisfied in sample automatically and does not require checking. Note that some models assume that the expectation of residuals is equal to one instead of zero (e.g. multiplicative error models). The idea of the assumption stays the same, it is only the value that changes. The diagnostics of the problem would be similar to the case of non-linear transformations or autocorrelations: plotting residuals vs fitted or residuals vs time and trying to find patterns. If the mean of residuals changes either with the change of fitted values of with time, then the conditional expectation of residuals is not zero, and something is missing in the model. 12.2.4 Distributional assumptions In some cases we are interested in using methods that imply specific distributional assumptions about the model and its residuals. For example, it is assumed in the classical linear model that the error term follows Normal distribution. Estimating this model using MLE with the probability density function of Normal distribution or via minimisation of Mean Squared Error (MSE) would give efficient and consistent estimates of parameters. If the assumption of normality does not hold, then the estimates might be inefficient and in some cases inconsistent. When it comes to forecasting, the main issue in the wrong distributional assumption appears, when prediction intervals are needed: they might rely on a wrong distribution and be narrower or wider than needed. Finally, if we deal with the wrong distribution, then the model selection mechanism might be flawed and would lead to the selection of an inappropriate model. The most efficient way of diagnosing this, is constructing QQ-plot of residuals (discussed in Section 2.2). plot(mtcarsALM02,6) Figure 12.12: QQ-plot of residuals of model 2 for mtcars dataset. Figure 12.12 shows that all the points lie close to the line (with minor fluctuations around it), so we can conclude that the residuals follow the normal distribution. In comparison, Figure 12.12 demonstrates how residuals would look in case of a wrong distribution. Although the values lie not too far from ths straight line, there are several observations in the tails that are further away than needed. Comparing the two plots, we would select the on in Figure 12.12, as the residuals are better behaved. plot(mtcarsALM01,6) Figure 12.13: QQ-plot of residuals of model 1 for mtcars dataset. 12.2.5 Distribution does not change This assumption aligns with the Subsection 12.2.4, but in this specific context implies that all the parameters of distribution stay the same and the shape of distribution does not change. If the former is violated then we might have one of the issues discussed above. If the latter is violated then we might produce biased forecasts and underestimate / overestimate the uncertainty about the future. The diagnosis of this comes to analysing QQ-plots, similar to Subsection 12.2.4. References "],["assumptionsXreg.html", "12.3 The explanatory variables are not correlated with anything but the response variable", " 12.3 The explanatory variables are not correlated with anything but the response variable There are two assumptions in this group: No multicollinearity; No endogeneity; 12.3.1 Multicollinearity One of the classical issues in econometrics and in statistics in regression context is the issue of multicollinearity, the effect when several explanatory variables are correlated. In a way, this has nothing to do with classical assumptions of linear regression, because it is unreasonable to assume that the explanatory variables have some specific relation between them - they are what they are, and multicollinearity mainly causes issues with estimation of the parameters of model, not with its structure. But it is an issue nonetheless, so it is worth exploring. Multicollinearity appears, when either some of explanatory variables are correlated with each other (see Section 6.3), or their linear combination explains another explanatory variable included in the model. Depending on the strength of this relation and the estimation method used for model construction, the multicollinearity might cause issues of varying severity. For example, in the case, when two variables are perfectly correlated (correlation coefficient is equal to 1 or -1), the model will have perfect multicollinearity and it would not be possible to estimate its parameters. Another example is a case, when an explanatory variable can be perfectly explained by a set of other explanatory variables (resulting in \\(R^2\\) being close to one), which will cause exactly the same issue. The classical example of this situation is the dummy variables trap (see Section 10), when all values of categorical variable are included in regression together with the constant resulting in the linear relation \\(\\sum_{j=1}^k d_j = 1\\). Given that the square root of \\(R^2\\) of linear regression is equal to multiple correlation coefficient, these two situations are equivalent and just come to “absolute value of correlation coefficient is equal to 1.” Finally, if correlation coefficient is high, but not equal to one, the effect of multicollinearity will lead to less efficient estimates of parameters. The loss of efficiency is in this case proportional to the absolute value of correlation coefficient. In case of forecasting, the effect is not as straight forward, and in some cases might not damage the point forecasts, but can lead to prediction intervals of an incorrect width. The main issue of multicollinearity comes to the difficulties in the model estimation in a sample. If we had all the data in the world, then the issue would not exist. All of this tells us how this problem can be diagnosed and that this diagnosis should be carried out before constructing regression model. First, we can calculate correlation matrix for the available variables. If they are all numeric, then cor() function from stats should do the trick (we remove the response variable from consideration): cor(mtcars[,-1]) ## cyl disp hp drat wt qsec ## cyl 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 -0.59124207 ## disp 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 -0.43369788 ## hp 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 -0.70822339 ## drat -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 0.09120476 ## wt 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 -0.17471588 ## qsec -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 1.00000000 ## vs -0.8108118 -0.7104159 -0.7230967 0.44027846 -0.5549157 0.74453544 ## am -0.5226070 -0.5912270 -0.2432043 0.71271113 -0.6924953 -0.22986086 ## gear -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 -0.21268223 ## carb 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 -0.65624923 ## vs am gear carb ## cyl -0.8108118 -0.52260705 -0.4926866 0.52698829 ## disp -0.7104159 -0.59122704 -0.5555692 0.39497686 ## hp -0.7230967 -0.24320426 -0.1257043 0.74981247 ## drat 0.4402785 0.71271113 0.6996101 -0.09078980 ## wt -0.5549157 -0.69249526 -0.5832870 0.42760594 ## qsec 0.7445354 -0.22986086 -0.2126822 -0.65624923 ## vs 1.0000000 0.16834512 0.2060233 -0.56960714 ## am 0.1683451 1.00000000 0.7940588 0.05753435 ## gear 0.2060233 0.79405876 1.0000000 0.27407284 ## carb -0.5696071 0.05753435 0.2740728 1.00000000 This matrix tells us that there are some variables that are highly correlated and might reduce efficiency of estimates of parameters of regression model if included in the model together. This mainly applies to cyl and disp, which both characterise the size of engine. If we have a mix of numerical and categorical variables, then assoc() (aka association()) function from greybox will be more appropriate (see Section 6). assoc(mtcars) In order to cover the second situation with linear combination of variables, we can use the determ() (aka determination()) function from greybox: determ(mtcars[,-1]) ## cyl disp hp drat wt qsec vs am ## 0.9349544 0.9537470 0.8982917 0.7036703 0.9340582 0.8671619 0.7986256 0.7848763 ## gear carb ## 0.8133441 0.8735577 This function will construct linear regression models for each variable from all the other variables and report the \\(R^2\\) from these models. If there are coefficients of determination close to one, then this might indicate that the variables would cause multicollinearity in the model. In our case, we see that disp is linearly related to other variables, and we can expect it to cause the reduction of efficiency of estimate of parameters. If we remove it from the consideration (we do not want to include it in our model anyway), then the picture will change: determ(mtcars[,-c(1,3)]) ## cyl hp drat wt qsec vs am gear ## 0.9299952 0.8596168 0.6996363 0.8384243 0.8553748 0.7965848 0.7847198 0.8121855 ## carb ## 0.7680136 Now cyl has linear relation with some other variables, so it would not be wise to include it in the model with the other variables. We would need to decide, what to include based on our understanding of the problem. Instead of calculating the coefficients of determination, econometricians prefer to calculate Variance Inflation Factor (VIF), which shows by how many times the estimates of parameters will loose efficiency. Its formula is based on the \\(R^2\\) calculated above: \\[\\begin{equation*} \\mathrm{VIF}_i = \\frac{1}{1-R_i^2} \\end{equation*}\\] for each model \\(i\\). Which in our case can be calculated as: 1/(1-determ(mtcars[,-c(1,3)])) ## cyl hp drat wt qsec vs am gear ## 14.284737 7.123361 3.329298 6.189050 6.914423 4.916053 4.645108 5.324402 ## carb ## 4.310597 This is useful when you want to see the specific impact on the variance of parameters, but is difficult to work with, when it comes to model diagnostics, because the value of VIF lies between zero and infinity. So, I prefer using the determination coefficients instead, which is always bounded by \\((0, 1)\\) region and thus easier to interpret. Finally, in some cases nothing can be done with multicollinearity, it just exists, and we need to include those correlated variables. This might not be a big problem, as long as we acknowledge the issues it will cause to the estimates of parameters. 12.3.2 Engogeneity Endogeneity applies to the situation, when the dependent variable \\(y_j\\) influences the explanatory variable \\(x_j\\) in the model on the same observation. The relation in this case becomes bi-directional, meaning that the basic model is not appropriate in this situation any more. The parameters and forecasts will typically be biased, and a different estimation method would be needed or maybe a different model would need to be constructed in order to fix this. Endogeneity cannot be properly diagnosed and comes to the judgment: do we expect the relation between variable to be one directional or bi-directional? Note that if we work with time series, then endogeneity would only appear, when the bi-directional relation happens at the same time \\(t\\), not over time. "],["likelihoodApproach.html", "Chapter 13 Likelihood Approach", " Chapter 13 Likelihood Approach We will use different estimation techniques throughout this book, one of the main of which is Maximum Likelihood Estimate (MLE). The very rough idea of the approach is to maximise the chance that each observation in the sample follows a pre-selected distribution with specific set of parameters. In a nutshell, what we try to do when using likelihood for estimation, is fit the distribution function to the data. In order to demonstrate this idea, we start in a non-conventional way, with an example in R. We will then move to the mathematical side of the problem. "],["an-example-in-r.html", "13.1 An example in R", " 13.1 An example in R We consider a simple example, when we want to estimate the model \\(y_j = \\mu_y + \\epsilon_j\\) (global average), assuming that the error term follows normal distribution: \\(\\epsilon_j \\sim \\mathcal{N}(0, \\sigma^2)\\), which means that \\(y_j \\sim \\mathcal{N}(\\mu_{y}, \\sigma^2)\\). In this case we want to estimate two parameters using likelihood: location \\(\\hat{\\mu}_y\\) and scale \\(\\hat{\\sigma}^2\\). First, we generate the random variable in R and plot its distribution: y &lt;- rnorm(1000, 100, 10) hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) As expected, the distribution of this variable (1000 observations) has the bell shape of Normal distribution. In order to estimate the parameters, for the distribution, we will try them one by one and see how the likelihood and the shape of the fitted curve to this histogram change. We start with \\(\\hat{\\mu}_y=80\\) and \\(\\hat{\\sigma}=10\\) just to see how the probability density function of normal distribution fits the data: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),80,10),col=&quot;red&quot;,lwd=2) abline(v=80,col=&quot;red&quot;,lwd=2) Figure 13.1: ML example with Normal curve and \\(\\hat{\\mu}_y=80\\) and \\(\\hat{\\sigma}=10\\) and we get the following log-likelihood value (we will discuss how this formula can be obtained later): sum(dnorm(y,80,10,log=T)) ## [1] -5693.649 In order for the normal distribution on 13.1 to fit the data well, we need to shift the estimate of \\(\\mu_y\\) to the right, thus increasing the value to, let’s say, \\(\\hat{\\mu}_y=90\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),90,10),col=&quot;orange&quot;,lwd=2) abline(v=90,col=&quot;orange&quot;,lwd=2) Figure 13.2: ML example with Normal curve and \\(\\hat{\\mu}_y=90\\) and \\(\\hat{\\sigma}=10\\) Now, in Figure 13.2, the normal curve is much closer to the data, but it is still a bit off. The log-likelihood value in this case is -4211.206, which is higher than the previous one, indicating that we are moving towards the maximum of the likelihood function. Moving it further, setting \\(\\hat{\\mu}_y=100\\), we get: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),100,10),col=&quot;green3&quot;,lwd=2) abline(v=100,col=&quot;green3&quot;,lwd=2) Figure 13.3: ML example with Normal curve and \\(\\hat{\\mu}_y=100\\) and \\(\\hat{\\sigma}=10\\) Figure 13.2 demonstrates a much better fit than in the previous cases with the log-likelihood of -3728.763, which is even higher than in the previous case. We are almost there. In fact, in order to maximise this likelihood, we just need to calculate the sample mean of the variable (this is the MLE of the location parameter in normal distribution) and insert it in the function to obtain: hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),10),col=&quot;darkgreen&quot;,lwd=2) abline(v=mean(y),col=&quot;darkgreen&quot;,lwd=2) Figure 13.4: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=10\\) So the value of \\(\\hat{\\mu}_y=\\bar{y}=\\) 99.824 (where \\(\\bar{y}\\) is the sample mean) maximises the likelihood function, resulting in log-likelihood of -3728.609. In a similar fashion we can get the MLE of the scale parameter \\(\\sigma^2\\) of the model. In this case, we will be changing the height of the distribution. Here is an example with \\(\\hat{\\mu}_y=\\) 99.824 and \\(\\hat{\\sigma}=15\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),15),col=&quot;royalblue&quot;,lwd=2) abline(v=mean(y),col=&quot;royalblue&quot;,lwd=2) Figure 13.5: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=15\\) Figure 13.5 demonstrates that the curve is located lower than needed, which implies that the scale parameter \\(\\hat{\\sigma}\\) is too high. The log-likelihood value in this case is -3852.36. In order to get a better fit of the curve to the data, we need to reduce the \\(\\hat{\\sigma}\\). Here how the situation would look for the case of \\(\\hat{\\sigma}=10\\): hist(y, xlim=c(50,150), main=&quot;&quot;, probability=TRUE) lines(c(50:150),dnorm(c(50:150),mean(y),10),col=&quot;darkblue&quot;,lwd=2) abline(v=mean(y),col=&quot;darkblue&quot;,lwd=2) Figure 13.6: ML example with Normal curve and \\(\\hat{\\mu}_y=\\bar{y}\\) and \\(\\hat{\\sigma}=10\\) The fit on Figure 13.6 is better than on Figure 13.5, which is also reflected in the log-likelihood value being equal to -3728.609 instead of -3852.36. The best fit and the maximum of the likelihood is obtained, when the scale parameter is estimated using the formula \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{j=1}^n\\left(y_j - \\bar{y}\\right)^2\\), resulting in log-likelihood of -3728.559. Note that if we use the unbiased estimate of the variance \\(\\hat{s}^2 = \\frac{1}{n-1}\\sum_{j=1}^n\\left(y_j - \\bar{y}\\right)^2\\), the log-likelihood will not reach the maximum and will be equal to -3728.559. In our special case the difference between the two is infinitesimal, because of the large sample (1000 observations), but it will be more substantial on small samples. Still, the two likelihood values are different, which can be checked in R via the following commands: # The maximum log-likelihood with the biased variance logLik01 &lt;- sum(dnorm(y,mean(y),sqrt(mean((y-mean(y))^2)),log=TRUE)) # The log-likelihood value with the unbiased variance logLik02 &lt;- sum(dnorm(y,mean(y),sd(y),log=TRUE)) # The difference between the two logLik01 - logLik02 ## [1] 0.0002501668 All of this is great, but so far we have discussed a very special case, when the data follows normal distribution and we fit the respective model. But what if the model is wrong (no kidding!)? In that case the idea stays the same: we need to find the parameters of the normal distribution, that would guarantee the best possible fit to the non-normal data. Here is an example with MLE of parameters of Normal distribution for the data following Log Normal one: y &lt;- rlnorm(1000, log(80), 0.4) hist(y, main=&quot;&quot;, probability=T, xlim=c(0,300)) lines(c(0:300),dnorm(c(0:300),mean(y),sd(y)),col=&quot;blue&quot;,lwd=2) Figure 13.7: ML example with Normal curve on Log Normal data Figure 13.7 shows that the Normal model does not fit the Log Normal data properly, but this is the best we can get, given our assumptions. The log-likelihood in this case is -5051.123. The much better model would be the Log Normal one: hist(y, main=&quot;&quot;, probability=T, xlim=c(0,300)) lines(c(0:300),dlnorm(c(0:300),mean(log(y)),sd(log(y))),col=&quot;red&quot;,lwd=2) Figure 13.8: ML example with Log Normal curve on Log Normal data The model in Figure 13.8 has the log likelihood of -4859.033. This indicates that the Log Normal model is more appropriate for the data and gives us an idea that it is possible to compare different distributions via the likelihood, finding the better fit to the data. This idea is explored further in the next section. As a final word, when it comes to more complicated models with more parameters and dynamic structure, the specific curves and data become more complicated, but the logic of the likelihood approach stays the same. "],["likelihoodApproachMaths.html", "13.2 Mathematical explanation", " 13.2 Mathematical explanation Now we can discuss the same idea from the mathematical point of view. We estimated the following simple model: \\[\\begin{equation} y_j = \\mu_{y} + \\epsilon_j, \\tag{13.1} \\end{equation}\\] assuming normal distribution of the residuals. In order to make things closer to the regression context, we will introduce changing location, which is defined by the regression line (thus, it is conditional on the set of \\(k-1\\) explanatory variables): \\[\\begin{equation} y_j = \\mu_{y,j} + \\epsilon_j, \\tag{13.2} \\end{equation}\\] where \\(\\mu_{y,j}\\) is the population regression line, defined via: \\[\\begin{equation} \\mu_{y,j} = \\beta_0 + \\beta_1 x_{1,j}+ \\beta_2 x_{2,j} + \\dots + \\beta_{k-1} x_{k-1,j} . \\tag{13.3} \\end{equation}\\] The typical assumption in regression context is that \\(\\epsilon_j \\sim \\mathcal{N}(0, \\sigma^2)\\) (normal distribution with zero mean and fixed variance), which means that \\(y_j \\sim \\mathcal{N}(\\mu_{y,j}, \\sigma^2)\\). We can use this assumption in order to calculate the point likelihood value for each observation based on the PDF of Normal distribution (Subsection 3.1): \\[\\begin{equation} \\mathcal{L} (\\mu_{y,j}, \\sigma^2 | y_j) = f(y_j | \\mu_{y,j}, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{\\left(y_j - \\mu_{y,j} \\right)^2}{2 \\sigma^2} \\right). \\tag{13.4} \\end{equation}\\] Very roughly, what the value (13.4) shows is the chance that the specific observation comes from the assumed model with specified parameters (we know that in real world the data does not come from any model, but this interpretation is easier to work with). Note that the likelihood is not the same as probability, because for any continuous random variables the probability for it to be equal to any specific number is equal to zero. However, the idea of likelihood has some similarities with the probability, so we prefer to refer to it as a “chance.” The point likelihood (13.4) is not very helpful on its own, but we can get \\(n\\) values like that, based on our sample of data. We can then summarise them in one number, that would characterise the whole sample, given the assumed distribution, applied model and selected values of parameters: \\[\\begin{equation} \\mathcal{L} (\\boldsymbol{\\theta} | \\mathbf{y}) = \\mathcal{L} (\\mu_{y,j}, \\sigma^2 | \\mathbf{y}) = \\prod_{j=1}^n f(y_j | \\mu_{y,j}, \\sigma^2), \\tag{13.5} \\end{equation}\\] where \\(\\boldsymbol{\\theta}\\) is the vector of all parameters in the model (in our example, it is just the two of them). We take the product of likelihoods in (13.5) because we need to get the joint likelihood for all observations and because we can typically assume that the point likelihoods are independent of each other (for example, the value on observation \\(j\\) will not be influenced by the value on \\(j-1\\)). The value (13.5) shows the summary chance that the data comes from the assumed model with specified parameters. Having this value, we can change the values of parameters of the model, getting different value of (13.5) (as we did in the example above). Using an iterative procedure, we can get such estimates of parameters that would maximise the likelihood (13.5), which are called Maximum Likelihood Estimates (MLE) of parameters. However, working with the products in that formula is difficult, so typically we linearise it using natural logarithm, obtaining log-likelihood: \\[\\begin{equation} \\ell (\\boldsymbol{\\theta} | \\mathbf{y}) = \\log \\mathcal{L} (\\boldsymbol{\\theta} | \\mathbf{y}) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) -\\sum_{j=1}^n \\frac{\\left(y_j - \\mu_{y,j} \\right)^2}{2 \\sigma^2} . \\tag{13.6} \\end{equation}\\] Based on that, we can find some of parameters of the model analytically. For example, we can take derivative of (13.6) with respect to the scale \\(\\hat{\\sigma}^2\\) (which is an estimate of the true parameter \\(\\sigma^2\\)) and equate it to zero in order to find the value that maximises the log-likelihood function in our sample: \\[\\begin{equation} \\frac{d \\ell (\\boldsymbol{\\theta} | \\mathbf{y})}{d \\hat{\\sigma}^2} = -\\frac{n}{2} \\frac{1}{\\hat{\\sigma}^2} + \\frac{1}{2 \\hat{\\sigma}^4}\\sum_{j=1}^n \\left(y_j - \\mu_{y,j} \\right)^2 =0 , \\tag{13.7} \\end{equation}\\] which after multiplication of both sides by \\(2 \\hat{\\sigma}^4\\) leads to: \\[\\begin{equation} n \\hat{\\sigma}^2 = \\sum_{j=1}^n \\left(y_j - \\mu_{y,j} \\right)^2 , \\tag{13.8} \\end{equation}\\] or \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{j=1}^n \\left(y_j - \\mu_{y,j} \\right)^2 . \\tag{13.9} \\end{equation}\\] The value (13.9) is in fact a Mean Squared Error (MSE) of the model. If we calculate the value of \\(\\hat{\\sigma}^2\\) using the formula (13.9), we will maximise the likelihood with respect to the scale parameter. In fact, we can insert (13.9) in (13.6) in order to obtain the so called concentrated (or profile) log-likelihood for the normal distribution: \\[\\begin{equation} \\ell^* (\\boldsymbol{\\theta}, \\hat{\\sigma}^2 | \\mathbf{y}) = -\\frac{n}{2}\\left( \\log(2 \\pi e) + \\log \\hat{\\sigma}^2 \\right) . \\tag{13.10} \\end{equation}\\] This function is useful because it simplifies some calculations and also demonstrates the condition, for which the likelihood is maximised: the first part on the right hand side of the formula does not depend on the parameters of the model, it is only the \\(\\log \\hat{\\sigma}^2\\) that does. So, the maximum of the concentrated log-likelihood (13.10) is obtained, when \\(\\hat{\\sigma}^2\\) is minimised, implying the minimisation of MSE, which is the mechanism behind the “Ordinary Least Squares” (OLS) ) estimation method. By doing this, we have just demonstrated that if we assume normality in the model, then the estimates of its parameters obtained via the maximisation of the likelihood coincide with the values obtained from OLS. So, why bother with MLE, when we have OLS? First, the finding above holds for normal distribution only. If we assume a different distribution, we would get different estimates of parameters. In some cases, it might not be possible or reasonable to use OLS, but MLE would be a plausible option (for example, logistic, Poisson and any other non-standard model). Second, the MLE of parameters have good statistical properties: they are consistent and efficient. These properties hold almost universally for many likelihoods under very mild conditions. Note that the MLE of parameters are not necessarily unbiased, but after estimating the model, one can de-bias some of them (for example, calculate the standard deviation of the error via devision of the sum of squared errors by the number of degrees of freedom \\(n-k\\) instead of \\(n\\)). Third, likelihood can be used for the model assessment, even when the standard statistics, such as \\(R^2\\) or F-test are not available. We do not discuss these aspects in this textbook. Finally, it permits the model selection via information criteria. In general, this is not possible to do unless you assume a distribution and maximise the respective likelihood. In some statistical literature, you can notice that information criteria are calculated for the models estimated via OLS, but what the authors of such resources do not tell you is that there is still an assumption of normality behind this (see the link between OLS and MLE of Normal distribution above). Note that the likelihood approach assumes that all parameters of the model are estimated, including location, scale, shape, shift etc of distribution. So typically it has more parameters to estimate than, for example, the OLS does. This is discussed in some detail later in the next section. "],["statisticsNumberOfParameters.html", "13.3 Calculating number of parameters in models", " 13.3 Calculating number of parameters in models When performing model selection and calculating different statistics, it is important to know how many parameters were estimated in the model. While this might seems trivial there are a number of edge cases and wrinkles that are seldom discussed in detail. When it comes to inference based on regression models, the general idea is to calculate the number of all the independent estimated parameters \\(k\\). This typically includes all initial components and all coefficients of the model together with the scale, shape and shift parameters of the assumed distribution (e.g. variance in the Normal distribution). Example 13.1 In a simple regression model: \\(y_j = \\beta_0 + \\beta_1 x_j + \\epsilon_j\\) - assuming Normal distribution for \\(\\epsilon_j\\), using the MLE will result in the estimation of \\(k=3\\): the two parameters of the model (\\(\\beta_0\\) and \\(\\beta_1\\)) and the variance of the error term \\(\\sigma^2\\). If likelihood is not used, then the number of parameters might be different. For example, if we estimate the model via the minimisation of MSE (similar to OLS), then the number of all estimated parameters does not include the variance anymore - it is obtained as a by product of the estimation. This is because the likelihood needs to have all the parameters of distribution in order to be maximised, but with MSE, we just minimise the mean of squared errors, and the variance of the distribution is obtained automatically. While the values of parameters might be the same, the logic is slightly different. Example 13.2 This means that for the same simple linear regression, estimated using OLS, the number of parameters is equal to 2: estimates of \\(\\beta_0\\) and \\(\\beta_1\\). In addition, all the restrictions on the parameters can reduce the number of estimated parameters, when they get to the boundary values. Example 13.3 If we know that the parameter \\(\\beta_1\\) lies between 0 and 1, and in the estimation process it gets to the value of 1 (due to how the optimiser works), it can be considered as a restriction \\(\\beta_1=1\\). So, when estimated via the minimum of MSE with this restriction, this would imply that \\(k=1\\). In general, if a parameter is provided in the model, then it does not count towards the number of all estimated parameters. So, setting \\(b_1=1\\) acts in the same fashion. Finally, if a parameter is just a function of another one, then it does not count towards the \\(k\\) as well. Example 13.4 If we know that in the same simple linear regression \\(\\beta_1 = \\frac{\\beta_0}{\\sigma^2}\\), then the number of all the estimated parameter via the maximum likelihood is 2: \\(\\beta_0\\) and \\(\\sigma^2\\). We will come back to the number of parameters later in this textbook, when we discuss specific models. A final note: typically, the standard maximum likelihood estimators for the scale, shape and shift parameters are biased in small samples and do not coincide with the OLS estimators. For example, in case of Normal distribution, OLS estimate of variance has \\(n-k\\) in the denominator, while the likelihood one has just \\(n\\). This needs to be taken into account, when the variance is used in forecasting. "],["informationCriteria.html", "13.4 Information criteria", " 13.4 Information criteria There are different ways how to select the most appropriate model for the data. One can use judgment, statistical tests, cross-validation or meta learning. The state of the art one in the field of exponential smoothing relies on the calculation of information criteria and on selection of the model with the lowest value. This approach is discussed in detail in Burnham and Anderson (2004). Here we briefly explain how this approach works and what are its advantages and disadvantages. 13.4.1 The idea Before we move to the mathematics and well-known formulae, it makes sense to understand what we are trying to do, when we use information criteria. The idea is that we have a pool of model under consideration, and that there is a true model somewhere out there (not necessarily in our pool). This can be presented graphically in the following way: Figure 13.9: An example of a model space This plot 13.9 represents a space of models. There is a true one in the middle, and there are four models under consideration: Model 1, Model 2, Model 3 and Model 4. They might differ in terms of functional form (additive vs. multiplicative), or in terms of included/omitted variables. All models are at some some distance (the grey dashed lines) from the true model in this hypothetic model space: Model 1 is closest while Model 2 is farthest. Models 3 and 4 have similar distances to the truth. In the model selection exercise what we typically want to do is to select the model closest to the true one (Model 1 in our case). This is easy to do when you know the true model: just measure the distances and select the closest one. This can be written very roughly as: \\[\\begin{equation} \\begin{split} d_1 = \\ell^* - \\ell_1 \\\\ d_2 = \\ell^* - \\ell_2 \\\\ d_3 = \\ell^* - \\ell_3 \\\\ d_4 = \\ell^* - \\ell_4 \\end{split} , \\tag{13.11} \\end{equation}\\] where \\(\\ell_j\\) is the position of the \\(j^{th}\\) model and \\(\\ell^*\\) is the position of the true one. One of ways of getting the position of the model is by calculating the log-likelihood (logarithms of likelihood) values for each model, based on the assumed distributions. The likelihood of the true model will always be fixed, so if it is known it just comes to calculating the values for the models 1 - 4, inserting them in the equations in (13.11), and selecting the model that has the lowest distance \\(d_j\\). In reality, however, we never know the true model. We therefore need to find some other way of measuring the distances. The neat thing about the maximum likelihood approach is that the true model has the highest possible likelihood by definition! This means that it is not important to know \\(\\ell^*\\) – it will be the same for all the models. So, we can drop the \\(\\ell^*\\) in the formulae (13.11) and compare the models via their likelihoods \\(\\ell_1, \\ell_2, \\ell_3 \\text{ and } \\ell_4\\) alone: \\[\\begin{equation} \\begin{split} d_1 = - \\ell_1 \\\\ d_2 = - \\ell_2 \\\\ d_3 = - \\ell_3 \\\\ d_4 = - \\ell_4 \\end{split} , \\tag{13.12} \\end{equation}\\] This is a very simple method that allows us to get to the model closest to the true one in the pool. However, we should not forget that we usually work with samples of data instead of the entire population and correspondingly will have only estimates of likelihoods and not the true ones. Inevitably, they will be biased and will need to be corrected. Akaike (1974) showed that the bias can be corrected if the number of parameters in each model is added to the distances (13.12) resulting in the bias corrected formula: \\[\\begin{equation} d_j = k_j - \\ell_j \\tag{13.13}, \\end{equation}\\] where \\(k_j\\) is the number of estimated parameters in model \\(j\\) (this typically includes scale parameters when dealing with Maximum Likelihood Estimates). Akaike (1974) suggests “An Information Criterion” which multiplies both parts of the right-hand side of (13.13) by 2 so that there is a correspondence between the criterion and the well-known likelihood ratio test (Wikipedia, 2020c): \\[\\begin{equation} \\mathrm{AIC}_j = 2 k_j - 2 \\ell_j \\tag{13.14}. \\end{equation}\\] This criterion now more commonly goes by the “Akaike Information Criterion.” Various alternative criteria motivated by similar ideas have been proposed. The following are worth mentioning: AICc (Sugiura, 1978), which is a sample corrected version of the AIC for normal and related distributions, which takes the number of observations into account: \\[\\begin{equation} \\mathrm{AICc}_j = 2 \\frac{n}{n-k_j-1} k_j - 2 \\ell_j \\tag{13.15}, \\end{equation}\\] where \\(n\\) is the sample size. BIC (Schwarz, 1978) (aka “Schwarz criterion”), which is derived from Bayesian statistics: \\[\\begin{equation} \\mathrm{BIC}_j = \\log(n) k_j - 2 \\ell_j \\tag{13.16}. \\end{equation}\\] BICc (McQuarrie, 1999) - the sample-corrected version of BIC, relying on the assumption of normality: \\[\\begin{equation} \\mathrm{BICc}_j = \\frac{n \\log (n)}{n-k_j-1} k_j - 2 \\ell_j \\tag{13.17}. \\end{equation}\\] In general, the use of the sample-corrected versions of the criteria (AICc, BICc) is recommended unless sample size is very large (thousands of observations), in which case the effect of the number of observations on the criteria becomes negligible. The main issue is that corrected versions of information criteria for non-normal distributions need to be derived separately and will differ from (13.15) and (13.17). Still, Burnham and Anderson (2004) recommend using formulae (13.15) and (13.17) in small samples even if the distribution of variables is not normal and the correct formulae are not known. The motivation for this is that the corrected versions still take sample size into account, correcting the sample bias in criteria to some extent. A thing to note is that the approach relies on asymptotic properties of estimators and assumes that the estimation method used in the process guarantees that the likelihood functions of the models are maximised. In fact, it relies on asymptotic behaviour of parameters, so it is not very important whether the maximum of the likelihood in sample is reached or not or whether the final solution is near the maximum. If the sample size changes, the parameters guaranteeing the maximum will change as well so we cannot get the point correctly in sample anyway. However, it is much more important to use an estimation method that will guarantee consistent maximisation of the likelihood. This implies that we might select wrong models in some cases in sample, but that is okay, because if we use the adequate approach for estimation and selection, with the increase of the sample size, we will select the correct model more often than an incorrect one. While the “increase of sample size” might seem as an unrealistic idea in some real life cases, keep in mind that this might mean not just the increase of \\(n\\), but also the increase of the number of series under consideration. So, for example, the approach should select the correct model on average, when you test it on a sample of 10,000 SKUs. Summarising, the idea of model selection via information criteria is to: form a pool of competing models, construct and estimate them, calculate their likelihoods, calculate the information criteria, and finally, select the model that has the lowest value under the information criterion. This approach is relatively fast (in comparison with cross-validation, judgmental selection or meta learning) and has good theory behind it. It can also be shown that for normal distributions selecting time series models on the basis of AIC is asymptotically equivalent to the selection based on leave-one-out cross-validation with MSE. This becomes relatively straightforward, if we recall that typically time series models rely on one step ahead errors \\((e_t = y_t - \\mu_{t|t-1})\\) and that the maximum of the likelihood of Normal distribution gives the same estimates as the minimum of MSE. As for the disadvantages of the approach, as mentioned above, it relies on the in-sample value of the likelihood, based on one step ahead error, and does not guarantee that the selected model will perform well for the holdout for multiple steps ahead. Using the cross-validation or rolling origin for the full horizon could give better results if you suspect that information criteria do not work. Furthermore, any criterion is random on its own, and will change with the sample This means that there is model selection uncertainty and that which model is best might change with new observations. In order to address this issue, combinations of models can be used, which allows mitigating this uncertainty. 13.4.2 Common confusions related to information criteria Similar to the discussion of hypothesis testing, I have decided to collect common mistakes and confusions related to information criteria. Here they are: “AIC relies on Normal distribution.” This is not correct. AIC relies on the value of maximised likelihood function. It will use whatever you provide it, so it all comes to the assumptions you make. Having said that, if you use the sample corrected versions of information criteria, such as AICc or BICc, then you should keep in mind that the formulae (13.15) and (13.17) are derived for Normal distribution. If you use a different one (not related to Normal, so not Log Normal, Box-Cox Normal, Logit Normal etc), then you would need to derive AICc and BICc for it. Still Burnham and Anderson (2004) argue that even if you do not have the correct formula for your distribution, using (13.15) and (13.17) is better than using the non-corrected versions, because there is at least some correction of the bias caused by sample size. “We have removed outlier from the model, AIC has decreased.” AIC will always decrease if you decrease the sample size and fit the model with the same specification. This is because likelihood function relies on the joint PDF of all observations in sample. If the sample decreases, the likelihood increases. This effect is observed not only in cases, when outliers are removed, but also in case of taking differences of the data. So, when comparing models, make sure that they are constructed on exactly the same data. “We have estimated model with logarithm of response variable, and AIC has decreased” (in comparison with the linear one). AIC is comparable only between models with the same response variable. If you transform the response variable, you inevitably assume a different distribution. For example, taking logarithm and assuming that error term follows normal distribution is equivalent to assuming that the original data follows log-normal distribution. If you want to make information criteria comparable in this case, either estimate the original model with a different distribution or transform AIC for the multiplicative model. “We have used quantile regression, assuming normality and AIC is…” Information criteria only work, when the likelihood with the assumed distribution is maximised, because only then it can be guaranteed that the estimates of parameters will be consistent and efficient. If you assume normality, then you either need to maximise the respective likelihood or minimise MSE - they will give the same solution. If you use quantile regression, then you should use likelihood of Asymmetric Laplace. If you estimate parameters via minimisation of MAE, then Laplace distribution of residuals is a suitable assumption for your model. In the cases when distribution and loss are not connected, the selection mechanism might break and not work as intended. References "],["uncertaintyModel.html", "Chapter 14 Uncertainty about the model form", " Chapter 14 Uncertainty about the model form In this Chapter, we discuss more advanced topics related to regression modelling. In a way, this part builds upon elements of Statistical Learning (see, for example, the textbook of Hastie et al., 2009) and focuses on how to select variables for regression model. We start with a fundamental idea of bias-variance tradeoff, which lies in the core of many selection methods. We then move to the discussion of information criteria, explaining what they imply, after that - to several existing variable selection approaches, explaining their advantages and limitations. Furthermore, we discuss combination approaches and what they mean in terms of parameters of models. We finish this chapter with an introductory discussion of regularisation techniques (such as LASSO and RIDGE). References "],["bias-variance-tradeoff.html", "14.1 Bias-variance tradeoff", " 14.1 Bias-variance tradeoff 14.1.1 Graphical explanation In order to better understand, why we need to bother with model selection, combinations and advanced estimators, we need to understand the principle of bias-variance tradeoff. Consider a simple example of relation between fuel consumption of a car and the engine size based on the mtcars dataset in R (Figure 14.1): plot(mtcars$disp, mtcars$mpg, xlab=&quot;Engine size&quot;, ylab=&quot;Miles per galon&quot;) Figure 14.1: Fuel consumption vs engine size The plot in Figure 14.1 demonstrates clear non-linearity. Indeed, we would expect the relation between these variables to be non-linear in real life: it is difficult to imagine the situation, where a car with no engine will be able to drive at all. On the other hand, a car with a huge engine will still be able to drive some distance, although probably very small. The linear model would assume that the “no engine” case would correspond to the value of approximately 30 miles per gallon (the intersection with y-axis), while the case of “huge engine” would probably result in negative mileage. So, the theoretically suitable model should be multiplicative, which for example can be formulated in logarithms: \\[\\begin{equation} \\log mpg_j = \\beta_0 + \\beta_1 \\log disp_j + \\epsilon_j , \\tag{14.1} \\end{equation}\\] where \\(mpg_j\\) is the miles per galon of a car and \\(disp_j\\) is the displacement (size) of engine. We will assume for now that this is the “true model,” which would fit the data in the following way if we knew all the data in the universe (Figure 14.2): Figure 14.2: Fuel consumption vs engine size and the true model While being wrong, we could still use the linear model to capture some relations in some parts of the data. It would not be a perfect fit (and would have some issues in the tails of our data), but it would be an acceptable approximation of the true model in some situations. If we vary the sample, we will see how the model would behave, which would help us in understanding of the uncertainty associated with it (Figure 14.3). Figure 14.3: Fuel consumption vs engine size, the true and the linear models Figure 14.3 demonstrates the situation, where the linear model was fit to randomly peaked sub-samples of the original data. We can see that the linear model would exhibit some sort of bias in comparison with the true one: it is consistently above the true model in the region in the middle and is consistently below it in the tails of the sample. Alternatively, we could fit a high order polynomial model to approximate the data and repeat the same proecedure with sub-samples as before (Figure 14.4): Figure 14.4: Fuel consumption vs engine size, the true, the linear and the polynomial models. The new polynomial model on the plot in Figure 14.4 has a lower bias than the linear one, because on average it is closer to the true model in sample, getting closer to the green line (true model) even in the tails. However, it is also apparent that it has higher variability. This is because it is a more complex model than the linear one: it includes more variables (polynomial terms), making it more sensitive to specific observations in the sample. If we were to introduce even more polynomial terms, the model would have even more variance around the true model than before (Figure 14.5). Figure 14.5: Fuel consumption vs engine size, the true and the polynomial (7th order) models The model on plot in Figure 14.5 exhibits even higher variance in comparison with the true model, but it is still less biased than the linear model. The pattern that we observe in this demonstration is that the variance of the model in comparison with the true one increases with the increase of complexity, while the bias either decreases or does not change substantially. This is bias-variance tradeoff in action. It is the principle that states that with the increase of complexity of model, its variance (with respect to the true one) increases, while the bias decreases. This implies that typically you cannot minimise both variance and bias at the same time - depending on how you formulate and estimate a model, it will either have bigger variance or a bigger bias. This principle can be applied not only to models, but also to estimates of parameters or to forecasts from the models. It is one of the fundamental basic modelling principles. 14.1.2 Mathematical explanation Mathematically, it is represented for an estimate as parts of Mean Squared Error (MSE) of that estimate (we drop the index of observations \\(j\\) in \\(\\hat{y}_j\\) for convenience): \\[\\begin{equation} \\mathrm{MSE} = \\mathrm{Bias}(\\hat{y})^2 + \\mathrm{V}(\\hat{y}) + \\sigma^2 , \\tag{14.2} \\end{equation}\\] where \\(\\hat{y}\\) is the fitted value of our model, \\(\\mathrm{Bias}(\\hat{y})=\\mathrm{E}(\\mu_y-\\hat{y})\\), \\(\\mathrm{V}(\\hat{y})=\\mathrm{E}\\left((\\mu_y-\\hat{y})^2\\right)\\), \\(\\mu_y\\) is the fitted value of the true model, and \\(\\sigma^2\\) is the variance of the white noise of the true model. Proof. The Mean Squared Error of a model by definition is the expectation of the squared difference between the actual and the fitted values: \\[\\begin{equation} \\begin{aligned} \\mathrm{MSE} = &amp; \\mathrm{E}\\left((y -\\hat{y})^2\\right) = \\mathrm{E}\\left((\\mu_y +\\epsilon -\\hat{y})^2\\right) = \\\\ &amp; \\mathrm{E}\\left(\\left(\\mu_y +\\epsilon -\\hat{y} +\\mathrm{E}(\\hat{y}) -\\mathrm{E}(\\hat{y})\\right)^2\\right) \\end{aligned} \\tag{14.3} \\end{equation}\\] The expectation of square of sum can be expanded as: \\[\\begin{equation} \\begin{aligned} \\mathrm{MSE} = &amp; \\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y}))^2\\right) +\\mathrm{E}\\left((\\mathrm{E}(\\hat{y}) -\\hat{y})^2\\right) +\\mathrm{E}(\\epsilon^2) +2\\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y})) \\epsilon \\right) +2\\mathrm{E}\\left((\\mathrm{E}(\\hat{y}) -\\hat{y}) \\epsilon \\right) +2\\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y}))(\\mathrm{E}(\\hat{y}) -\\hat{y}) \\right) \\end{aligned} \\tag{14.4} \\end{equation}\\] We now can consider each element of the sum of squares in (14.4) to understand what they are equal to. We start with the first one, which can be expanded to: \\[\\begin{equation} \\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y}))^2\\right) = \\mathrm{E}(\\mu_y^2) -2 \\mathrm{E}(\\mu_y \\mathrm{E}(\\hat{y})) + \\mathrm{E}(\\mathrm{E}(\\hat{y})^2). \\tag{14.5} \\end{equation}\\] Given that \\(\\mu_y\\) is the value of the model in the population, which is fixed, its expectation will be equal to itself. In addition, the expectation of expectation is just an expectation (\\(\\mathrm{E}(\\mathrm{E}(\\hat{y})^2)=\\mathrm{E}(\\hat{y})^2\\)). This leads to the following, which is just the bias of the estimated model: \\[\\begin{equation} \\begin{aligned} \\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y}))^2\\right) = &amp; \\mu_y^2 -2 \\mu_y \\mathrm{E}(\\hat{y}) + \\mathrm{E}(\\hat{y})^2 = \\\\ &amp; (\\mu_y -\\mathrm{E}(\\hat{y}))^2 = \\mathrm{Bias}(\\hat{y})^2 \\end{aligned} \\tag{14.6} \\end{equation}\\] The second term in (14.4) is the variance of the model (by the definition of variance): \\[\\begin{equation} \\mathrm{E}\\left((\\mathrm{E}(\\hat{y}) -\\hat{y})^2\\right) = \\mathrm{V}(\\hat{y}) \\tag{14.7} \\end{equation}\\] The third term is equal to the variance of the error term as long as the expectation of the error is zero (which is one of the conventional assumptions, discussed in Subsection 12.2.3): \\[\\begin{equation} \\mathrm{E}(\\epsilon^2) = \\sigma^2 \\tag{14.8} \\end{equation}\\] The more complicated thing is to show that the the other three elements are equal to zero. For the elements number four and five, we can use the assumption that the error term of the true model is independent of anything else (see discussion in Section 12.2), leading respectively to: \\[\\begin{equation} \\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y})) \\epsilon \\right) = \\mathrm{E}(\\mu_y -\\mathrm{E}(\\hat{y})) \\times \\mathrm{E}(\\epsilon) \\tag{14.9} \\end{equation}\\] and \\[\\begin{equation} \\mathrm{E}\\left((\\mathrm{E}(\\hat{y}) -\\hat{y}) \\epsilon \\right) = \\mathrm{E}(\\mathrm{E}(\\hat{y})-\\hat{y}) \\times \\mathrm{E}(\\epsilon) . \\tag{14.10} \\end{equation}\\] Given that \\(\\mathrm{E}(\\epsilon)=0\\) due to one of the assumptions (Subsection 12.2.3), both terms will be equal to zero. Finally, the last term can be expanded to: \\[\\begin{equation} \\begin{aligned} \\mathrm{E}\\left((\\mu_y -\\mathrm{E}(\\hat{y}))(\\mathrm{E}(\\hat{y}) -\\hat{y}) \\right) = &amp; \\mathrm{E}(\\mu_y \\mathrm{E}(\\hat{y}) -\\mu_y \\hat{y} -\\mathrm{E}(\\hat{y})\\mathrm{E}(\\hat{y}) +\\mathrm{E}(\\hat{y})\\hat{y}) = \\\\ &amp; \\mu_y \\mathrm{E}(\\hat{y}) -\\mu_y \\mathrm{E}(\\hat{y}) -\\mathrm{E}(\\hat{y})^2 +\\mathrm{E}(\\hat{y})^2 = \\\\ &amp; 0 \\end{aligned} \\tag{14.11} \\end{equation}\\] So, this means that the last three terms in (14.4) are equal to zero and thus, inserting (14.5), (14.6) and (14.7) in (14.4), we get: \\[\\begin{equation*} \\mathrm{MSE} = \\mathrm{Bias}(\\hat{y})^2 + \\mathrm{V}(\\hat{y}) + \\sigma^2 \\end{equation*}\\] The similar mathematical formula holds for any other estimate, for example for an estimate of parameter. What this formula tells us is that there are two forces in the MSE of estimate that impact its value. Minimisation MSE does not imply that we reduce both of them, but most probably we are reducing one at the cost of another. The classical plot based on this looks as shown in Figure 14.6. Figure 14.6: Bias, Variance and MSE as functions of model complexity. This plot shows the basic principles: with increase of complexity, the bias of estimate decreases, but its variance increases. There is typically the specific type of model (the point on Complexity axis) that minimises MSE (the vertical line on the plot), which will have some combination of variance and bias. But in practice, this point does not guarantee that we will have an accurate adequate model. In some situations we might prefer moving to the left on the plot in Figure 14.6, sacrificing unbiasedness of model to get the reduced variance. The model selection, model combinations and regularisation methods all aim to find the sweet spot on the plot for the specific sample available to the analyst. 14.1.3 Why having biased estimate can be better than having the inefficient one? It might not be clear to everyone why the model with some bias in it might be better than the model with high variance. In order to answer this question, consider the situation, where we want to estimate the value of parameter \\(\\beta_j\\), and we have two methods to do that. Given that we work on a sample of data, the estimates will have some sorts of distributions, shown in Figure 14.7. Figure 14.7: Example. Which of the two estimates would you prefer: the first one or the second one? The conventional statistician might choose Estimate 1, because it is unbiased, meaning that on average we will have the correct value of the true parameter. However, if we rephrase the question slightly, making it more realistic, their answer would probably change: “Which of the two estimates would you prefer on small sample?” In this situation, we understand that we have limited data and need to make a decision based on what we have on hands, we might not be able to rely on asymptotic properties, on LLN and CLT (Chapter 4). If we choose Estimate 1, then on our specific sample, we might end up easily with value for \\(b_j\\) of -2, 0 or 6, just due to the pure chance - this is how wide the distribution is. On the other hand, if we choose the Estimate 2, we will end up with the value, which will be close to the true one: 2.5, 3 or 4. Yes, this value will be typically higher than needed, but at least it will not lead us to confusing conclusions. Having said that, if the bias was too high (e.g. if the distribution of the Estimate 2 was placed around -4), the estimate might become unreliable, so there should be some balance in how much bias one should impose on the estimates. But this example explains why model selection, model combinations and regularisation are important and have become so popular over the last few decades. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
